{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f291981c",
      "metadata": {
        "id": "f291981c"
      },
      "source": [
        " Copyright © Sorbonne University.\n",
        "\n",
        " This source code is licensed under the MIT license found in the\n",
        " LICENSE file in the root directory of this source tree."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "587b539b",
      "metadata": {
        "id": "587b539b"
      },
      "source": [
        "# Outlook\n",
        "\n",
        "In this notebook, you will code a naive actor-critic algorithm in the tabular case. Then you will tune it using grid search and Bayesian optimization, potentially using the [optuna](https://optuna.readthedocs.io/en/stable/) library.\n",
        "Finally, you will get the best hyper-parameters obtained with both methods and perform a statistical test to see if there is a statistically significant difference between these methods and with respect to naive hyper-parameter values.\n",
        "\n",
        "## Install libraries\n",
        "\n",
        "### 展望\n",
        "\n",
        "在本笔记本中，你将编写一个用于表格情况的简单的**演员-评论家算法**（Actor-Critic Algorithm）。然后，你将使用**网格搜索**和**贝叶斯优化**来对其进行调优，可能会使用 [optuna](https://optuna.readthedocs.io/en/stable/) 库。  \n",
        "最后，你将获取通过这两种方法得到的最佳超参数，并进行统计测试，以确定这些方法与原始超参数值之间是否存在统计学上显著的差异。\n",
        "\n",
        "## 安装库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ec87094b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec87094b",
        "outputId": "7d97b326-c50f-4da9-8f5d-4e7a25f59729"
      },
      "outputs": [],
      "source": [
        "# Installs the necessary Python and system libraries\n",
        "try:\n",
        "    from easypip import easyimport, easyinstall, is_notebook\n",
        "except ModuleNotFoundError as e:\n",
        "    get_ipython().run_line_magic(\"pip\", \"install 'easypip>=1.2.0'\")\n",
        "    from easypip import easyimport, easyinstall, is_notebook\n",
        "\n",
        "easyinstall(\"swig\")\n",
        "easyinstall(\"bbrl>=0.2.2\")\n",
        "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
        "easyinstall(\"tensorboard\")\n",
        "easyinstall(\"moviepy\")\n",
        "easyinstall(\"box2d-kengz\")\n",
        "easyinstall(\"optuna\")\n",
        "easyinstall(\"gymnasium\")\n",
        "easyinstall(\"mazemdp\")\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "import hydra\n",
        "import optuna\n",
        "import yaml\n",
        "from omegaconf import OmegaConf, DictConfig\n",
        "\n",
        "# For visualization\n",
        "os.environ[\"VIDEO_FPS\"] = \"5\"\n",
        "if not os.path.isdir(\"./videos\"):\n",
        "    os.mkdir(\"./videos\")\n",
        "\n",
        "from IPython.display import Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9b2e8a1e",
      "metadata": {
        "id": "9b2e8a1e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e6281c86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6281c86",
        "lines_to_next_cell": 1,
        "outputId": "abddb4b6-a6d4-49d4-d0b6-2885738ef96d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matplotlib backend: Agg\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "from bbrl.utils.chrono import Chrono\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mazemdp.toolbox import sample_categorical\n",
        "from mazemdp.mdp import Mdp\n",
        "from bbrl_gymnasium.envs.maze_mdp import MazeMDPEnv\n",
        "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "from functools import partial\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4073b453",
      "metadata": {
        "id": "4073b453"
      },
      "source": [
        "# Step 1: Coding the naive Actor-critic algorithm\n",
        "\n",
        "We consider the naive actor-critic algorithm with a categorical policy.\n",
        "The algorithm learns a critic with the standard temporal difference mechanism\n",
        "using a learning rate $\\alpha_{critic}$.\n",
        "\n",
        "We consider a value-based critic $V(s)$. The extension to an action value function $Q(s,a)$ is straightforward.\n",
        "\n",
        "To update the critic, the algorithm computes the temporal difference error:\n",
        "\n",
        "$$\\delta_t = r(s_t, a_t) + \\gamma V^{(n)}(s_{t+1})-V^{(n)}(s_t).$$\n",
        "\n",
        "Then it applies it to the critic:\n",
        "\n",
        "$$V^{(n+1)}(s_t) = V^{(n)}(s_t) + \\alpha_{critic} \\delta_t.$$\n",
        "\n",
        "To update the actor, the general idea is the same, using the temporal difference error with another learning rate $\\alpha_{actor}$.\n",
        "\n",
        "However, naively applying the same learning rule would not ensure that the probabilities of all actions in a state sum to 1.\n",
        "Besides, when the temporal difference error $\\delta_t$ is negative, it may happen that the probability of an action gets negative or null, which raises an issue when applying renormalization.\n",
        "\n",
        "So, instead of applying the naive rule, we apply the following one:\n",
        "$$\n",
        "\\pi_{temp}(a_t|s_t) =  \\begin{cases}\n",
        "\\pi^{(i)}(a_t|s_t) + \\alpha_{actor} \\delta_t & \\mathrm{if } \\pi^{(i)}(a_t|s_t) + \\alpha_{actor} \\delta_t > 10^{-8}\\\\\n",
        "10^{-8} & \\mathrm{otherwise.} \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Then we can apply renormalization so that the probabilities of actions still sum to 1, with\n",
        "$$\n",
        "\\forall a, \\pi^{(i+1)}(a|s_t) = \\frac{\\pi_{temp}^{(i+1)}(a|s_t)} {\\sum_{a'} \\pi_{temp}^{(i+1)}(a'|s_t)}\n",
        "$$ with\n",
        "$$\n",
        "\\pi_{temp}^{(i+1)}(a|s_t) =  \\begin{cases}\n",
        "\\pi_{temp}(a|s_t) & \\mathrm{if } a = a_t\\\\\n",
        "\\pi^{(i)}(a|s_t) & \\mathrm{otherwise.} \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "## Exercise 1\n",
        "\n",
        "### 1. Code the naive actor-critic algorithm as specified above.\n",
        "\n",
        "Some hints:\n",
        "\n",
        "- a good idea to build this code it to take inspiration from the code of Q-learning, to add an actor (a categorical policy), both learning rates,\n",
        "and to take care about the renormalization function.\n",
        "\n",
        "- for the next steps of this lab, having a function to repeatedly call your actor-critic algorithm and save the learning trajectories and\n",
        "norms of the value function is a good idea."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38730429",
      "metadata": {
        "id": "38730429"
      },
      "source": [
        "### 第一步：编写简单的演员-评论家算法\n",
        "\n",
        "我们将考虑带有**分类策略**的简单演员-评论家（Actor-Critic）算法。  \n",
        "该算法通过标准的**时序差分机制**（temporal difference mechanism）学习一个评论家（critic），使用学习率 $\\alpha_{critic}$。\n",
        "\n",
        "我们考虑一个基于值的评论家 $V(s)$。将其扩展到**动作值函数** $Q(s,a)$ 是直接的。\n",
        "\n",
        "为了更新评论家，算法计算**时序差分误差**：\n",
        "\n",
        "$$\\delta_t = r(s_t, a_t) + \\gamma V^{(n)}(s_{t+1}) - V^{(n)}(s_t).$$\n",
        "\n",
        "然后将其应用于评论家的更新：\n",
        "\n",
        "$$V^{(n+1)}(s_t) = V^{(n)}(s_t) + \\alpha_{critic} \\delta_t.$$\n",
        "\n",
        "对于**演员**（actor）的更新，基本思路相同，使用时序差分误差并结合另一个学习率 $ \\alpha_{actor} $。\n",
        "\n",
        "然而，直接应用相同的学习规则不能保证在某个状态下所有动作的概率之和为1。  \n",
        "此外，当时序差分误差 $\\delta_t$ 为负时，可能导致某个动作的概率变为负数或为零，这在归一化时会产生问题。\n",
        "\n",
        "因此，**不是直接应用简单规则**，我们使用以下更新规则：\n",
        "\n",
        "$$\n",
        "\\pi_{temp}(a_t|s_t) =  \\begin{cases}\n",
        "\\pi^{(i)}(a_t|s_t) + \\alpha_{actor} \\delta_t & \\mathrm{if } \\pi^{(i)}(a_t|s_t) + \\alpha_{actor} \\delta_t > 10^{-8}\\\\\n",
        "10^{-8} & \\mathrm{otherwise.} \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "然后我们可以应用归一化，使得动作的概率和仍然等于1：\n",
        "\n",
        "$$\n",
        "\\forall a, \\pi^{(i+1)}(a|s_t) = \\frac{\\pi_{temp}^{(i+1)}(a|s_t)} {\\sum_{a'} \\pi_{temp}^{(i+1)}(a'|s_t)}\n",
        "$$\n",
        "\n",
        "其中，\n",
        "\n",
        "$$\n",
        "\\pi_{temp}^{(i+1)}(a|s_t) =  \\begin{cases}\n",
        "\\pi_{temp}(a|s_t) & \\mathrm{if } a = a_t\\\\\n",
        "\\pi^{(i)}(a|s_t) & \\mathrm{otherwise.} \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "## 练习 1\n",
        "\n",
        "### 1. 按照上面的描述编写简单的演员-评论家算法。\n",
        "\n",
        "一些提示：\n",
        "\n",
        "- 编写此代码的一个好方法是参考**Q-learning**的代码，添加一个**演员**（分类策略）、两个学习率，并注意**归一化函数**的处理。\n",
        "\n",
        "- 对于本实验的后续步骤，建议你编写一个函数，重复调用你的演员-评论家算法，并保存学习轨迹和值函数的范数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1e7643de",
      "metadata": {
        "id": "1e7643de",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Critic:\n",
        "    def __init__(self, nb_states, alpha=0.5, init_v=0.0):\n",
        "        self.v = np.zeros(nb_states)\n",
        "        self.v[:] = init_v\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def update(self, x, y, r, gamma):\n",
        "        delta = r + gamma * self.v[y] - self.v[x]\n",
        "        self.v[x] += self.alpha * delta\n",
        "\n",
        "    def get_value_function(self):\n",
        "        return self.v\n",
        "\n",
        "\n",
        "class Actor:\n",
        "    def __init__(self, nb_states, action_space_size, alpha=0.5):\n",
        "        self.policy = np.ones((nb_states, action_space_size)) / action_space_size\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def update(self, x, u, delta):\n",
        "        if self.policy[x, u] + self.alpha * delta > 1e-8:\n",
        "            self.policy[x, u] += self.alpha * delta\n",
        "        else:\n",
        "            self.policy[x, u] = 1e-8\n",
        "\n",
        "        self.policy[x] /= np.sum(self.policy[x])  # Normalize the policy\n",
        "\n",
        "    def get_policy(self):\n",
        "        return self.policy\n",
        "\n",
        "\n",
        "def Actor_Critic(mdp, nb_episodes=20, timeout=50, alpha_critic=0.5, alpha_actor=0.5, render=True, init_v=0.0, uniform=True):\n",
        "    # Initialize Critic and Actor\n",
        "    critic = Critic(mdp.unwrapped.nb_states, alpha=alpha_critic, init_v=init_v)\n",
        "    actor = Actor(mdp.unwrapped.nb_states, mdp.action_space.n, alpha=alpha_actor)\n",
        "\n",
        "    v_list = []\n",
        "    time_list = []\n",
        "\n",
        "    mdp.timeout = timeout  # episode length\n",
        "\n",
        "    if render:\n",
        "        mdp.init_draw(\"Actor-Critic\")\n",
        "\n",
        "    for _ in range(nb_episodes):\n",
        "        # Draw the first state of episode i using a uniform distribution over all the states\n",
        "        x, _ = mdp.reset(uniform=uniform)\n",
        "        cpt = 0\n",
        "\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        while not (terminated or truncated):\n",
        "            # Show the agent in the maze\n",
        "            if render:\n",
        "                mdp.draw_v_pi(critic.get_value_function(), actor.get_policy().argmax(axis=1))\n",
        "\n",
        "            # Draw an action\n",
        "            u = np.random.choice(mdp.action_space.n, p=actor.get_policy()[x])\n",
        "\n",
        "            # Perform a step of the MDP\n",
        "            y, r, terminated, truncated, _ = mdp.step(u)\n",
        "\n",
        "            # Update the Critic\n",
        "            critic.update(x, y, r, mdp.unwrapped.gamma)\n",
        "\n",
        "            # Update the Actor policy\n",
        "            actor.update(x, u, r + mdp.unwrapped.gamma * critic.get_value_function()[y] - critic.get_value_function()[x])\n",
        "\n",
        "            # Update the agent position\n",
        "            x = y\n",
        "            cpt += 1\n",
        "\n",
        "        v_list.append(np.linalg.norm(np.maximum(critic.get_value_function(), 0)))\n",
        "        time_list.append(cpt)\n",
        "\n",
        "    if render:\n",
        "        mdp.current_state = 0\n",
        "        mdp.draw_v_pi(critic.get_value_function(), actor.get_policy())\n",
        "\n",
        "    return critic.get_value_function(), v_list, time_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da53e73",
      "metadata": {
        "id": "7da53e73"
      },
      "source": [
        "### 2. Provide a plot function\n",
        "\n",
        "Your plot function should show the evolution through time of number of steps the agent takes to find the reward in the maze.\n",
        "If your algorithm works, this number of steps should decrease through time.\n",
        "\n",
        "Your plot function should also show a mean and a standard deviation (or some more advanced statistics) over a collection of learning runs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb1a0365",
      "metadata": {
        "id": "fb1a0365"
      },
      "source": [
        "### 2. 提供一个绘图函数\n",
        "\n",
        "你的绘图函数应展示**随着时间推移，智能体找到迷宫中的奖励所花步数的变化**。  \n",
        "如果你的算法有效，那么这些步数应该随着时间的推移逐渐减少。\n",
        "\n",
        "你的绘图函数还应该展示**多个学习运行的均值**和**标准差**（或者一些更高级的统计数据）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "04f00ef3",
      "metadata": {
        "id": "04f00ef3",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def plot_learning_curve(time_list, nb_episodes):\n",
        "    \"\"\"\n",
        "    Plot the learning curve showing the evolution of the number of steps over episodes.\n",
        "\n",
        "    Parameters:\n",
        "        time_list (list of lists): A list containing lists of steps for each episode from multiple runs.\n",
        "        nb_episodes (int): Number of episodes.\n",
        "    \"\"\"\n",
        "    # Convert time_list into a numpy array for easier manipulation\n",
        "    time_array = np.array(time_list)\n",
        "\n",
        "    # Calculate the mean and standard deviation\n",
        "    mean_steps = time_array.mean(axis=0)\n",
        "    std_steps = time_array.std(axis=0)\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(mean_steps, label='Mean Steps', color='b')\n",
        "    plt.fill_between(range(nb_episodes), mean_steps - std_steps, mean_steps + std_steps,\n",
        "                     color='b', alpha=0.2, label='±1 Std Dev')\n",
        "\n",
        "    plt.title('Learning Curve: Steps to Reach Reward Over Episodes')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Number of Steps')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3c4acf6",
      "metadata": {
        "id": "a3c4acf6"
      },
      "source": [
        "## Actor-critic hyper-parameters\n",
        "\n",
        "To represent the hyper-parameters of the experiments performed in this notebook, we suggest using the dictionary below.\n",
        "This dictionary can be read using omegaconf.\n",
        "Using it is not mandatory.\n",
        "You can also change the value of hyper-parameters or environment parameters at will."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee41dad",
      "metadata": {
        "id": "cee41dad"
      },
      "source": [
        "## 演员-评论家算法的超参数\n",
        "\n",
        "为了表示本笔记本中实验的超参数，我们建议使用以下字典。  \n",
        "该字典可以通过 `omegaconf` 读取。  \n",
        "使用它并不是强制的，你也可以随意更改超参数或环境参数的值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a685f967",
      "metadata": {
        "id": "a685f967",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "ac_params = {\n",
        "    \"save_curves\": True,\n",
        "    \"save_heatmap\": True,\n",
        "    \"mdp\": {\n",
        "        \"name\": \"MazeMDP-v0\",\n",
        "        \"width\": 5,\n",
        "        \"height\": 5,\n",
        "        \"ratio\": 0.2,\n",
        "        \"render_mode\": \"rgb_array\",\n",
        "        },\n",
        "\n",
        "    \"log_dir\": \"./tmp/actor_critics'\",\n",
        "    \"video_dir\": \"./tmp/videos\",\n",
        "\n",
        "    \"nb_episodes\": 150, #original=100\n",
        "    \"timeout\": 200,\n",
        "    \"render\": False, # True, #\n",
        "    \"nb_repeats\": 5,\n",
        "\n",
        "    \"alpha_critic\": 0.5,\n",
        "    \"alpha_actor\": 0.5,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "370af64a",
      "metadata": {
        "id": "370af64a"
      },
      "source": [
        "### 3. Test your code\n",
        "\n",
        "Once everything looks OK, save the obtained plot for your lab report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "316bb3de",
      "metadata": {
        "id": "316bb3de"
      },
      "outputs": [],
      "source": [
        "\n",
        "# To be completed...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set up environment parameters\n",
        "env = gym.make(\n",
        "    id=\"MazeMDP-v0\",\n",
        "    kwargs={\"width\": 5, \"height\": 5, \"ratio\": 0.2},\n",
        "    render_mode=\"rgb_array\",\n",
        ")\n",
        "\n",
        "env.reset()\n",
        "env.unwrapped.init_draw(\"The maze\")\n",
        "\n",
        "def run_multiple_episodes(mdp, nb_runs, nb_episodes, timeout, alpha_critic, alpha_actor, render):\n",
        "    all_time_lists = []\n",
        "    for run in range(nb_runs):\n",
        "        _, time_list, _ = Actor_Critic(mdp, nb_episodes, timeout, alpha_critic, alpha_actor, render)\n",
        "        all_time_lists.append(time_list)\n",
        "    return all_time_lists\n",
        "\n",
        "def save_learning_curve_plot(time_lists, nb_episodes, save_path):\n",
        "    plot_learning_curve(time_lists, nb_episodes)\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()  # Close the figure to free memory\n",
        "def save_heatmap(heatmap_data, alpha_range, save_path):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(heatmap_data, cmap='viridis', interpolation='nearest')\n",
        "    plt.colorbar(label='Norm of Value Function')\n",
        "    plt.xticks(ticks=np.arange(len(alpha_range)), labels=[f\"{lr:.2f}\" for lr in alpha_range], rotation=45)\n",
        "    plt.yticks(ticks=np.arange(len(alpha_range)), labels=[f\"{lr:.2f}\" for lr in alpha_range])\n",
        "    plt.xlabel('Actor Learning Rate')\n",
        "    plt.ylabel('Critic Learning Rate')\n",
        "    plt.title('Heatmap of Norm of Value Function')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "# Main function to execute the test\n",
        "def main():\n",
        "    # Setup the environment\n",
        "    mdp = env\n",
        "\n",
        "    # Run multiple episodes\n",
        "    all_time_lists = run_multiple_episodes(\n",
        "        mdp,\n",
        "        nb_runs=ac_params['nb_repeats'],\n",
        "        nb_episodes=ac_params['nb_episodes'],\n",
        "        timeout=ac_params['timeout'],\n",
        "        alpha_critic=ac_params['alpha_critic'],\n",
        "        alpha_actor=ac_params['alpha_actor'],\n",
        "        render=ac_params['render']\n",
        "    )\n",
        "\n",
        "    # Create directory for saving plots if it doesn't exist\n",
        "    if ac_params['save_curves']:\n",
        "        os.makedirs(ac_params['log_dir'], exist_ok=True)\n",
        "        plot_path = os.path.join(ac_params['log_dir'], 'learning_curve.png')\n",
        "        save_learning_curve_plot(all_time_lists, ac_params['nb_episodes'], plot_path)\n",
        "\n",
        "    if ac_params['save_heatmap']:\n",
        "        # Add your heatmap saving logic here\n",
        "        os.makedirs(ac_params['log_dir'], exist_ok=True)\n",
        "        plot_path = os.path.join(ac_params['log_dir'], 'learning_curve.png')\n",
        "        save_learning_curve_plot(all_time_lists, ac_params['nb_episodes'], plot_path)\n",
        "\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4DquveEtaUha",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DquveEtaUha",
        "outputId": "c794863a-20a7-400f-95fa-dbd5c1dae117"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "932ad6ba",
      "metadata": {
        "id": "932ad6ba"
      },
      "source": [
        "# Step 2: Tuning hyper-parameters\n",
        "\n",
        "In this part, you have to optimize two hyper-parameters of the actor-critic algorithm, namely the actor and critic learning rates.\n",
        "You have to do so using a simple grid search method and some Bayesian optimization method.\n",
        "For the latter, we suggest using the default sampler from [optuna](https://optuna.readthedocs.io/en/stable/).\n",
        "Follow the above link to understand how optuna works.\n",
        "Note that it also supports grid search and many other hyper-parameters tuning algorithms.\n",
        "\n",
        "You should make sure that the hyper-parameters tuning algorithms that you compare benefit from the same training budget\n",
        "We suggest 400 training runs overall for each method,\n",
        "which means 20 values each for the actor and the critic learning rates in the case of grid search.\n",
        "\n",
        "## Exercise 2\n",
        "\n",
        "### 1. Perform hyper-parameters tuning with two algorithms as suggested above.\n",
        "\n",
        "### 2. Provide a \"heatmap\" of the norm of the value function given the hyper-parameters, after training for each pair of hyper-parameters.\n",
        "\n",
        "### 3. Collect the value of the best hyper-parameters found with each algorithm. You will need them for Step 3.\n",
        "\n",
        "### 4. Include in your report the heatmaps and the best hyper-parameters found for each method."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14cdbcc8",
      "metadata": {
        "id": "14cdbcc8"
      },
      "source": [
        "# 第二步：调优超参数\n",
        "\n",
        "在本部分中，你需要优化演员-评论家算法中的两个超参数，分别是**演员**和**评论家**的学习率。  \n",
        "你需要使用**简单网格搜索方法**和某种**贝叶斯优化方法**来完成此任务。  \n",
        "对于后者，我们建议使用 [optuna](https://optuna.readthedocs.io/en/stable/) 的默认采样器。  \n",
        "你可以点击上面的链接了解 optuna 的工作原理。  \n",
        "请注意，optuna 也支持网格搜索和其他多种超参数调优算法。\n",
        "\n",
        "你应该确保所比较的超参数调优算法拥有相同的训练预算。  \n",
        "我们建议每种方法整体进行 400 次训练运行，  \n",
        "这意味着在网格搜索的情况下，演员和评论家学习率各取 20 个值。\n",
        "\n",
        "## 练习 2\n",
        "\n",
        "### 1. 使用上述建议的两种算法进行超参数调优。\n",
        "\n",
        "### 2. 提供一个关于值函数范数的“热图”（heatmap），展示每对超参数在训练后的表现。\n",
        "\n",
        "### 3. 收集每种算法找到的最佳超参数值。你将在步骤3中需要这些参数。\n",
        "\n",
        "### 4. 在报告中包含热图以及每种方法找到的最佳超参数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "251a2c6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "251a2c6a",
        "outputId": "9aacdccd-e8a3-4bae-f484-8b811e1cfa31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best value norm: 0.5561 (Actor LR: 0.01, Critic LR: 0.01)\n",
            "New best value norm: 3.6662 (Actor LR: 0.01, Critic LR: 0.06)\n",
            "New best value norm: 4.0213 (Actor LR: 0.01, Critic LR: 0.06)\n",
            "New best value norm: 5.8637 (Actor LR: 0.01, Critic LR: 0.11)\n",
            "New best value norm: 6.4490 (Actor LR: 0.01, Critic LR: 0.11)\n",
            "New best value norm: 7.0331 (Actor LR: 0.01, Critic LR: 0.11)\n",
            "New best value norm: 7.0934 (Actor LR: 0.01, Critic LR: 0.11)\n",
            "New best value norm: 10.0201 (Actor LR: 0.01, Critic LR: 0.17)\n",
            "New best value norm: 10.6363 (Actor LR: 0.01, Critic LR: 0.17)\n",
            "New best value norm: 11.5954 (Actor LR: 0.01, Critic LR: 0.17)\n",
            "New best value norm: 13.0808 (Actor LR: 0.01, Critic LR: 0.22)\n",
            "New best value norm: 13.9967 (Actor LR: 0.01, Critic LR: 0.22)\n",
            "New best value norm: 14.7926 (Actor LR: 0.01, Critic LR: 0.27)\n",
            "New best value norm: 16.2509 (Actor LR: 0.01, Critic LR: 0.27)\n",
            "New best value norm: 16.9974 (Actor LR: 0.01, Critic LR: 0.27)\n",
            "New best value norm: 19.2514 (Actor LR: 0.01, Critic LR: 0.43)\n",
            "New best value norm: 20.3794 (Actor LR: 0.01, Critic LR: 0.58)\n",
            "New best value norm: 22.0450 (Actor LR: 0.06, Critic LR: 0.22)\n",
            "New best value norm: 23.3381 (Actor LR: 0.06, Critic LR: 0.27)\n",
            "New best value norm: 23.8247 (Actor LR: 0.06, Critic LR: 0.27)\n",
            "New best value norm: 24.2148 (Actor LR: 0.06, Critic LR: 0.27)\n",
            "New best value norm: 24.7037 (Actor LR: 0.06, Critic LR: 0.32)\n",
            "New best value norm: 25.3387 (Actor LR: 0.06, Critic LR: 0.32)\n",
            "New best value norm: 26.0582 (Actor LR: 0.06, Critic LR: 0.37)\n",
            "New best value norm: 26.6635 (Actor LR: 0.06, Critic LR: 0.37)\n",
            "New best value norm: 26.9411 (Actor LR: 0.06, Critic LR: 0.43)\n",
            "New best value norm: 27.1335 (Actor LR: 0.06, Critic LR: 0.43)\n",
            "New best value norm: 27.3445 (Actor LR: 0.06, Critic LR: 0.43)\n",
            "New best value norm: 27.5172 (Actor LR: 0.06, Critic LR: 0.48)\n",
            "New best value norm: 27.9179 (Actor LR: 0.06, Critic LR: 0.58)\n",
            "New best value norm: 28.0444 (Actor LR: 0.06, Critic LR: 0.64)\n",
            "New best value norm: 28.2368 (Actor LR: 0.11, Critic LR: 0.48)\n",
            "New best value norm: 28.4400 (Actor LR: 0.11, Critic LR: 0.48)\n",
            "New best value norm: 28.9710 (Actor LR: 0.11, Critic LR: 0.53)\n",
            "New best value norm: 28.9863 (Actor LR: 0.11, Critic LR: 0.53)\n",
            "New best value norm: 29.0126 (Actor LR: 0.11, Critic LR: 0.69)\n",
            "New best value norm: 29.1331 (Actor LR: 0.17, Critic LR: 0.53)\n",
            "New best value norm: 29.2987 (Actor LR: 0.17, Critic LR: 0.58)\n",
            "New best value norm: 29.3827 (Actor LR: 0.17, Critic LR: 0.64)\n",
            "New best value norm: 29.4251 (Actor LR: 0.17, Critic LR: 0.69)\n",
            "New best value norm: 29.4743 (Actor LR: 0.17, Critic LR: 0.69)\n",
            "New best value norm: 29.7940 (Actor LR: 0.17, Critic LR: 0.74)\n",
            "New best value norm: 29.7958 (Actor LR: 0.22, Critic LR: 0.84)\n",
            "New best value norm: 29.8254 (Actor LR: 0.27, Critic LR: 0.69)\n",
            "New best value norm: 30.0620 (Actor LR: 0.27, Critic LR: 0.79)\n",
            "New best value norm: 30.1355 (Actor LR: 0.58, Critic LR: 0.79)\n",
            "Best Actor Learning Rate: 0.5831578947368421\n",
            "Best Critic Learning Rate: 0.791578947368421\n",
            "Best Norm of Value Function: 30.13554444060969\n"
          ]
        }
      ],
      "source": [
        "# To be completed...\n",
        "\n",
        "# Hyperparameters for grid search\n",
        "num_points = 20 # Number of points in the grid for each learning rate\n",
        "learning_rate_range = np.linspace(0.01, 1.0, num_points)  # Learning rates range for both actor and critic\n",
        "best_actor_lr = None\n",
        "best_critic_lr = None\n",
        "best_value_norm = -np.inf\n",
        "\n",
        "# Create directories for saving plots if needed\n",
        "log_dir = \"./tmp/actor_critics\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def grid_search(mdp,num_runs,nb_episodes, timeout, alpha_critic, alpha_actor, render):\n",
        "  global best_actor_lr,best_critic_lr,best_value_norm\n",
        "\n",
        "  value_norms = np.zeros((num_points, num_points))\n",
        "\n",
        "  for i, alpha_actor in enumerate(learning_rate_range):\n",
        "    for j, alpha_critic in enumerate(learning_rate_range):\n",
        "      # print(f\"Testing Actor LR: {alpha_actor:.2f}, Critic LR: {alpha_critic:.2f}\")\n",
        "      all_time_lists = []\n",
        "\n",
        "      for run in range(num_runs):\n",
        "        v, v_list, _ = Actor_Critic(mdp, nb_episodes, timeout, alpha_critic, alpha_actor, render)\n",
        "\n",
        "        # Store the final norm of the value function\n",
        "        final_value_norm = np.linalg.norm(v)\n",
        "        value_norms[i, j] = final_value_norm\n",
        "\n",
        "        # Update best hyperparameters if needed\n",
        "        if final_value_norm >= best_value_norm:\n",
        "          best_value_norm = final_value_norm\n",
        "          best_actor_lr = alpha_actor\n",
        "          best_critic_lr = alpha_critic\n",
        "          # Print the current best value and the corresponding hyperparameters\n",
        "          print(f\"New best value norm: {best_value_norm:.4f} (Actor LR: {best_actor_lr:.2f}, Critic LR: {best_critic_lr:.2f})\")\n",
        "\n",
        "  return value_norms, best_actor_lr, best_critic_lr\n",
        "\n",
        "# Function to plot heatmap\n",
        "def plot_heatmap(value_norms, save_path):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(value_norms, cmap='viridis', interpolation='nearest')\n",
        "    plt.colorbar(label='Norm of Value Function')\n",
        "    plt.xticks(ticks=np.arange(num_points), labels=[f\"{lr:.2f}\" for lr in learning_rate_range], rotation=45)\n",
        "    plt.yticks(ticks=np.arange(num_points), labels=[f\"{lr:.2f}\" for lr in learning_rate_range])\n",
        "    plt.xlabel('Actor Learning Rate')\n",
        "    plt.ylabel('Critic Learning Rate')\n",
        "    plt.title('Heatmap of Norm of Value Function')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "# Main function to execute grid search and save heatmap\n",
        "def main():\n",
        "    # Setup the environment\n",
        "    mdp = env\n",
        "\n",
        "    # Run grid search\n",
        "    value_norms, best_actor_lr, best_critic_lr = grid_search(mdp,\n",
        "        num_runs=ac_params['nb_repeats'],\n",
        "        nb_episodes=ac_params['nb_episodes'],\n",
        "        timeout=ac_params['timeout'],\n",
        "        alpha_critic=ac_params['alpha_critic'],\n",
        "        alpha_actor=ac_params['alpha_actor'],\n",
        "        render=ac_params['render'])\n",
        "\n",
        "    # Save the heatmap\n",
        "    heatmap_path = os.path.join(log_dir, 'heatmap_value_norms.png')\n",
        "    plot_heatmap(value_norms, heatmap_path)\n",
        "\n",
        "    # Output best hyperparameters\n",
        "    print(\"Best Actor Learning Rate:\", best_actor_lr)\n",
        "    print(\"Best Critic Learning Rate:\", best_critic_lr)\n",
        "    print(\"Best Norm of Value Function:\", best_value_norm)\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "wQK9ZQsohQyG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQK9ZQsohQyG",
        "outputId": "e14616e2-5f1e-467c-ec8c-e068cc850c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best value norm: 17.0044 (Actor LR: 0.27, Critic LR: 0.15)\n",
            "New best value norm: 26.9727 (Actor LR: 0.17, Critic LR: 0.39)\n",
            "New best value norm: 28.9826 (Actor LR: 0.35, Critic LR: 0.54)\n",
            "New best value norm: 29.3344 (Actor LR: 0.86, Critic LR: 0.68)\n",
            "New best value norm: 29.5961 (Actor LR: 0.39, Critic LR: 0.65)\n",
            "New best value norm: 29.6123 (Actor LR: 0.64, Critic LR: 0.91)\n",
            "New best value norm: 29.7374 (Actor LR: 0.85, Critic LR: 0.88)\n",
            "New best value norm: 29.7711 (Actor LR: 0.87, Critic LR: 0.87)\n",
            "New best value norm: 29.7741 (Actor LR: 0.80, Critic LR: 0.91)\n",
            "New best value norm: 29.7829 (Actor LR: 0.65, Critic LR: 0.88)\n",
            "New best value norm: 29.8316 (Actor LR: 0.76, Critic LR: 0.83)\n",
            "New best value norm: 29.9327 (Actor LR: 0.99, Critic LR: 0.75)\n",
            "Best Actor Learning Rate (Random Search): 0.9905006877843173\n",
            "Best Critic Learning Rate (Random Search): 0.7519294407720812\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Number of training runs\n",
        "total_training_runs = 400\n",
        "num_random_samples = total_training_runs\n",
        "\n",
        "# Learning rate range for random search\n",
        "learning_rate_range = (0.01, 1.0)\n",
        "\n",
        "# Function to randomly sample learning rates\n",
        "def sample_learning_rate():\n",
        "    return np.random.uniform(learning_rate_range[0], learning_rate_range[1])\n",
        "\n",
        "# Create directories for saving plots and results if needed\n",
        "log_dir = \"./tmp/actor_critics\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Random search for hyper-parameter tuning\n",
        "def random_search(mdp, num_runs, nb_episodes, timeout, render):\n",
        "    best_actor_lr = None\n",
        "    best_critic_lr = None\n",
        "    best_value_norm = -np.inf\n",
        "    sampled_actor_critic_pairs = []\n",
        "    value_norms = []\n",
        "\n",
        "    for run in range(num_random_samples):\n",
        "        alpha_actor = sample_learning_rate()\n",
        "        alpha_critic = sample_learning_rate()\n",
        "\n",
        "        # Store the pair of learning rates\n",
        "        sampled_actor_critic_pairs.append((alpha_actor, alpha_critic))\n",
        "\n",
        "        # Store the final value norms over multiple repeats\n",
        "        all_value_norms = []\n",
        "\n",
        "        for repeat in range(num_runs):\n",
        "            v, v_list, _ = Actor_Critic(mdp, nb_episodes, timeout, alpha_critic, alpha_actor, render)\n",
        "            final_value_norm = np.linalg.norm(v)\n",
        "            all_value_norms.append(final_value_norm)\n",
        "\n",
        "        avg_value_norm = np.mean(all_value_norms)\n",
        "        value_norms.append(avg_value_norm)\n",
        "\n",
        "        # Update best hyperparameters if needed\n",
        "        if avg_value_norm > best_value_norm:\n",
        "            best_value_norm = avg_value_norm\n",
        "            best_actor_lr = alpha_actor\n",
        "            best_critic_lr = alpha_critic\n",
        "\n",
        "            print(f\"New best value norm: {best_value_norm:.4f} (Actor LR: {best_actor_lr:.2f}, Critic LR: {best_critic_lr:.2f})\")\n",
        "\n",
        "    return sampled_actor_critic_pairs, value_norms, best_actor_lr, best_critic_lr\n",
        "\n",
        "# Function to create a heatmap\n",
        "def plot_random_search_heatmap(sampled_pairs, value_norms, save_path):\n",
        "    # Convert learning rates and value norms to 2D arrays for heatmap plotting\n",
        "    actor_lrs = [pair[0] for pair in sampled_pairs]\n",
        "    critic_lrs = [pair[1] for pair in sampled_pairs]\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = plt.scatter(actor_lrs, critic_lrs, c=value_norms, cmap='viridis', s=100, edgecolor='k')\n",
        "    plt.colorbar(scatter, label='Norm of Value Function')\n",
        "    plt.xlabel('Actor Learning Rate')\n",
        "    plt.ylabel('Critic Learning Rate')\n",
        "    plt.title('Random Search - Heatmap of Norm of Value Function')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "# Main function to execute random search and save results\n",
        "def main():\n",
        "\n",
        "\n",
        "    # Setup the environment (assuming mdp environment is set up elsewhere)\n",
        "    mdp = env\n",
        "\n",
        "    # Random search for hyper-parameters\n",
        "    sampled_pairs, value_norms, best_actor_lr_random, best_critic_lr_random = random_search(\n",
        "        mdp,\n",
        "        num_runs=ac_params['nb_repeats'],\n",
        "        nb_episodes=ac_params['nb_episodes'],\n",
        "        timeout=ac_params['timeout'],\n",
        "        render=ac_params['render']\n",
        "    )\n",
        "\n",
        "    # Save the heatmap\n",
        "    heatmap_path = os.path.join(log_dir, 'random_search_heatmap.png')\n",
        "    plot_random_search_heatmap(sampled_pairs, value_norms, heatmap_path)\n",
        "\n",
        "    # Log the best hyperparameters found\n",
        "    print(\"Best Actor Learning Rate (Random Search):\", best_actor_lr_random)\n",
        "    print(\"Best Critic Learning Rate (Random Search):\", best_critic_lr_random)\n",
        "    return best_actor_lr_random, best_critic_lr_random\n",
        "\n",
        "# Run the main function and store the best hyperparameters\n",
        "best_actor_lr_random, best_critic_lr_random = main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21ba521f",
      "metadata": {
        "id": "21ba521f"
      },
      "source": [
        "# Step 3: Statistical tests\n",
        "\n",
        "Now you have to compare the performance of the actor-critic algorithm tuned\n",
        "with all the best hyper-parameters you found before, using statistical tests.\n",
        "\n",
        "The functions below are provided to run Welch's T-test over learning curves.\n",
        "They have been adapted from a github repository: https://github.com/flowersteam/rl_stats\n",
        "You don't need to understand them in detail (though it is always a good idea to try to understand more code)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93663e15",
      "metadata": {
        "id": "93663e15"
      },
      "source": [
        "# 第三步：统计测试\n",
        "\n",
        "现在，你需要对比使用前面找到的所有最佳超参数调优后的**演员-评论家算法**的性能，使用**统计测试**进行对比。\n",
        "\n",
        "下面提供的函数用于对学习曲线运行**Welch’s T检验**。  \n",
        "这些函数是从以下 GitHub 仓库改编的：https://github.com/flowersteam/rl_stats  \n",
        "你不需要详细理解这些函数（尽管理解更多代码总是个好主意）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a2258ad0",
      "metadata": {
        "id": "a2258ad0",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_ind\n",
        "import bootstrapped.bootstrap as bs\n",
        "import bootstrapped.compare_functions as bs_compare\n",
        "import bootstrapped.stats_functions as bs_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ba2201b1",
      "metadata": {
        "id": "ba2201b1",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def compute_central_tendency_and_error(id_central, id_error, sample):\n",
        "\n",
        "    try:\n",
        "        id_error = int(id_error)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if id_central == \"mean\":\n",
        "        central = np.nanmean(sample, axis=1)\n",
        "    elif id_central == \"median\":\n",
        "        central = np.nanmedian(sample, axis=1)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if isinstance(id_error, int):\n",
        "        low = np.nanpercentile(sample, q=int((100 - id_error) / 2), axis=1)\n",
        "        high = np.nanpercentile(sample, q=int(100 - (100 - id_error) / 2), axis=1)\n",
        "    elif id_error == \"std\":\n",
        "        low = central - np.nanstd(sample, axis=1)\n",
        "        high = central + np.nanstd(sample, axis=1)\n",
        "    elif id_error == \"sem\":\n",
        "        low = central - np.nanstd(sample, axis=1) / np.sqrt(sample.shape[0])\n",
        "        high = central + np.nanstd(sample, axis=1) / np.sqrt(sample.shape[0])\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    return central, low, high"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3f30e0af",
      "metadata": {
        "id": "3f30e0af",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def run_test(test_id, data1, data2, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Compute tests comparing data1 and data2 with confidence level alpha\n",
        "    :param test_id: (str) refers to what test should be used\n",
        "    :param data1: (np.ndarray) sample 1\n",
        "    :param data2: (np.ndarray) sample 2\n",
        "    :param alpha: (float) confidence level of the test\n",
        "    :return: (bool) if True, the null hypothesis is rejected\n",
        "    \"\"\"\n",
        "    data1 = data1.squeeze()\n",
        "    data2 = data2.squeeze()\n",
        "    n1 = data1.size\n",
        "    n2 = data2.size\n",
        "\n",
        "    # perform Welch t-test\":\n",
        "    _, p = ttest_ind(data1, data2, equal_var=False)\n",
        "    return p < alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3058af0c",
      "metadata": {
        "id": "3058af0c"
      },
      "source": [
        "This last function was adapted for the lab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4da5ea1",
      "metadata": {
        "id": "e4da5ea1"
      },
      "source": [
        "最后这个函数是为该实验室改编的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "24227c11",
      "metadata": {
        "id": "24227c11",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def perform_test(perf1, perf2, name1, name2, sample_size=20, downsampling_fact=5, confidence_level=0.01):\n",
        "\n",
        "    perf1 = perf1.transpose()\n",
        "    perf2 = perf2.transpose()\n",
        "    nb_datapoints = perf1.shape[1]\n",
        "    nb_steps = perf1.shape[0]\n",
        "\n",
        "    legend = [name1, name2]\n",
        "\n",
        "    # what do you want to plot ?\n",
        "    id_central = 'mean' # \"median\"  #\n",
        "    id_error = 80  # (percentiles), also: 'std', 'sem'\n",
        "\n",
        "    test_id = \"Welch t-test\"  # recommended\n",
        "\n",
        "    sample1 = perf1[:, np.random.randint(0, nb_datapoints, sample_size)]\n",
        "    sample2 = perf2[:, np.random.randint(0, nb_datapoints, sample_size)]\n",
        "\n",
        "    steps = np.arange(0, nb_steps, downsampling_fact)\n",
        "    sample1 = sample1[steps, :]\n",
        "    sample2 = sample2[steps, :]\n",
        "\n",
        "    # test\n",
        "    sign_diff = np.zeros([len(steps)])\n",
        "    for i in range(len(steps)):\n",
        "        sign_diff[i] = run_test(\n",
        "            test_id, sample1[i, :], sample2[i, :], alpha=confidence_level\n",
        "        )\n",
        "\n",
        "    central1, low1, high1 = compute_central_tendency_and_error(\n",
        "        id_central, id_error, sample1\n",
        "    )\n",
        "    central2, low2, high2 = compute_central_tendency_and_error(\n",
        "        id_central, id_error, sample2\n",
        "    )\n",
        "\n",
        "    # plot\n",
        "    _, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
        "    lab1 = plt.xlabel(\"training steps\")\n",
        "    lab2 = plt.ylabel(\"performance\")\n",
        "\n",
        "    plt.plot(steps, central1, linewidth=10)\n",
        "    plt.plot(steps, central2, linewidth=10)\n",
        "    plt.fill_between(steps, low1, high1, alpha=0.3)\n",
        "    plt.fill_between(steps, low2, high2, alpha=0.3)\n",
        "    leg = ax.legend(legend, frameon=False)\n",
        "\n",
        "    # plot significative difference as dots\n",
        "    idx = np.argwhere(sign_diff == 1)\n",
        "    y = max(np.nanmax(high1), np.nanmax(high2))\n",
        "    plt.scatter(steps[idx], y * 1.05 * np.ones([idx.size]), s=100, c=\"k\", marker=\"o\")\n",
        "\n",
        "    # style\n",
        "    for line in leg.get_lines():\n",
        "        line.set_linewidth(10.0)\n",
        "    ax.spines[\"top\"].set_linewidth(5)\n",
        "    ax.spines[\"right\"].set_linewidth(5)\n",
        "    ax.spines[\"bottom\"].set_linewidth(5)\n",
        "    ax.spines[\"left\"].set_linewidth(5)\n",
        "\n",
        "    plt.savefig(\n",
        "        f\"./{name1}_{name2}.png\", bbox_extra_artists=(leg, lab1, lab2), bbox_inches=\"tight\", dpi=100\n",
        "    )\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1567ee0",
      "metadata": {
        "id": "a1567ee0"
      },
      "source": [
        "## Exercise 3\n",
        "\n",
        "As hyper-parameters, you will use:\n",
        "\n",
        "- naive tuning, that is a pair (0.5, 0.5) for the actor and critic learning rates,\n",
        "- the best hyper-parameters you found with the different tuning algorithms you used before.\n",
        "\n",
        "### 1. For each set of hyper-parameters, collect a large dataset of learning curves.\n",
        "\n",
        "We suggest using 150 training episodes.\n",
        "\n",
        "### 2. Perform statistical comparisons\n",
        "\n",
        "- Take two datasets of learning curves obtained with the hyper-parameters sets that you found with different tuning algorithms.\n",
        "- Use the ``` perform_test(...)``` function to compare each possible pair of sets.\n",
        "\n",
        "You should obtain an image for each pair you have tried.\n",
        "In this image, black dots signal the time step where there is a statistically significant difference between two learning curves.\n",
        "\n",
        " ### 3. Conclude."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e679640",
      "metadata": {
        "id": "6e679640"
      },
      "source": [
        "## 练习 3\n",
        "\n",
        "作为超参数，你将使用：\n",
        "\n",
        "- **简单调优**：即演员和评论家的学习率为 (0.5, 0.5) 的组合，\n",
        "- 使用前面不同调优算法找到的**最佳超参数**。\n",
        "\n",
        "### 1. 对每组超参数，收集大量学习曲线数据集。\n",
        "\n",
        "我们建议使用150个训练回合。\n",
        "\n",
        "### 2. 进行统计比较\n",
        "\n",
        "- 选取使用不同调优算法找到的超参数组所得到的两组学习曲线数据集。\n",
        "- 使用 `perform_test(...)` 函数比较每一对可能的超参数组。\n",
        "\n",
        "你应该为每一对超参数组生成一张图像。在图像中，**黑点**表示在某个时间步上，两个学习曲线之间存在**统计显著差异**。\n",
        "\n",
        "### 3. 总结分析。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5ad1626f",
      "metadata": {
        "id": "5ad1626f"
      },
      "outputs": [],
      "source": [
        "# To be completed...\n",
        "def collect_learning_curves(mdp, num_runs,nb_episodes, timeout, alpha_critic, alpha_actor, render):\n",
        "    learning_curves = []\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        v_list = []  # Store value norms or other performance metrics per episode\n",
        "\n",
        "        # Run Actor-Critic algorithm\n",
        "        v, v_list, _ = Actor_Critic(\n",
        "            mdp, nb_episodes,\n",
        "            timeout,\n",
        "            alpha_critic,\n",
        "            alpha_actor,\n",
        "            render\n",
        "        )\n",
        "\n",
        "        # Save learning curve for this run\n",
        "        learning_curves.append(v_list)\n",
        "\n",
        "    return np.array(learning_curves)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "89xNrzD01seB",
      "metadata": {
        "id": "89xNrzD01seB"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'optimized_save_path' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(optimized_save_path, optimized_curves)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[16], line 37\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m perform_test(perf1\u001b[38;5;241m=\u001b[39mnaive_curves, perf2\u001b[38;5;241m=\u001b[39mrandom_search_curves, name1\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNaive\u001b[39m\u001b[38;5;124m'\u001b[39m, name2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Search\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m np\u001b[38;5;241m.\u001b[39msave(naive_save_path, naive_curves)\n\u001b[0;32m---> 37\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[43moptimized_save_path\u001b[49m, optimized_curves)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optimized_save_path' is not defined"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Setup the environment\n",
        "    mdp = env\n",
        "\n",
        "    naive_curves = collect_learning_curves(mdp, num_runs=ac_params['nb_repeats'],\n",
        "        nb_episodes=150,\n",
        "        timeout=ac_params['timeout'],\n",
        "        alpha_critic=0.5,\n",
        "        alpha_actor=0.5,\n",
        "        render=ac_params['render'])\n",
        "\n",
        "    # Collect learning curves for optimized tuning (best_actor_lr, best_critic_lr)\n",
        "    grid_search_curves = collect_learning_curves(mdp,\n",
        "        num_runs=ac_params['nb_repeats'],\n",
        "        nb_episodes=150,\n",
        "        timeout=ac_params['timeout'],\n",
        "        alpha_critic=best_critic_lr,\n",
        "        alpha_actor=best_actor_lr,\n",
        "        render=ac_params['render'])\n",
        "\n",
        "\n",
        "    random_search_curves = collect_learning_curves(mdp,\n",
        "        num_runs=ac_params['nb_repeats'],\n",
        "        nb_episodes=150,\n",
        "        timeout=ac_params['timeout'],\n",
        "        alpha_critic=best_critic_lr_random,\n",
        "        alpha_actor=best_actor_lr_random,\n",
        "        render=ac_params['render'])\n",
        "\n",
        "    naive_save_path = os.path.join(log_dir, 'naive_curves.npy')\n",
        "    grid_search_save_path = os.path.join(log_dir, 'grid_search_curves.npy')\n",
        "    random_save_path = os.path.join(log_dir, 'random_curves.npy')\n",
        "\n",
        "    perform_test(perf1=naive_curves, perf2=grid_search_curves, name1='Naive', name2='Grid Search')\n",
        "    perform_test(perf1=naive_curves, perf2=random_search_curves, name1='Naive', name2='Random Search')\n",
        "    np.save(naive_save_path, naive_curves)\n",
        "    np.save(optimized_save_path, optimized_curves)\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "Q1NUg6we33q4",
      "metadata": {
        "id": "Q1NUg6we33q4"
      },
      "outputs": [],
      "source": [
        "def perform_test(curves1, curves2):\n",
        "    \"\"\"\n",
        "    Compare two sets of learning curves to find statistically significant differences.\n",
        "    curves1, curves2: numpy arrays of shape (num_repeats, nb_episodes)\n",
        "    Returns a list of indices (time steps) where the difference is statistically significant.\n",
        "    \"\"\"\n",
        "    significant_steps = []\n",
        "    for t in range(curves1.shape[1]):\n",
        "        # Apply a statistical test (e.g., t-test) to compare the two sets of curves at each time step\n",
        "        t_stat, p_value = ttest_ind(curves1[:, t], curves2[:, t])\n",
        "\n",
        "        # Use a significance level of 0.05\n",
        "        if p_value < 0.05:\n",
        "            significant_steps.append(t)\n",
        "\n",
        "    return significant_steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "aWTL2E7436Z4",
      "metadata": {
        "id": "aWTL2E7436Z4"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Load saved learning curves (optional if you've saved previously)\n",
        "naive_save_path = os.path.join(log_dir, 'naive_curves.npy')\n",
        "optimized_save_path = os.path.join(log_dir, 'optimized_curves.npy')\n",
        "naive_curves = np.load(naive_save_path)\n",
        "optimized_curves = np.load(optimized_save_path)\n",
        "\n",
        "# Perform the statistical comparison\n",
        "significant_steps = perform_test(naive_curves, optimized_curves)\n",
        "\n",
        "# Plot the results\n",
        "def plot_comparison(curves1, curves2, significant_steps, title, save_path):\n",
        "    mean1 = np.mean(curves1, axis=0)\n",
        "    mean2 = np.mean(curves2, axis=0)\n",
        "    std1 = np.std(curves1, axis=0)\n",
        "    std2 = np.std(curves2, axis=0)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot mean and standard deviation of both sets of curves\n",
        "    plt.plot(mean1, label='Naive Tuning (0.5, 0.5)', color='blue')\n",
        "    plt.fill_between(range(len(mean1)), mean1 - std1, mean1 + std1, color='blue', alpha=0.2)\n",
        "\n",
        "    plt.plot(mean2, label='Optimized Tuning', color='green')\n",
        "    plt.fill_between(range(len(mean2)), mean2 - std2, mean2 + std2, color='green', alpha=0.2)\n",
        "\n",
        "    # Highlight statistically significant differences\n",
        "    plt.scatter(significant_steps, mean1[significant_steps], color='black', label='Significant Difference', marker='o')\n",
        "\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Norm of Value Function')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Save the figure\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "# Plot and save the comparison result\n",
        "comparison_save_path = os.path.join(log_dir, 'comparison.png')\n",
        "plot_comparison(naive_curves, optimized_curves, significant_steps,\n",
        "                \"Naive vs Optimized Hyper-parameters\", comparison_save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9fae786",
      "metadata": {
        "id": "a9fae786"
      },
      "source": [
        "# Lab report\n",
        "\n",
        "Your report should contain:\n",
        "- your source code (probably this notebook), do not forget to put your names on top of the notebook,\n",
        "- in a separate pdf file with your names in the name of the file:\n",
        "    + a detailed enough description of the choices you have made: the parameters you have set, the libraries you have used, etc.,\n",
        "    + the heatmaps obtained using the hyper-parameters tuning algorithms that you have used,\n",
        "    + the figures resulting from performing Welch's T-test using the best hyper-parameters from the above approaches,\n",
        "    + your conclusion from these experiments.\n",
        "\n",
        "Beyond the elements required in this report, any additional studies will be rewarded.\n",
        "For instance, you can try using a Q-function as critic, using random search as hyper-parameters tuning algorithm,\n",
        "using more challenging environments, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87430ec7",
      "metadata": {
        "id": "87430ec7"
      },
      "source": [
        "# 实验报告\n",
        "\n",
        "你的报告应包含以下内容：\n",
        "- 你的源代码（通常是这个笔记本），不要忘记在笔记本的顶部写上你的名字，\n",
        "- 一个单独的 PDF 文件，文件名中包含你的名字：\n",
        "  + 详细描述你所做的选择：你设置的参数、使用的库等，\n",
        "  + 使用超参数调优算法生成的热图，\n",
        "  + 使用最佳超参数进行 Welch’s T 检验后得到的图像，\n",
        "  + 你从这些实验中得出的结论。\n",
        "\n",
        "除了报告中要求的内容，任何额外的研究都会获得加分。  \n",
        "例如，你可以尝试使用 Q 函数作为评论家，使用随机搜索作为超参数调优算法，使用更具挑战性的环境等。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "cell_markers": "\"\"\""
    },
    "kernelspec": {
      "display_name": "deepdac",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
