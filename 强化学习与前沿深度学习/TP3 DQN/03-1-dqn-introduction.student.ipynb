{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73968375",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    " Copyright © Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree.\n",
    "\n",
    " 这段源代码的许可协议是 MIT 许可协议，详细信息请参见根目录中的 LICENSE 文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55f8f0b",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "In this notebook, using BBRL, we code a simple version of the DQN algorithm\n",
    "without a replay buffer nor a target network so as to better understand the\n",
    "inner mechanisms.\n",
    "\n",
    "To understand this code, you need to know more about [the BBRL interaction\n",
    "model](https://github.com/osigaud/bbrl/blob/master/docs/overview.md) Then you\n",
    "should run [a didactical\n",
    "example](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/02-multi_env_noautoreset.student.ipynb)\n",
    "to see how agents interact in BBRL when autoreset=False.\n",
    "\n",
    "The DQN algorithm is explained in [this\n",
    "video](https://www.youtube.com/watch?v=CXwvOMJujZk) and you can also read [the\n",
    "corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/dqn.pdf).\n",
    "\n",
    "# 展望\n",
    "\n",
    "在本笔记本中，我们使用 BBRL 编写一个简单版本的 DQN 算法，不包含回放缓冲区和目标网络，以便更好地理解其内部机制。\n",
    "\n",
    "要理解这段代码，你需要更多地了解 [BBRL 交互模型](https://github.com/osigaud/bbrl/blob/master/docs/overview.md)。然后，你应该运行 [一个教学示例](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/02-multi_env_noautoreset.student.ipynb) 以查看在 `autoreset=False` 时，BBRL 中的智能体是如何交互的。\n",
    "\n",
    "DQN 算法的解释可以参考 [这个视频](https://www.youtube.com/watch?v=CXwvOMJujZk)，你还可以阅读 [相关幻灯片](http://pages.isir.upmc.fr/~sigaud/teach/dqn.pdf)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e96a520",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen_guanyu/deepdac/lib/python3.10/site-packages/bbrl_utils/notebook.py:46: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # noqa: F401\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the environment\n",
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "    assert run([\"pip\", \"install\", \"easypip\"]).returncode == 0, \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup(maze_mdp=True)\n",
    "\n",
    "import os\n",
    "\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents\n",
    "from bbrl_utils.algorithms import EpisodicAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d387744d",
   "metadata": {},
   "source": [
    "# Learning environment\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The learning environment is controlled by a configuration that define a few\n",
    "important things as described in the example below. This configuration can\n",
    "hold as many extra information as you need, the example below is the minimal\n",
    "one.\n",
    "\n",
    "# 学习环境\n",
    "\n",
    "## 配置\n",
    "\n",
    "学习环境由一个配置控制，该配置定义了一些重要的内容，如下面的示例所示。这个配置可以包含你需要的额外信息，以下示例为最小配置。\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    # This defines the a path for logs and saved models\n",
    "    \"base_dir\": \"${gym_env.env_name}/myalgo_${current_time:}\",\n",
    "\n",
    "    # The Gymnasium environment\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "\n",
    "    # Algorithm\n",
    "    \"algorithm\": {\n",
    "        # Seed used for the random number generator\n",
    "        \"seed\": 1023,\n",
    "\n",
    "        # Number of parallel training environments\n",
    "        \"n_envs\": 8,\n",
    "                \n",
    "        # Minimum number of steps between two evaluations\n",
    "        \"eval_interval\": 500,\n",
    "        \n",
    "        # Number of parallel evaluation environments\n",
    "        \"nb_evals\": 10,\n",
    "\n",
    "        # Number of epochs (loops)\n",
    "        \"max_epochs\": 40000,\n",
    "\n",
    "    },\n",
    "}\n",
    "\n",
    "# Creates the configuration object, i.e. cfg.algorithm.nb_evals is 10\n",
    "cfg = OmegaConf.create(params)\n",
    "```\n",
    "\n",
    "## The RL algorithm\n",
    "\n",
    "In this notebook, the RL algorithm is based on `EpisodicAlgo`, that defines\n",
    "the algorithm environment when using episodes. To use such environment, we\n",
    "just need to subclass `EpisodicAlgo` and to define two things, namely the\n",
    "`train_policy` and the `eval_policy`. Both are BBRL agents that, given the\n",
    "environment state, select the action to perform.\n",
    "\n",
    "## 强化学习算法\n",
    "\n",
    "在本笔记本中，强化学习算法基于 `EpisodicAlgo`，该类定义了使用回合的算法环境。为了使用这种环境，我们只需要继承 `EpisodicAlgo` 并定义两个内容，即 `train_policy` 和 `eval_policy`。这两个都是 BBRL 智能体，它们根据环境状态选择要执行的动作。\n",
    "\n",
    "```py\n",
    "  class MyAlgo(EpisodicAlgo):\n",
    "      def __init__(self, cfg):\n",
    "          super().__init__(cfg)\n",
    "\n",
    "          # Define the train and evaluation policies\n",
    "          # (the agents compute the workspace `action` variable)\n",
    "          self.train_policy = MyPolicyAgent(...)\n",
    "          self.eval_policy = MyEvalAgent(...)\n",
    "\n",
    "algo = MyAlgo(cfg)\n",
    "```\n",
    "\n",
    "The `EpisodicAlgo` defines useful objects:\n",
    "\n",
    "- `algo.cfg` is the configuration\n",
    "- `algo.nb_steps` (integer) is the number of steps since the training began\n",
    "- `algo.logger` is a logger that can be used to collect statistics during training:\n",
    "    - `algo.logger.add_log(\"critic_loss\", critic_loss, algo.nb_steps)` registers the `critic_loss` value on tensorboard\n",
    "- `algo.evaluate()` evaluates the current `eval_policy` if needed, and keeps the\n",
    "agent if it was the best so far (average cumulated reward);\n",
    "- `algo.visualize_best()` runs the best agent on one episode, and displays the video\n",
    "\n",
    "Besides, it also defines an `iter_episodes` is simple:\n",
    "\n",
    "\n",
    "`EpisodicAlgo` 定义了以下有用的对象：\n",
    "\n",
    "- `algo.cfg` 是配置\n",
    "- `algo.nb_steps`（整数）是自训练开始以来的步数\n",
    "- `algo.logger` 是一个记录器，用于在训练期间收集统计信息：\n",
    "  - `algo.logger.add_log(\"critic_loss\", critic_loss, algo.nb_steps)` 将 `critic_loss` 值记录到 TensorBoard\n",
    "- `algo.evaluate()` 评估当前的 `eval_policy`（如果需要），并保留如果它是迄今为止最好的智能体（平均累积奖励）\n",
    "- `algo.visualize_best()` 运行最好的智能体进行一个回合，并显示视频\n",
    "\n",
    "此外，它还定义了一个 `iter_episodes`，其功能简单：\n",
    "\n",
    "```py\n",
    "  # With episodes\n",
    "  for workspace in rl_algo.iter_episodes():\n",
    "      # workspace is a workspace containing transitions\n",
    "      # Episodes shorter than the longer one contain duplicated\n",
    "      # transitions (with `env/done` set to true)\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c6a5c",
   "metadata": {},
   "source": [
    "## Definition of agents\n",
    "\n",
    "The [DQN](https://daiwk.github.io/assets/dqn.pdf) algorithm is a critic only\n",
    "algorithm. Thus we just need a Critic agent (which is also used to output\n",
    "actions) and an Environment agent.\n",
    "\n",
    "### The critic agent\n",
    "\n",
    "The critic agent is an instance of the `DiscreteQAgent` class. We first build\n",
    "a deterministic neural network that takes the state as input (so it has one\n",
    "input neuron per state variable) and that outputs the Q-value of each action\n",
    "in that state (so it has one output neuron per action).\n",
    "\n",
    "As any BBRL agent, the DiscreteQAgent has a `forward()` function that takes a\n",
    "time state as input. This `forward()` function outputs the Q-values of all\n",
    "actions at the corresponding time step. Additionally, if the critic is used to\n",
    "choose an action, it also outputs the chosen action at the same time step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1574094",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 智能体的定义\n",
    "\n",
    "[DQN](https://daiwk.github.io/assets/dqn.pdf) 算法是一种仅使用评论员的算法。因此，我们只需要一个评论员智能体（同时用于输出动作）和一个环境智能体。\n",
    "\n",
    "### 评论员智能体\n",
    "\n",
    "评论员智能体是 `DiscreteQAgent` 类的一个实例。我们首先构建一个确定性的神经网络，该网络以状态作为输入（因此它有一个输入神经元对应每个状态变量），并输出该状态下每个动作的 Q 值（因此它有一个输出神经元对应每个动作）。\n",
    "\n",
    "像任何 BBRL 智能体一样，`DiscreteQAgent` 也有一个 `forward()` 函数，该函数以时间状态作为输入。这个 `forward()` 函数输出在对应时间步下所有动作的 Q 值。此外，如果评论员用于选择动作，它还会在同一时间步输出所选择的动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fae4a10",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class DiscreteQAgent(Agent):\n",
    "    \"\"\"BBRL agent (discrete actions) based on a MLP\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = build_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        \"\"\"An Agent can use self.workspace\"\"\"\n",
    "\n",
    "        # Retrieves the observation from the environment at time t\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "\n",
    "        # Computes the critic (Q) values for the observation\n",
    "        q_values = self.model(obs)\n",
    "\n",
    "        # ... and sets the q-values (one for each possible action)\n",
    "        self.set((\"q_values\", t), q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f89e138",
   "metadata": {},
   "source": [
    "#### Greedily choosing the action\n",
    "\n",
    "The ArgmaxActionSelector is in charge of choosing the action whose Q-value is\n",
    "the highest given the Q-values of all actions. We may use it when we do not\n",
    "want to explore.\n",
    "\n",
    "#### 贪婪地选择动作\n",
    "\n",
    "`ArgmaxActionSelector` 负责选择 Q 值最高的动作。在我们不想进行探索时，可以使用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4c65e2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ArgmaxActionSelector(Agent):\n",
    "    \"\"\"BBRL agent that selects the best action based on Q(s,a)\"\"\"\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        q_values = self.get((\"q_values\", t))\n",
    "        action = q_values.argmax(1)\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620e1e5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Creating an Exploration method\n",
    "\n",
    "As Q-learning, DQN needs some exploration to prevent too early convergence.\n",
    "Here we use the simple $\\epsilon$-greedy exploration method.\n",
    "It is implemented as an agent which chooses an action based on the Q-values.\n",
    "\n",
    "### 创建探索方法\n",
    "\n",
    "与 Q-learning 一样，DQN 需要一定的探索以防止过早收敛。在这里，我们使用简单的 $\\epsilon$-贪婪探索方法。它被实现为一个根据 Q 值选择动作的智能体。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7ac7c1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class EGreedyActionSelector(Agent):\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        # Retrieves the q values\n",
    "        # (matrix nb. of episodes x nb. of actions)\n",
    "        q_values: torch.Tensor = self.get((\"q_values\", t))\n",
    "        size, nb_actions = q_values.shape\n",
    "\n",
    "        # Flag\n",
    "        is_random = torch.rand(size) > self.epsilon\n",
    "        \n",
    "        # Actions (random / argmax)\n",
    "        random_action = torch.randint(nb_actions, size=(size,))\n",
    "        max_action = q_values.argmax(-1)\n",
    "\n",
    "        # Choose the action based on the is_random flag\n",
    "        action = torch.where(is_random, random_action, max_action)\n",
    "\n",
    "        # Sets the action at time t\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17647a",
   "metadata": {},
   "source": [
    "## Heart of the algorithm\n",
    "\n",
    "### Computing the critic loss\n",
    "\n",
    "The role of the `compute_critic_loss` function is to implement the Bellman\n",
    "backup rule. In Q-learning, this rule was written:\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha [ r(s_t,a_t) + \\gamma \\max_a\n",
    "Q(s_{t+1},a) - Q(s_t,a_t)]$$\n",
    "\n",
    "In DQN, the update rule $Q \\leftarrow Q + \\alpha [\\delta] $ is replaced by a\n",
    "gradient descent step over the Q-network.\n",
    "\n",
    "We first compute a target value: $ target = r(s_t,a_t) + \\gamma \\max_a\n",
    "Q(s_{t+1},a)$ from a set of samples.\n",
    "\n",
    "Then we get a TD error $\\delta$ by substracting $Q(s_t,a_t)$ for these\n",
    "samples, and we use the squared TD error as a loss function: $ loss = (target\n",
    "- Q(s_t,a_t))^2$.\n",
    "\n",
    "To implement the above calculation in BBRL, the difficulty is to properly deal\n",
    "with time indexes.\n",
    "\n",
    "The `compute_critic_loss` function receives rewards, q_values and actions as\n",
    "tensors that have been computed over a complete episode.\n",
    "\n",
    "We need to take `reward[1:]`, which means all the rewards except the first one\n",
    "because the reward from $(s_t, a_t)$ is $r_{t+1}$. Similarly, to get $\\max_a\n",
    "Q(s_{t+1}, a)$, we need to ignore the first of the max_q values, using\n",
    "`max_q[1:]`.\n",
    "\n",
    "Do not forget to apply .detach() when computing the values of $\\max_a\n",
    "Q(s_{t+1}, a)$, as **we do not want to apply gradient descent on this $\\max_a\n",
    "Q(s_{t+1}, a)$**, we only apply gradient descent to $Q(s_t, a_t)$ according to\n",
    "this target value. In practice, `x.detach()` detaches a computation graph from\n",
    "a tensor, so it avoids computing a gradient over this tensor.\n",
    "\n",
    "The `must_bootstrap` tensor is used as a trick to deal with terminal states,\n",
    "as explained\n",
    "[here](https://github.com/osigaud/bbrl/blob/master/docs/time_limits.md) In\n",
    "practice, `must_bootstrap` is the logical negation of `terminated`. In the\n",
    "autoreset=False version we use full episodes, thus `must_bootstrap` is always\n",
    "True for all steps but the last one.\n",
    "\n",
    "To compute $Q(s_t,a_t)$ we use the [`torch.gather()`](https://pytorch.org/docs/stable/generated/torch.gather.html) function. This function is\n",
    "a little tricky to use, see [this\n",
    "page](https://github.com/osigaud/bbrl/blob/master/docs/using_gather.md) for\n",
    "useful explanations.\n",
    "\n",
    "In particular, the q_vals output that we get is not properly conditioned,\n",
    "hence the need for the `qval[:-1]` (we ignore the last dimension). Finally we\n",
    "just need to compute the difference target - qvals, square it, take the mean\n",
    "and send it back as the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d0361f",
   "metadata": {},
   "source": [
    "### 算法核心\n",
    "\n",
    "#### 计算批评损失\n",
    "\n",
    "`compute_critic_loss` 函数的作用是实现 Bellman 回溯规则。在 Q-learning 中，这个规则被写作：\n",
    "\n",
    "$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r(s_t, a_t) + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]$$\n",
    "\n",
    "在 DQN 中，更新规则 $Q \\leftarrow Q + \\alpha [\\delta]$ 被替换为对 Q 网络进行梯度下降步骤。\n",
    "\n",
    "我们首先从一组样本中计算目标值：$ target = r(s_t, a_t) + \\gamma \\max_a Q(s_{t+1}, a)$。\n",
    "\n",
    "然后，通过从这些样本中减去 $Q(s_t, a_t)$ 来获得 TD 误差 $\\delta$，并使用平方 TD 误差作为损失函数：$ loss = (target - Q(s_t, a_t))^2$。\n",
    "\n",
    "在 BBRL 中实现上述计算的难点在于正确处理时间索引。\n",
    "\n",
    "`compute_critic_loss` 函数接收的奖励、Q 值和动作作为张量计算，已在完整的一个 episode 中获得。\n",
    "\n",
    "我们需要使用 `reward[1:]`，这意味着除第一个外的所有奖励，因为 $(s_t, a_t)$ 的奖励是 $r_{t+1}$。类似地，为了获得 $\\max_a Q(s_{t+1}, a)$，我们需要忽略第一个 `max_q` 值，使用 `max_q[1:]`。\n",
    "\n",
    "不要忘记在计算 $\\max_a Q(s_{t+1}, a)$ 时应用 `.detach()`，因为 **我们不想对这个 $\\max_a Q(s_{t+1}, a)$ 应用梯度下降**，我们只对 $Q(s_t, a_t)$ 根据这个目标值应用梯度下降。在实践中，`x.detach()` 从张量中分离计算图，这样可以避免在该张量上计算梯度。\n",
    "\n",
    "`must_bootstrap` 张量用于处理终止状态的技巧，如 [这里](https://github.com/osigaud/bbrl/blob/master/docs/time_limits.md) 所解释的。在 `autoreset=False` 版本中，我们使用完整的 episodes，因此 `must_bootstrap` 对于除了最后一步以外的所有步骤都是 True。\n",
    "\n",
    "为了计算 $Q(s_t, a_t)$，我们使用 [`torch.gather()`](https://pytorch.org/docs/stable/generated/torch.gather.html) 函数。这个函数的使用有点复杂，参见 [这个页面](https://github.com/osigaud/bbrl/blob/master/docs/using_gather.md) 以获取有用的说明。\n",
    "\n",
    "特别地，得到的 q_vals 输出没有正确条件化，因此需要 `qval[:-1]`（我们忽略最后一个维度）。最后，我们只需计算 target - qvals 的差异，平方它，取平均值，并将其作为损失返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f1232d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_critic_loss(\n",
    "    cfg,\n",
    "    reward: torch.Tensor,\n",
    "    must_bootstrap: torch.Tensor,\n",
    "    done: torch.Tensor,\n",
    "    q_values: torch.Tensor,\n",
    "    action: torch.LongTensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the temporal difference loss from a dataset to\n",
    "    update a critic\n",
    "\n",
    "    For the tensor dimensions:\n",
    "\n",
    "    - T = maximum number of time steps\n",
    "    - B = number of episodes run in parallel\n",
    "    - A = action space dimension\n",
    "\n",
    "    :param cfg: The configuration\n",
    "    :param reward: A (T x B) tensor containing the rewards\n",
    "    :param must_bootstrap: a (T x B) tensor containing 0 at (t, b) if the\n",
    "        episode b was terminated at time $t$ (or before)\n",
    "    :param done: a (T x B) tensor containing 0 at (t, b) if the\n",
    "        episode b is truncated or terminated at time $t$ (or before)\n",
    "    :param q_values: a (T x B x A) tensor containing the Q-values at each time\n",
    "        step, and for each action\n",
    "    :param action: a (T x B) long tensor containing the chosen action\n",
    "\n",
    "    :return: The DQN loss\n",
    "    \"\"\"\n",
    "    # We compute the max of Q-values over all actions and detach (so that this\n",
    "    # part of the computation graph is not included in the gradient\n",
    "    # backpropagation)\n",
    "\n",
    "    # Compute the loss\n",
    "\n",
    "    max_q = q_values.max(-1).values.detach()   # results in a (T x B) tensor\n",
    "\n",
    "    # To get the max of Q(s_{t+1}, a), we take max_q[1:]\n",
    "    # The same about must_bootstrap.\n",
    "    target = (\n",
    "        reward[1:]\n",
    "        + cfg.algorithm.discount_factor * max_q[1:] * must_bootstrap[1:].int()\n",
    "    )\n",
    "\n",
    "    # To get Q(s_t, a_t), we use torch.gather along the 3rd dimension (the action)\n",
    "    qsa_t = q_values.gather(2, action.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Compare the temporal difference (use done as to mask out finished espisodes)\n",
    "    not_done = (~done[:-1]).int()\n",
    "    td = (target - qsa_t[:-1]) ** 2 * not_done\n",
    "    critic_loss = td.sum() / not_done.sum()\n",
    "\n",
    "\n",
    "    # assert False, 'Not implemented yet'\n",
    "\n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156afd9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Main training loop\n",
    "\n",
    "Note that everything about the shared workspace between all the agents is\n",
    "completely hidden under the hood. This results in a gain of productivity, at\n",
    "the expense of having to dig into the BBRL code if you want to understand the\n",
    "details, change the multiprocessing model, etc.\n",
    "\n",
    "The next cells defines a `EpisodicDQN` that deals with various part of the\n",
    "training loop.\n",
    "\n",
    "## 主训练循环\n",
    "\n",
    "请注意，所有关于各个智能体之间共享工作空间的细节都完全隐藏在底层。这提高了生产力，但如果你想了解细节、修改多线程模型等，就需要深入研究 BBRL 代码。\n",
    "\n",
    "接下来的代码单元定义了 `EpisodicDQN` 类，该类处理训练循环的各个部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2475bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicDQN(EpisodicAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        # Get the observation / action state space dimensions\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "\n",
    "        # Our discrete Q-Agent\n",
    "        self.q_agent = DiscreteQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.hidden_size, act_size\n",
    "        )\n",
    "\n",
    "        # The e-greedy strategy (when training)\n",
    "        explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n",
    "\n",
    "        # The training agent combines the Q agent\n",
    "        self.train_policy = Agents(self.q_agent, explorer)\n",
    "\n",
    "        # The optimizer for the Q-Agent parameters\n",
    "        self.optimizer = setup_optimizer(self.cfg.optimizer, self.q_agent)\n",
    "\n",
    "        # ...and the evaluation policy (select the most likely action)\n",
    "        self.eval_policy = Agents(self.q_agent, ArgmaxActionSelector())\n",
    "\n",
    "    def run(self):\n",
    "        for train_workspace in self.iter_episodes():\n",
    "            q_values, terminated, done, reward, action = train_workspace[\n",
    "                \"q_values\", \"env/terminated\", \"env/done\", \"env/reward\", \"action\"\n",
    "            ]\n",
    "\n",
    "            # Determines whether values of the critic should be propagated\n",
    "            # True if the episode reached a time limit or if the task was not done\n",
    "            # See https://github.com/osigaud/bbrl/blob/master/docs/time_limits.md\n",
    "            must_bootstrap = ~terminated\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = compute_critic_loss(\n",
    "                self.cfg, reward, must_bootstrap, done, q_values, action\n",
    "            )\n",
    "\n",
    "            # Store the loss for tensorboard display\n",
    "            self.logger.add_log(\"critic_loss\", critic_loss, self.nb_steps)\n",
    "            dqn.logger.add_log(\"q_values/min\", q_values.max(-1).values.min(), dqn.nb_steps)\n",
    "            dqn.logger.add_log(\"q_values/max\", q_values.max(-1).values.max(), dqn.nb_steps)\n",
    "            dqn.logger.add_log(\"q_values/mean\", q_values.max(-1).values.mean(), dqn.nb_steps)\n",
    "\n",
    "            # Gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.q_agent.parameters(), self.cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Evaluate the current policy (if needed)\n",
    "            self.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1793c5d6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 752), started 5 days, 17:28:59 ago. (Use '!kill 752' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-98a1b06085c48314\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-98a1b06085c48314\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We setup tensorboard before running DQN\n",
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e244851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 10:50:36.685001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-06 10:50:36.802964: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-06 10:50:36.835943: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-06 10:50:37.063807: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-06 10:50:39.651639: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"save_best\": False,\n",
    "    \"base_dir\": \"${gym_env.env_name}/dqn-simple-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": True,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 3,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"epsilon\": 0.1,\n",
    "        \"n_envs\": 8,\n",
    "        \"eval_interval\": 5_000,\n",
    "        \"max_epochs\": 500,\n",
    "        \"nb_evals\": 10,\n",
    "        \"discount_factor\": 0.99,\n",
    "        \"architecture\": {\"hidden_size\": [256, 256]},\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPole-v1\",\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 2e-3,\n",
    "    },\n",
    "}\n",
    "\n",
    "dqn = EpisodicDQN(OmegaConf.create(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a58e645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c114fe18fe84558b0cfea663d4e2fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video of best agent recorded in outputs/CartPole-v1/dqn-simple-S3_20241006-105035/best_agent.mp4\n",
      "Moviepy - Building video /home/chen_guanyu/M2A/M2A_RLD/outputs/CartPole-v1/dqn-simple-S3_20241006-105035/best_agent.mp4.\n",
      "Moviepy - Writing video /home/chen_guanyu/M2A/M2A_RLD/outputs/CartPole-v1/dqn-simple-S3_20241006-105035/best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chen_guanyu/M2A/M2A_RLD/outputs/CartPole-v1/dqn-simple-S3_20241006-105035/best_agent.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div align=middle><video src='data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAASKttZGF0AAACrwYF//+r3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1OSByMjk5MSAxNzcxYjU1IC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAxOSAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTEyIGxvb2thaGVhZF90aHJlYWRzPTIgc2xpY2VkX3RocmVhZHM9MCBucj0wIGRlY2ltYXRlPTEgaW50ZXJsYWNlZD0wIGJsdXJheV9jb21wYXQ9MCBjb25zdHJhaW5lZF9pbnRyYT0wIGJmcmFtZXM9MyBiX3B5cmFtaWQ9MiBiX2FkYXB0PTEgYl9iaWFzPTAgZGlyZWN0PTEgd2VpZ2h0Yj0xIG9wZW5fZ29wPTAgd2VpZ2h0cD0yIGtleWludD0yNTAga2V5aW50X21pbj0yNSBzY2VuZWN1dD00MCBpbnRyYV9yZWZyZXNoPTAgcmNfbG9va2FoZWFkPTQwIHJjPWNyZiBtYnRyZWU9MSBjcmY9MjMuMCBxY29tcD0wLjYwIHFwbWluPTAgcXBtYXg9NjkgcXBzdGVwPTQgaXBfcmF0aW89MS40MCBhcT0xOjEuMDAAgAAAAhNliIQAJ//+9bF8CmrJ84oM6DIu4Zckya62IuJtAMAAJShQAAADABLJlCWzIByZd0AAABMgA4ofQZQeYiYqxUCV4g4GauIBCHqhecfbJSxr5DaUickAVLEpEWwT3iE3s5+iXwVsgKNS8K15k37mJ+E+v7L2qr3n0PfFjZnwWM79wfJ0SGleNUows9jKsu4Dw5y8GdF+S9HyXV2N8LR0YkzbOHxrFXUNUAGO/XMVOldzZCiSShxruZZ1UXAAVeM/8V4Kcfa31zdjMgCQEj0V5OuqvUpO9xNs5c8Wre4FRnRXjIVJ4JmmwS61YB3kAAMKeizg0v3PHzPnpfg2soxzUj+/pIySBaYvhUV1yILZUqu9oXtfzNpxC90grEpZYNLbJFK6Jkzxfht1XNIbyDOQAR01ToXqr1ncdTvvsEuDpMJ0hKlhYmEzRhvvPMqvXL+yoqrRXc6QcEp8zROlwpWDkCsF2eD8WS0p6g+Oz3Qgf1CY/WdaMwitpd/X/kjFK/gw4wgxdhHrgWDbSMyoP8jKzIeOM6JgVciu0jEW92+4v+1jx5LpddfzgGMDNDCWbhWJ7ESg38+LrUK35XEil2wEJS2/m09kG1hvIAAAAwEOJiSaJ5K45UiLQ7tERTX/G7CO7h56rMEm1WNsGaowYwvJP89Zs5DdpRdKR483aEJz5dJ8rSptWjUEI7ZghoAAAAMAAAMAEjEAAABXQZokbEK//jhAAAENGlOtEWA+rl8YZrT+nSOwPt5oAbDbz1v/rDEPn03f+/7s+lKFA8wjhSvy4VHYgh1OFlbzjyoExHeAIefOnxm6KslEFxUMwXpnhihAAAAANUGeQniEfwAAFr49ZpT9AemQO4j06OxH0/tACth6NhVYk0hUN+GBrMu1griGrZgjGjX9UCNhAAAAJAGeYXRH/wAAIqw5B3FfFu2CZtbQp+awSjIalIbVPJjRU6bPgAAAAB4BnmNqR/8AACO/BCQQ8i4fK4wD5zplBc/e/olEH/EAAAAhQZpoSahBaJlMCE///fEAAAMABUfYi2CM7vsCC7nSZ5nhAAAAGEGehkURLCP/AAADAC1qtWqchwgiSw1Q4QAAABIBnqV0R/8AAAMAR1h5jTyDFBEAAAAOAZ6nakf/AAADAAADAakAAABMQZqsSahBbJlMCE///fEAAAMCnwLw4FNx5e/upfkAM3k1G340NQq9WS+q//gUsDRZZ1Ip2MEqPnIFEKuPtZkNeR7gsKqZ+HT3dUxHwAAAACxBnspFFSwj/wAAFrxDzbcV5z5JX9RaXeGhQyA7RBqF0cMfAb3Dm+4hBFLjoQAAAB0Bnul0R/8AAAUf0MHOtoxEiGzrwLCgisw+ZV9twAAAAB4BnutqR/8AACO/BCcSyWmXFo0cDqBJf/RFTarckkwAAABrQZrwSahBbJlMCE///fEAAAMAI7ppxtan/TdkBgdAoAiFTuKsTWYUMzsfpYehr0nH/4ow4nwViUPCHa2UmAJpAShlj0jJD9Ca55oNo6AK6WJ3/7U+Fm1zFj7f9eRGby0RsbMFgN13vrYyEIEAAAAsQZ8ORRUsI/8AAAMDG+5Ft0Om52QOxachwPNVPEDDpe3uS1YD+GIRYX45o4EAAAAfAZ8tdEf/AAADAduyJh/ZreucRPn1fahlcda4xartgQAAABwBny9qR/8AAAT7NcvmoM5RZAPnsgYYXSEW4R8wAAAAa0GbNEmoQWyZTAhf//6MsAAARiXBc8Cxxan72wAiongVDeWUE48Hh0P6GdeP6PlJcQhA7wAgEHlqphfyHNsaEbL4QJb8mcFXXMh7be2xR2oH3h5R0Tpx4OpHuSm6R2OwdnRn7z11mzT6JBz4AAAAI0GfUkUVLCP/AAAWvEPNtxXOWlMyDv/iWiTyZkg17AhQCM+BAAAAKgGfcXRH/wAABRtLuKHy1X/FhoEYAAWxlrHonFlVe7MfRZLg3E72oJ7+zAAAABkBn3NqR/8AACO/BCcSyWmauFZVFuiS0YLgAAAAVEGbeEmoQWyZTAhX//44QAAAZPz4P3bc81e9WILqcYl0N6wk2OzsorYNwMMfjvNVPAJbKFPHLiW6z2SVe+aglbQhViwWZMrdnzKWiUFZfzvFcrnkyQAAAClBn5ZFFSwj/wAACC8nNmF058YtgLAARDUMbErWT2Yct9m8ScReTFx74AAAABIBn7V0R/8AAAMAGlwoXN2/xMEAAAAfAZ+3akf/AAANNJ2rY2rSLCD6NUIH8MI8VN+1ZvJbQQAAABdBm7xJqEFsmUwIR//94QAAAwAAAwAxYAAAABJBn9pFFSwj/wAAAwAAUgJ+WLEAAAAOAZ/5dEf/AAADAAADAakAAAApAZ/7akf/AAADALfWH5rUSWqRgY/ZdBUirdcRwAQ29et75OOd7kMS7akAAABYQZv/SahBbJlMCFf//jhAAAEM+jKADXpNAJSIHUJSCDNAtsrKMUXvxYwRvReuO/ijX6qRKIL6OTyNUPgjc61usVIozvHgUcS4NCeFgVnNAmciQkvBli0tQwAAACZBnh1FFSwj/wAAFrVYVTRDQyq+FeqLCuxCAdaJDeuZ+zKygePHBgAAACQBnj5qR/8AACPHxrpjPGekOOnsUmhl1o6WdaUnPt7WOBvToIAAAAAmQZojSahBbJlMCE///fEAAAMAYn03X1eOurL+/rzJ+sDMLRDONK0AAAAhQZ5BRRUsI/8AAAMDObvslhcQkk2M+zP460cy+qEBCoqAAAAAGgGeYHRH/wAABRxbeZLHx/9Faoh81gJfcICNAAAAGgGeYmpH/wAABR0+YOJnHII0GnX3HMWRLZUYAAAANEGaZ0moQWyZTAhP//3xAAADAp8C8OA0ZiwCmw2wYi6gkT4b0SDk9ia4oWSewPXFMc774EEAAAAYQZ6FRRUsI/8AABa8TmzC2EsLS+iFPWfBAAAADgGepHRH/wAAAwAAAwGpAAAAFAGepmpH/wAAI7IrBxM44aoUpfWfAAAAQEGaq0moQWyZTAhX//44QAAAJqc4PfKRdcrf9lwwTqYB6AEduyun0X/6tDDk7O2ESnaHVPtlc0MIBWPS5v4b5RgAAAAgQZ7JRRUsI/8AAAMDOFMKsVnID4wUlQGIFcNwKplquBAAAAAaAZ7odEf/AAAFHFt5ksfH/8YuoiA5MdojdwMAAAARAZ7qakf/AAADAAPNA+cQUw8AAAAeQZrvSahBbJlMCE///fEAAAMAALpnwOQDMLJ88RnXAAAAFUGfDUUVLCP/AAADAAZIp9PVJiDtQQAAABIBnyx0R/8AAAMACfC5wofvr4EAAAAQAZ8uakf/AAADAAPM/8VnwQAAAEZBmzNJqEFsmUwIV//+OEAAAQz6JbkUEAAW0quDS0fWwUxNkVXy5mS2M/wMCfKkapHaC4zIw1khM0pgiD0z1A11dAldab84AAAAL0GfUUUVLCP/AAAWtVhVNENDKr51ETYANo8p8j3j12ehnn5jdvHqiLuLDv5Nv9swAAAAFwGfcHRH/wAAI6w5Ba5PHtnnMwe/UlxBAAAAGwGfcmpH/wAABR0+YOBt5kDv70QwpGi15ZpCYAAAAEpBm3dJqEFsmUwIT//98QAAAwAjr+gfFFK3nwzWadWqAxwpuTh3TQE6yfdav/vCYVQ2JxbglRKNePdK1Mrd/PT4YV6ZGDPjTrAdDgAAACBBn5VFFSwj/wAAAwE15ObMLYUmaa22OHWQabAiBFEhMQAAABEBn7R0R/8AAAMAA8sZduH5iwAAABwBn7ZqR/8AAAMB5c1GjgV1lp5q/VdAXOHjaZ/TAAAANUGbu0moQWyZTAhP//3xAAADAp8C8OAhlEIWmmh2waRqgsisFJdOakC8oVRjKZ/8d+ebf919AAAANEGf2UUVLCP/AAAWtVhXMucSL9sQVt+tPQaELIjud7OcT6+qkm/TMHS/czMgkscge6GckS8AAAAbAZ/4dEf/AAAjrDkFrk8fPxWmp3lv3II0KZyNAAAAKwGf+mpH/wAABUXZ4wuS0ca8JtxugNZO0qXHc4E1Ohlbj39Yi6zafOtRI2YAAABYQZv/SahBbJlMCFf//jhAAAEW6JrAWqlQHYoS5l/QdDNZcOVfViHmTsg5V4AcVTJaeh8Kb6GYW5oBuCn1GyJY3OFDRK499pqva95Wup1c1k9xMJ58u9LtHwAAACVBnh1FFSwj/wAAFrxObR45xIv2y0PWs9Bo1XO53tRDfoj/eoKbAAAAIAGePHRH/wAABUXZ4wuS/vR23G7naib+TbFdNWdPioOAAAAAFgGePmpH/wAAI78aznveCbYrGdQoBAwAAAAfQZojSahBbJlMCE///fEAAAMAABm1XAxFK+24pnEaiwAAACRBnkFFFSwj/wAAAwMv7kEEeFXAsrWJhZvl2UueHFjIVPjwlakAAAAZAZ5gdEf/AAAFHFt5ksfH/8YnsOt6HF0iwQAAABgBnmJqR/8AAAUdPmDiZxyGOjIbeFnVyLAAAABQQZpnSahBbJlMCE///fEAAAMCsQKe3hwDUXdywzFvYVo/ELLYl4c36X8ctzl5TvAO64eouJ/iu1WPRRM+09DRJRi15/hGCR2OH/yYKDsvxv8AAAAhQZ6FRRUsI/8AABdMQ823Fc5aF5PjzBtjTUW+61l3wAdfAAAAGQGepHRH/wAABRxbeZLHx//GJ7DrehxdIsEAAAAZAZ6makf/AAAkvwQnEslpmrhWVRbOuRqHgQAAADhBmqtJqEFsmUwIT//98QAAAwAN35nIa+JAEEDjL1FVVDJRkEn5DAtJ1Vxt8B+4TCcImlefvjQwtAAAAChBnslFFSwj/wAAAwM5st0AANMZCcg/5nY5HeR+flTvxOWTPS4/3rUgAAAAGwGe6HRH/wAABRxbeevo2FovJXBOb9WvnCLUgQAAABABnupqR/8AAAUfNNW6gFBAAAAAT0Ga70moQWyZTAhX//44QAABFPoygCHDAgqoM9ym7wpd07gCnHdY5CsqXYqAoj3Fv2z5w0gMRNQ5aRGqV6/akLF8AJZKV72gZLRZNwKJf4AAAAAlQZ8NRRUsI/8AABdFKzdrWrL3Fc8w0RDuHfbLHAn+EXu0LfTggQAAABgBnyx0R/8AACTDF2ippc2ngYMJgEnjBJ0AAAAaAZ8uakf/AAAFHT5ht2RnbRcJyb7+27VqRYEAAAAXQZszSahBbJlMCE///fEAAAMAAAMAHpAAAAAQQZ9RRRUsI/8AAAMAAAMBBwAAAA4Bn3B0R/8AAAMAAAMBqQAAAA4Bn3JqR/8AAAMAAAMBqQAAADJBm3dJqEFsmUwIT//98QAAAwKxApcaRq4xwYXP6V7vANhspsc4AEsmgmxUB0biyZ7D7gAAAB5Bn5VFFSwj/wAAF0UrN222gBLaq7P6WXOwJQGjQbUAAAAXAZ+0dEf/AAAkwxdoqaVT1PvCdscQ1IAAAAAPAZ+2akf/AAAFHzUKAJuBAAAAVEGbu0moQWyZTAhP//3xAAADArHt3X4rIdwM/2VGdQofBKI7cadPNQWagA2tX9yhCgmYZ8GPKPDHawsKiyN2fBEq6BWNcnk4sul66Fq0PIeSy2AJhwAAACtBn9lFFSwj/wAAF0xDzbcmgABdaqvf8FWytTUUBplrS8jA41KScIDEbjggAAAAHQGf+HRH/wAAAwHlinZpU/cDsV0+dKwLBqVelg9xAAAAGQGf+mpH/wAAJL8EJxLJcEZHOFITUDIEgk4AAAAyQZv/SahBbJlMCE///fEAAAMAX/zPvT+w9uVgDQOZUIuce6MPDRx5ENnqXcjpw/OVpLEAAAAdQZ4dRRUsI/8AAAMDOFMK5evRG/XPztOrAiTwrUkAAAAbAZ48dEf/AAAFHFt56+jYWi8lcE5v1a+cItSAAAAADgGePmpH/wAAAwAAAwGpAAAAQkGaI0moQWyZTAhP//3xAAADArECntq6aNIACDAyYRziXvQUtovQP1JyI5eYc/wKrG0FIqajnvzqII+OWwOz/KPOPQAAACRBnkFFFSwj/wAAF0xDzbcV5z5JX9TNRz+1JhGB6unXBvz5ZFgAAAAcAZ5gdEf/AAAFQFtYIi4TEVi6CMHYnKnEBvd7gQAAACQBnmJqR/8AACSxzAlvqr3z+EdgZ1se4/e+Kw0c0q/LqYFx14AAAAA2QZpnSahBbJlMCE///fEAAAMCsbp12w0ucQuEmeuNp98GzaABua+qtcCejItDb29K+dWHtAm5AAAAMUGehUUVLCP/AAAXRSs3eIk1wAE6n45ZvYlqJb4ooCKFqa5R7lKaiK4GgMORIc6M2pEAAAAhAZ6kdEf/AAAkwxdoqaV5PL3o0FqeoEKw6gFUF5WdJDODAAAAHAGepmpH/wAABUEwDhoUllQmvWE50lAOdjdwKm0AAAAzQZqrSahBbJlMCE///fEAAAMADXx1xUeRZtRyh/tapjQAUJrm4y3psvEWbqrHtUBbEvgkAAAAHkGeyUUVLCP/AAADAHQiPpUMWbGilE0b3PseBBNqQAAAAA4Bnuh0R/8AAAMAAAMBqQAAABwBnupqR/8AAAUOEcfmK2mMswcZl1N+RJ9zzHBAAAAAUEGa70moQWyZTAhP//3xAAADArEDCZBStwzhaGlrquogq0eRk7q1riUN0RRfWTUp9TmrzP8ftQysSBVI9nV4Ui4/uGaUUdCST6NQicWmbUAQAAAAKEGfDUUVLCP/AAAXRSs3a1qy9xW7/bvBhW/ZkFFMnQ8//wdo1PstXYEAAAAXAZ8sdEf/AAAkq/hbeBKdaCOJf+asFbEAAAAaAZ8uakf/AAAFHT5ht2RnbRcJyb7+27VqRYEAAABUQZszSahBbJlMCFf//jhAAAEVTydpQJ/EzynOyukKh16fCCvpSHUJFMnDROGaagG3skKm/zb1jt93/iq8XkTsSGPe0UA06CF5ZSUgaYZD9TKTTRP8AAAALUGfUUUVLCP/AAAXTEPNtxU2sanftyWABF9xm+PMWmAYDCB4dYpWRWmPvJtrwAAAACABn3B0R/8AAAVAXEsIOUL2+fwV2pJcg47maOYWXOFrwQAAAB0Bn3JqR/8AACS/BCcSyWmXFo0gNG++Y+TXc79mfAAAACFBm3ZJqEFsmUwIT//98QAAAwANj93pAQld10ae+ZzQR8AAAAAnQZ+URRUsI/8AAAMDSiDeem/zh3aeR7jIn1yHuecu6iksZQf/K3vdAAAAHAGftWpH/wAABUM01Z/0NBbIHUVuDMZ9+/vEtqQAAAA/QZu6SahBbJlMCE///fEAAAMCsQKtNktCcIPptxIYAA42zb96Kx29qk7I5/Oq1bXyexMeIdFORkIcHno3BsTdAAAAHkGf2EUVLCP/AAAXRSs3U7qa4yTTdvWeS5w1mRoCmwAAABkBn/d0R/8AACSr+Ft4Ep1oJFg8D4RqDB8wAAAAEAGf+WpH/wAAAwC6ZqFAh4EAAAA0QZv+SahBbJlMCE///fEAAAMCse3dfish3Az/iydEvZFE5wl6hSZUhv5o1v+JSAsJezuPIAAAADBBnhxFFSwj/wAAF0UrN420F0AWM8uarsLA8MjKZ4tSPNr2cbdE1Lb7oZstd3rMPcEAAAAoAZ47dEf/AAAkq/hbdu0j7Q9dPmPuY1QNrqfJFsmlu4MjhP1LcAgPcQAAABwBnj1qR/8AAAMB5oAS+OBpuK11QpcJO2ZW4OCAAAAAYkGaIkmoQWyZTAhP//3xAAADArKzhcjDvlt36AEspMtvfE5dh5YYhKqZ9ETebncoAzMsr2UL/cKCe8iwytUuwc3GC8N0OTuoFfcKd+LvJgZjtGxpbrs3uctvXG+XNk2wDTOAAAAAL0GeQEUVLCP/AAAXRSs3jbPOAVG7/KK0fEUPD7ZfPgl0y9E4+qnJxUvXIspi1VFhAAAAFwGef3RH/wAAJMMXaKmlU9T7wnbHENSAAAAAHAGeYWpH/wAADYSdq2Nq0iwgGQI8v9wcjh9GEPcAAABBQZpmSahBbJlMCE///fEAAAMCse3fBQ15R/U8SUL6HPw7RHy2Nnw4ez38FonFRSTVXk9J6IaLeUaU3dVap4FKwaAAAAAeQZ6ERRUsI/8AABdMTmzC2EsLSp4iwXDQHGLgvHuBAAAADgGeo3RH/wAAAwAAAwGpAAAALAGepWpH/wAAJLIrBcCtxu+OcwALhh2C7qMtMRFl26IbgImb114S/PrgQstTAAAAL0GaqkmoQWyZTAhX//44QAABFThp38pF1yt/06JOGUp55qeO0/0Ge7u2sE/h6dxJAAAAHkGeyEUVLCP/AAAXTE5swthLC0qeNO9hY2GPH7YRYAAAAA4Bnud0R/8AAAMAAAMBqQAAABwBnulqR/8AACS/Gs573gm2YrFYhHZBZuQFD+6BAAAAPEGa7kmoQWyZTAhP//3xAAADACTEnj4dhxXJrABrcNxtoHpTulavM/A8QgUxfJgWXUp7EJbbPK3+F5XwngAAACBBnwxFFSwj/wAAAwE+xQtvTI3PcuNITy4jSC7DiDC7gAAAAA4Bnyt0R/8AAAMAAAMBqQAAABwBny1qR/8AAAMB8M3xm6Wa5/vmNHMT7kUtcr+pAAAAP0GbMkmoQWyZTAhP//3xAAADArD25qfVgthainPmeYlyngOHWACWrgoqBeSDjpHqm47Ux0IEflSGEw7w3t0cEQAAACpBn1BFFSwj/wAAF0NIi97p4AG9pLn5i0ReD4BQ6dkMIdp8B7PZcYGegsAAAAAlAZ9vdEf/AAAkrDkEgOejkBfc9DAtBL6UcuXjGQHrExHVQL5u4AAAABwBn3FqR/8AACS/Gs573gmR6QzrPba47g1ddZ9NAAAASUGbdkmoQWyZTAhP//3xAAADAsfJ9t0l+F/e4pqkVnlBLIku1QIyZ8b0s7cLKABhbfQRbcnzIm04Mv81EcX4BXdoNbDnKJcjnYAAAAAdQZ+URRUsI/8AAAMDTFOC+6BREbHsdHSiuAfY/0wAAAAcAZ+zdEf/AAADAL9+cKmKpPa7r/E/4RAqNZeILQAAAA4Bn7VqR/8AAAMAAAMBqQAAAE9Bm7pJqEFsmUwIT//98QAAAwLC9thnit+JPYADjypX7o2sv5YWLYlnP2wWreb/BmT/sKtjF2HQwhbGlT/Ce88VxUElXBs5u9e1ql/fNqr5AAAAKUGf2EUVLCP/AAAX6X0uY1pyqSOEAxQ0EhC3rLyN0o40/EMcYVpLhmoFAAAALQGf93RH/wAAJKw5BW21JEvPHOYAFo1CUNMsPUENcGSMbpx05hxLGhL7KvFJgAAAAB0Bn/lqR/8AACW/BCcSyWmauF32Wjv+uEbHWZ2qBQAAAG5Bm/5JqEFsmUwIT//98QAAAwLDAvDg+4Gtk20EvyAGbyajb+t7Tmx6Vptk8yxcQSMw+vT9g9tA62zR8xWIfIyiTMWiNJtU5T4P7jqmhGGmKRUWEtUzqZTA7UJqvkHUUb3PyKR/TdaL+LbxrCulgAAAACtBnhxFFSwj/wAAF+mDzbcVdENYfOsBnRDQ7B1VJKgo96CZOd/PjUswCEO1AAAAKwGeO3RH/wAABWezmhMvWimD0R/3HiUiA8iL1/j3FABABTnMxc4HJeRUHz8AAAAdAZ49akf/AAAlvwQnEslpmrhWUx8qO34vrY0EkGAAAABAQZoiSahBbJlMCE///fEAAAMCw7p12w0ucQT4xAZr+Qc834FngEf/ozeAUJqraHNSkzG24Uz0HapsRjfdzW/ZoAAAACNBnkBFFSwj/wAAF+mDzbcVzKyxFVxp34b2v6QsVGuBJdMVJwAAABABnn90R/8AAAMAR1h4oLKAAAAAHQGeYWpH/wAAJb8EJxLJaZq4VlMfKjt+L62NBJBhAAAAOEGaZkmoQWyZTAhP//3xAAADAsO6ddsNLnEEWkTPsdVORzvBBEO9vaXgWQfQLehDIhuWMFryUE/AAAAAH0GehEUVLCP/AAAX6YPNtxXMrJlJPgdCisuHnv/Ucg0AAAAOAZ6jdEf/AAADAAADAakAAAA4AZ6lakf/AAAlscwJmOw1wALqAOngZhKwuqVUeNFB6ubYHanBHTt0vKQfe0y0468EsSSLN6+d3WEAAAAvQZqqSahBbJlMCE///fEAAAMCw7p12w0ucQRaRM8nT+/HxIDo8J09MJW5XGRpdL8AAAAfQZ7IRRUsI/8AABfpg823FcysmUk+B0KKy4ee/9RyDAAAAA4Bnud0R/8AAAMAAAMBqQAAAC0BnulqR/8AACWxzAlXF43VkJboFmG0ba+an8TYfGHWFPNpn7+vh5x+a2/ut3EAAAAtQZruSahBbJlMCE///fEAAAMCw7p12w0ucQRaRFlEyadgNXCgWIEz+C+zh6DjAAAAPEGfDEUVLCP/AAAX4ms3Y3JYOwALW0WoZnbJpvg9e+KOS7RWoHj+MaZh5dq/VCK3dQmOh0uLoWSEHrx2gAAAABkBnyt0R/8AACXDF2ippVPU+8WWGZAY4RDxAAAALAGfLWpH/wAABWcwlnuJLTNluHP2LQAA9X05w5XkAJHoUFMDky+imME4rV1hAAAAVEGbMkmoQWyZTAhH//3hAAAEU1vAm/N5VbDazX8s/CAdE+R8dKNbPtvo2Y7kii3vGCWbADGCPADUoDCzKcfust1KIc8Cnrbmus5Jda2MrmVpiQBSZwAAACdBn1BFFSwj/wAAF+CHtw23JE4vLPXrhVUgN6UZaLfaXTGkV0kY7rAAAAAhAZ9vdEf/AAAlq/hbdu0kW971A51lTNuweXgB4ALKU+O0AAAAGQGfcWpH/wAAJb8EJxLJaZq4VlMfKGdiOmEAAAA9QZt2SahBbJlMCEf//eEAAARTc2PvuiHniI0sAEanf+drJlAuekh8IqIoiECwCjfXbl0Ad47GZo++3K//8AAAAB9Bn5RFFSwj/wAAF+JrN2taYXZrzTF17GS8tj9QXXm3AAAAHQGfs3RH/wAAJcMXaKmlU9T7wgokG6CARFiU0Wz5AAAAEgGftWpH/wAAAwIL8cVZl+BFwAAAAG5Bm7pJqEFsmUwIR//94QAABFZHLxKNyQWb9d7N/78H0nRB2TSzTP3EUZxOIJCWyyOXXfL+rSYfliNCN9ITPfpMMu2hCLMrt3z6MWno8YPJS4DyVcRgvrhbg1DkAZcvIuE3mEHPEqQQ+tZcngG3rQAAAC9Bn9hFFSwj/wAAF+CHtw23JE4vLKbHZPgUAQlQ3RLeOP0uBcfbERu4Gqanzhez4QAAACcBn/d0R/8AACWr+Ft4F0/P0Ca7mQHCOzh4wAC5tsSZWClCtCgQz4AAAAAYAZ/5akf/AAAlscwJb6lvshVI+9OXb9DBAAAAgEGb/UmoQWyZTAhP//3xAAADAsL25b/rUPHrACek936q5GMyGgMmIEYAXKGZXuBGI7i6Nt553/ay1p8No7ttEBxeIld5vpSSk9/dsw9LWL2EAb6pwCGjczOnHPW+L2GEAKIUVLS/JbW8JYHFmPG36k7DeHbpSODasP3LyoZ64GSAAAAAJUGeG0UVLCP/AAAX4IfOqDfbzAs8aRyABMjMld1aLOXPouqabcEAAAAlAZ48akf/AAAlvxs+SPolCBVWPCmxVPEwqITR4DeIVERCpbo91wAAAGBBmiFJqEFsmUwIR//94QAABFNb0gIz474sf9u1Ws940Jpl2P4UiIyLbPXS8XnMEQBCy4ROOqoemwDrmytwXNTewsmyjJ0PNbZRGk7TGvYtE5+OZdKj4et1MkGUC0L5p8AAAAAkQZ5fRRUsI/8AABfgiIvfKvCMukokLiGeglouqwCfpd5S0nTgAAAAFwGefnRH/wAAJcM+DnjYTXxg8k2XcHHBAAAAGgGeYGpH/wAAJbIrBxM44ZvQ0qneEoGDDSG3AAAAhkGaZUmoQWyZTAhH//3hAAAEcz+2sDuaMq4vxyEgTBLRXH3XV1Y1iAmx5QW/1YDsQa/MO6ABZX7amHB8Mjujzc24G7Drbzo6pLO6CF3kDgWgbrKGrP/aW1/CRu3/gX6nutY5DQqnrbOt6wOunz1LT1pv4ZWUKhdQfY9P+COe3RtNyXxnndBtAAAAOEGeg0UVLCP/AAAYgIe3Dd4U+N+9H7xSLyWO4REYIwb86PxHUNnK3PgUJTbcKVxsRtWuac7BshHaAAAAHwGeonRH/wAAJsMR5vfqt+EnNZIn7NXmKRsbTLWIy6kAAAAnAZ6kakf/AAAmvwQlkiTIfobBKZiPlMsBWA4czH7LuhsA20/irHaBAAAAcUGaqUmoQWyZTAhH//3hAAAEdo4uRKtyYDk/nvGFSsr3XCGZEUhyWvB3Bcx1v1CX8sXDG7oDBILJEFcXy7JTn6v8wbNJcP9BNMFtaE9T4OdqBT+ccVxh6x5O9Ma+QvpdWOoAYYSIoMXbUp0aggsuk1ixAAAAM0Gex0UVLCP/AAAYgIe3Df/OJWvF7ZeGsAYXepDB8iZuRODGESeKMN40bO+IpHn8KCopwQAAACABnuZ0R/8AACbDF2ippVPU/GaLZyUEaGXbW3+IJElrPgAAAC4BnuhqR/8AACaxzA22boGpy9uKhYD4ykE7Npu/Vqtrysfm9NzVEJndw07iFRnwAAAAPkGa7UmoQWyZTAhH//3hAAAEcz+aeGdXkEv1rwB1RxGn2eqbuoKaRgVkT6i/HzxLloPZ023AOe2ZbPFkSjDBAAAALkGfC0UVLCP/AAAYgIe3Dd4VXFhSAWOTWahqEAROjAtXgxC90F1I/gUlCZdUjKAAAAApAZ8qdEf/AAAmwJnd6C3xaXuq9ACWUx0SMMYvcMlkiP7VooEHa7EfZUAAAAAnAZ8sakf/AAAmscwNLYiFl2dRdSzOKI2wABdCuLCuvEN+1sHKHLKhAAAAhUGbMUmoQWyZTAhH//3hAAAEc0hDp0v8v7GY3Mb9xhjcQCUuz7lxC5BIg9Z27pIG7aDusVO8OMCTxUJjNT775onuGMOw1AwjzFfa15AV11o8/RHqszkU7pkiEY2bDzSUh3arYSGixpUaNExljt4NbcTJroGrBxvQ3FOb6Spbi+hOcbIohYUAAAAuQZ9PRRUsI/8AABh/6+i0PO1krhv0MAALhbiar9KEFTA2e1Ii4z1VI/wo+asWVQAAACgBn250R/8AACbAmR/OXl0BlLWpD8v2nWuwgBbje5h4IFXzs45DprKgAAAAIAGfcGpH/wAAJq5eM4F5RFqywLvGSnWdM7Wtkcraao5QAAAAUkGbdUmoQWyZTAhH//3hAAAEc0hDod3xDd53+sdOYc8UzzRaL7YnYy/F4bw15Xq5PUOrFkq0FZvNtzsEcI3+gCoE9nPEjMxo6i/xw9fN176PuaUAAAArQZ+TRRUsI/8AABhz1lUmVubTxcFqb3l4bTmWTpFY+rEgvINfrrOpOlLKgAAAACwBn7J0R/8AACajaVBPfqSpn/iUUFfcfyxFxkCgBLAxXQKAOTobx8qf0jjnwAAAACMBn7RqR/8AACauX/s4EYThe5l23ycYddRHxtW7Rh0wMZvcIQAAAIlBm7hJqEFsmUwIR//94QAABJM/trA7RjmED/kyJEYrAJR8F6NktGI2+NRqBsosRRCu1tsVnMfgCAUv9zrkXAFa+XeNQqer0AjqYBLhRIstICuIx1pJ8eJ0eM+KDA1A94ddUtrmCwFNNR3PawIeTFGKJHjqZ5iQ92JrrVfWDZEUX/R8Hg4Pn7GfsAAAACtBn9ZFFSwj/wAAGSl8w2AgW1YZUVQ85mZN7O80s7U+YDvX9C3maClOhSj5AAAAJwGf92pH/wAAJ8VT0zx6iIxlqgBJeAu47vQ2mIf5PInI5r5nTuHDlQAAAHJBm/xJqEFsmUwIR//94QAABJS2AyG72QCJqjafU2nUkrgl3H6y55jMXCp98IUKP2RPeTnymXEuMT1/zDVK+ABqiH/ywwF6bCLX2FEN4frOM4ISwWYYzLMH2tJlN2HnL2Z/zmLwSwYHf/z6TGqu0bNX6OAAAAA6QZ4aRRUsI/8AABkf6+i0WJt2q6SYwsS4sJrQsf1hgAzNclcTwgca++YAHvkq8Bwz7tmFmn9O0uF5QQAAACUBnjl0R/8AACfbVyYoVsmrsk+z0SGic8KFacN0AfuM5gBuNeJuAAAAJgGeO2pH/wAAJ8VT0z1GtBfE5JYgVdNdgpoAzCLIMRQcLPBr9bKBAAAAT0GaIEmoQWyZTAhH//3hAAAElj75G2KmeGcKEVEYbXcaI+iimtm6AmvcYCEv5UZgH/jZsll0xqT1TPi0kr+kGPPSzRjAp622sKotl/FAqCUAAAAuQZ5eRRUsI/8AABkf6+i0Q1TQ+v97ldZXOanklPHpSrpB2jMd74Ohs+zyYjOniwAAACEBnn10R/8AACeoOrGKjOWwpbmj/5fmlOrO/OxbEJujHWAAAAAiAZ5/akf/AAAnxVPTPUa0F8c/Wy+QuCOGWDCM0IOZSGQ04QAAAHpBmmRJqEFsmUwIR//94QAABLNODL1uUGCtjy9QVcV5Ixbra8HYsFMS0hCe2OIa1CQgKvEv5zBKt/BTF9gYAn7iANvGyj5zQgwdxUpHEEvHxDOEYMnmLJ2/dV+iHPmzVlz7mSYA+MKNTq0KARP85OET0CXlY0ZtX1zDQAAAAC5BnoJFFSwj/wAAGcl8TsmC7K9mQAKYCNQAsVhnuB7Ylixmqbu2PyAWIVtrrHWBAAAAKwGeoXRH/wAAJ9qdKDuVifX/6dACS7Ezvp+rDAMpiCUWQucSajkuMNj2xYAAAAApAZ6jakf/AAAo+W44rFSTK8DgfNd2IHaqqjnOgwoIsl2+aXu+0nhk04EAAABaQZqoSahBbJlMCEf//eEAAASzS+S7ZJMcYN3fWe/t3cQey4oPJ2aWOwcgqQxj0BfF/wAHC3RhX1Q7rgvCohn6gSMdZMjYxqExBm6ygUFp7N+plBf0QiDgLrNBAAAAREGexkUVLCP/AAAZyJth9Kx/zQDagAG2eyfW2ZmHvO1OVwXYFhWlCGUxqUcKZ/BGHTUvJZeifQfN4J/oX193iiYYD58XAAAAOwGe5XRH/wAAKPtXJihRXCMvgGI2uKTgpCVzFPwz3ebuye8L8uc38sM/y/uFm5LI5qps4ueX94UDa7rBAAAAJwGe52pH/wAAKOVT0zx6hlSFRNACWUml5wRnZMEUdO3YA4gAuC/7cAAAAGVBmuxJqEFsmUwIR//94QAABLNOEOfQd+nER8HgAC2l9OUeXvB78PG75vrTp+OnljkDKm4hidtrlSTfJwVow1cWoDXFBlMtPg8J/Tszj+lnKN9BuSp2vko85L4EMXfCPx1hkJHOgAAAACxBnwpFFSwj/wAAGb9DvQthjVd1hSi8nKxDMk8mVoJrgAasEbGXuMFFk552WQAAACEBnyl0R/8AACjIOrGKjOWwpbmj/5fmlOrO/OxbEJujHCAAAAAgAZ8rakf/AAAo5BmXyQhg7POmSBROW1DzHlb4ZfwR6Z8AAABwQZsuSahBbJlMFEwj//3hAAAEsz++wANIrLkv2pXUh4efBJsGCmP5zUezBYOnDRS1JRlcphfsBs7sQYGgQZTqpo2QQ8Z1NsllA0W0If9p2nVOZokiyG6OAF8V18ZUbhGsmZPaNyL3tYybJ6BESlXVIwAAADMBn01qR/8AACjkGoT8ipG9+sOjAiTwcRPgjjK/5XT+we7YU56ACb+itRLYT2pa4Ec9TssAAABQQZtQSeEKUmUwUsJ//fEAAAMDDkjYgAcK7TXGY1qWlva9LaqktLApF2MoXb6iaYSGiga0ZvyebDJrzlqTE6D+U7itNmOSCyVpTfNWri7j9rkAAAAsAZ9vakf/AAAqBVPTPHqGVLGS+Hj05buBYqwFoYo8BrUp41libOozoI0nsCAAAAAzQZt0SeEOiZTAhH/94QAABNNQNL94lIPm9AAOlu7BIn7cgZLFbTp9zL/1iWP6zqlHhaDZAAAAOkGfkkUVPCP/AAAaX3UdC15p4MXzYW7UALbTKpEqoblocfgi1fJ5BIwtL4j149QFQRdfW0qCa3d4mBEAAAAjAZ+xdEf/AAAqGp2NYh2cWUR+oHQMxb0j3R5ihycVP/1Mp+4AAAAnAZ+zakf/AAAqBBkycN88MGJd8v3mcZFhJ98wP/96AATqiBz+xH13AAAAfEGbuEmoQWiZTAhH//3hAAAE1RTjuWfIUgHlu4tw492sd/NJAb6NZm9j/X8PTVjeWx8ewAjtlZDqvklwGoEFgBCOMe7kpQ59iNNkUkMN7IeJRoxwsGVYSrxI3dytNC5IK+cuEoxgU9bbDGWnp/iiEaEaEe1kx44X2wQUaC0AAAA6QZ/WRREsI/8AABpfdT6mH0UBd1Sv3VppZTxFfTgc9WfSFu+b+QOqPmbOmst3+PYOZLfR4gG1qeYBbAAAADABn/V0R/8AACobVyYmqAjU+UnBzFe4Euz1IYLB7/ikzM065w9kOHf4lo3zINzAVXcAAAAmAZ/3akf/AAAqBBqKYfRasj5+aNtlaOdGtyZNvX9Tp8VwkDZbXcEAAAAwQZv5SahBbJlMCP/8hAAAE0wDPKMgOyBIZva5vZ8xKJ7h7sFyCLurldd1im1kkeqQAAACG2WIggAL//72rvzLK0cLlS4dWXuzUfLoSXL9iDE4PugAAAMAAAMAAAMAiuvdvYXQkPO5AAADAFzAELCSh5B5iJirFQKEcvgc7gBl3lAPSknwkxhnkX+IXCKV2DI5CyWLWW649e61Z/J5jj3f6MOBHoLXBeH+MFVLpyZfH5CLFVCd636ozcf7o/8SN3yJLPhovt4QhgBvA+fkFYHfxxBy5MEtfFGKaYRdvyUMnJmLBizLvW4vQUEdfF2LQS9o++2sampsk1+AXYphsQEH2ov4UzoCas5xkZVZgZB9v2M5cqiAxmHVSEG/Udl1ME9Q98Infe1QUS0CavzAYIYyjsG11pO6OgN0UAEhBJPi4Rc7F2MSSZmFBxwMPzaFCjJlb8n0y8up7Zjns6YITHgchuTKcZ08TRNx73cEhpIYPIV/MhZSIWnb0+6LVQgJxcQi4tHJ+VM0oaghgkgJWj32I6BU7QPbBNScclnaC7NGFC3d6Be9ZiH4zAQajuoRdw7g/lssHq0uAAiFrcIZjFn5CPI38h4EMshQH0fjjExY9W3bwvcu/FYh+saL2SL3B+TSj7zy/rX2inXEAe35P0aXSXwZo2KntaCCYmAWbL08MFwevwCSonzKP8koNKH+ef3HabLHGtHaShgBRDat4vyX+Zg5/ZpHjVjgf7pqoknd+YLGc+wub1od7m8UfeuDvATVlRcO7AAAAwAAAwAAAwLfAAAAk0GaJGxCP/3hAAAE9hy6Un1wb7I6tcTIBrdxHhxEJBtDVgSv8fpWRYZ5U7N3fTw3JBaS5aEaMJE7dMczZMH7l73TczBHEivemrD02VO2SdxbQjndheL7QqCbzNXsnsjG3FHqf2da8YhOc3PN8mTv1etXjwo0Hna0jOb8q4c0MPKFroHdt14krA7GM+0E0CsNzE5jegAAADhBnkJ4hH8AABr/aMPTllrcmTV6uid63W1owBo3WDpeLAPFaCk2wOHH+4+/m8TewFObjblDj3a2gQAAADIBnmF0R/8AACsfnpLu7JCX+iMasr+Ax3JBHFqZ2WPYuNgKAIjS9zYkXucxnefiZCDbQQAAAC0BnmNqR/8AACskUv2TyRXHz3z2nDfCGdJ2fL/xt8gxQ+em3JRX0saoJ0NKosAAAACvQZpoSahBaJlMCEf//eEAAAUZkpWklAPmcw54A3C6yh8Q+eW/OnF/igPp4jL4W2ZJxnqmFZmJVAP5nF6AOHs4GlfwuXqyN2GP66PcFrZ9xFO952ea6Rupyabqo/4lzsxfy1rgoy4RbfqrTluMslc3KuPMpzGcAjFhM6kSNxBqDQJO1+d1WDA3wdRDtTrDKbGKoMxIGO+mXtX54lY8NkqTwfLsuABfUrqbDJb45VIUzQAAAD5BnoZFESwj/wAAG6mGzy+peGJeU+bKIXOKqypPF6vfYSfPZ7Fw0UFICehDVY405voVdjVygfFw+iJCyPit/QAAAC8BnqV0R/8AACsfnqCoORe1GrrXMW8V3xFnTX/knZ+ht5O9K86VZ0W1fGXYM7a/gAAAAC4BnqdqR/8AACxXM0VUgubCEG1TyrB8a8jK4o1gGPxRB6czm9+1/t6S4eilbd7bAAAAXEGarEmoQWyZTAhH//3hAAAFGzE4SkttnVM98Mzx/XgAA0Y7sOvbTtDm4afZ5Z9+03XijzntsiT4/G0zeaDCnlRDab7bvX5kTmwcFpKVIc9TaE6ds5tYl8040hI4AAAAJUGeykUVLCP/AAAbn3UdC15p4MXzYVPXc3V+bGeUAOJleAH0efEAAAAnAZ7pdEf/AAAsWp2NYh2cWUR+oHUDdLovbtB7Nvqj9MQyMT/Jw/FtAAAAJAGe62pH/wAALEQZMnDfO6kRDXa2J0mqIUQMHVWa31GvH2FxfwAAAINBmvBJqEFsmUwIR//94QAABT2SaYoB8qnsBvgBa/uMaKxUY0ivnRJoZKGYxNA1NOReAWkryoRYP//fzIHSkDS8ci111VfVW+CUYMa/aToA56qh+YAPFO8gytdm9b9UV7Qu/+gulvFuwAUTSQFzVTs8+xVMHhyRO4R+RPpIacftbIkh3wAAADJBnw5FFSwj/wAAHFhx/UmC2Y3/PMgVNjOgxHXYFBniAA3zUayYWanVjeLjYJB7sDUt+AAAACMBny10R/8AACw/nqCjIP2wgXw8nOhrr/qbOGr8N3LbSguRfgAAAC0Bny9qR/8AAC1kGT/fZLMzJdHNGRT+V6vPLpKzHdlBbOJXyaHewpkOmdYLZuEAAAB2QZs0SahBbJlMCE///fEAAAMDTAzspb0ZjwCa5028JJs3zHa2+5Ryl01ibvEQgIm9mZPxNRGWr23qjxMdec48qTaXQW5nsaW8aTdszNcu5Inzjm9A+KkPYQ/aS0oW4swpt5t2N38IE+IW9RhLyR+MfYLZXGWqMAAAAExBn1JFFSwj/wAAHFcCF6nAz/JFHUk4/C0rtaB15KnVTkQjqEItKNJ1gUVPBMV0F/I1V+PP6usTwqXAH+lk2zjBhv31/kKgQqi+LXBcAAAAKwGfcXRH/wAALXBQvHETogdwo2dkKcf2n8ttYAx9GDZvtbmdAv1qdO2GzcEAAAArAZ9zakf/AAAtZBnuHznL8LrZdElvWtezVXwQ058fBl6PNrGyhxr6YYPUIQAAAIJBm3hJqEFsmUwIR//94QAABWGT0zZ8eIDwg4BHcWfROHLkIE1Xf3+QtyH+O32o88+MKq59kfiUeO5E55hrooupo3Ab5U66m/ibmiYSe2ul86nC5W5OPX+/JE6V0j6NgxCbH83twEpVjSS76YIvD8/7E4t4xHpLKrRbpC6DLUQjUIzBAAAAQUGflkUVLCP/AAAc/JBlTvjHHJyAAIhrF0FKes8wc9DjesynpkcMEqX8Mrc0PaplzyTgdW/zVaCDfcMLr+EkZ61AAAAANAGftXRH/wAALp4z3ZnbVORULkBXNuJebgKbzMXkWKwu1GGA5scAEnENy0uP81CO8ozEbpAAAAA9AZ+3akf/AAAuhBmXyQhhJXieAmbTvLUCs690UGhvgW2mEjSzMrm4lPMJYeSdDDb/7IXcYo1YdW9Iv1Oq8QAAAEdBm7xJqEFsmUwIR//94QAABWGT1Dod3x5R3nf8YQA1Qbmr/rYRx9ms0ZmPC6Kp3UTjD9POACZ5QSnlQe5/DSWw9/8aQuovowAAAChBn9pFFSwj/wAAHPyQZI7vd2wETx07yl3VejZpsKgBLZyfYmX5wawQAAAAMgGf+XRH/wAALpqc7i9SbsKQFEU9dFXZUv6K0DD+4iR/+92m4+4fzngncnAwiJVjhteBAAAANAGf+2pH/wAALoKc8wYoz9PtDv7PBAFQt/9CHrRK9N1sP4Z2/3INH4jB2a1WaavzgPQYNUwAAABZQZvgSahBbJlMCEf//eEAAAWFk9M2frgAzKoQfpAJq/Snh9A73/klHAVckpgnEOStRlgEDzLrTwJXlvyt2apJ7+hUcs3Len+VetAMl4lGI6aW7MEn2KmY/pkAAAAuQZ4eRRUsI/8AAB2skGSC15p4MXzYVa7IACITuNq7Or9AqhmQVpPqCYaHJSveYQAAADcBnj10R/8AAC/SxiEzSsQ28EDQFLBw2v1T8HzP2KNFvDE8GLDrupuI2Bt5jSx2tEtjmSf85knAAAAAJgGeP2pH/wAAL7jcx+kMfmq/h9b7zOMgPDwBMBND6nxSSGK8+cFTAAAAdUGaJEmoQWyZTAhH//3hAAAFqZTnNslx/y8iicNjoZ0aKh0W6d+3GoaWesp0ycSqcvwTbPdw2u3QXYE2hdl7gRjZTQytgazaNiDkAkwMlJ0mvqiCyKNfMLm+1PYZYDlQY2vf4myRdrxKDo6cB4TaoFD/NmMwwAAAAElBnkJFFSwj/wAAHmhyEVJguAG/A+89dNK53g133KHvX1alhLvzgBAwc/zFTGhCn7hzFIVe+YYProh+ikYAtrOQZiVDxEcP8wlrAAAAKgGeYXRH/wAAL9F9GizWK8BW7Cdhh05p3oy4uVpgZM/M5ryxNjnank17OQAAADsBnmNqR/8AADD3F7b2Tm2w686yAHBt94X+Jeq8KXZE+WScNq4gBLxDRdKzD28D7FmdXuJKKFxNCucEtQAAAFhBmmhJqEFsmUwI//yEAAAWpMg5QCPrmcIDUpuIChUvC8eUkR19dHayrkxcxNTaBYpo9F6LHpoeQLyrlSih0JD/VEAXF84dhlBdlUJfAqDkGr/bNyKZPGmAAAAAJ0GehkUVLCP/AAAfGHIRSh/PaW3MBBuGrG7gdu0y+EzsmD8+Rlhd/QAAACsBnqV0R/8AADD0fgadHkmXCLD6VgwKYnvtOonMJ3QOADmImSuyi8qNFbPgAAAAJgGep2pH/wAAMlI7fU3hsDlFDiVwH+BSJjI1JjUy7j09d2R5oInBAAAAS0GaqUmoQWyZTAj//IQAABauPtvDu7fKeItBm1gGBzgpJuOgC/xrHgNucuyHfoTv71jbhCatbXhVdhVlyAeF8JhErzzJu4ORWehx4AAAADxBmspJ4QpSZTAhH/3hAAAFzX+aeGMYb17Q6TIqd3B7A5NmgIx5QUpTQDiswn8NCeYl3NeQgVNwUedohGEAAABdQZruSeEOiZTAhH/94QAABfXx5MpJv8vgjXFPnx18qW/Mk0+orB8SA0qsmJ9Mg6CpuSjBv0qYcIsn/dLwTW9w3b5pdsvj6zSRWIn8iglZI151YHSK7G4hu0nwQBhhAAAAMkGfDEURPCP/AAAfyHIRUmC2Y3/PMgVNiesfkdu+XoWyGhpso7g2kfhIyDqnmjKht3mBAAAAJgGfK3RH/wAAMlHh+Jz0JFeKGTYCMNEjhR7fQ/bplEzlJuFWsaqDAAAAJwGfLWpH/wAAM3cX+DoOkXa/Q6HtBl1/0deS2bWoY2JYRAP7skmojQAAAG9BmzJJqEFomUwI//yEAAAX0BGxQ2XrmbsjzH6sTg5+j9fXcqVQikz5r2Trf3PCYjfBmLUGIWyv52hAFqzfObLxFcMLqOBtcVg5zWl7b4g4YJOVI9qASIGWYzN2gH0baH54+/GJbTCUh2IM3rlLK8QAAABDQZ9QRREsI/8AACC8eRvUbYQ2aJmd4EizjZ84JC/UJxlP7hQeWnCHciJF7Vl+0zkFY2PePXs0TIufTV38xnqJNF0JuQAAACYBn290R/8AADOR4lUR2iAexH+lEJUxemLaOQDPPjSElW9RKfjjQAAAACcBn3FqR/8AADTSO31N4bA5RC1x4WmJonnGszhbYSOoP2pQyGzLRBgAAABbQZtzSahBbJlMCEf//eEAAAYcSDLf6Ug9JvgAG8TRFoeDfhusX97w4Cdqw+XnL+kKaaBLEbkojLNst+P7xk4VhoYu/YUYSPVBOwq7qtCgYQE3twlyIb2SI/JHgQAAAFZBm5VJ4QpSZTBRUsI//eEAAAYcUDjoYGYmgFAAREojBNs3OD7bIbY2p/Qwbei4eks7GxRIfFsPNLhJbyzLLZqsKc5m2W4I+KwOevTrowmC3u5bHaPYUQAAACcBn7RqR/8AADS3nrb2TkC1X8Prfe8JsEjGq4wU27NO+s3OoTBpjg0AAAB7QZu4SeEOiZTAhH/94QAABkXyAgKS76Xqqp7H8wHrh1cIut2bj31QWu+QYztaLSV1Dd9UoSDFrpgBMvPGROMccRfs7iKfnxXxMjqo/NoXYXS7W8G+pbvhcQJSyavQ5R5syz2+UGrb+Lpf4qisecK2ybzqOgC4DtI0M0swAAAAP0Gf1kUVPCP/AAAhvHktOwnsLyER77jg5TmnrA3tC9/wGJHW+XZBr+eRAAN1HsttTzezWdZtZzMEbWpiGQYaqQAAACkBn/dqR/8AADX3nrb2Y8cftjAawkeZt73+vpSMIY57WnAYDe6+igivQQAAAGBBm/xJqEFomUwI//yEAAAZEBgBRZHrmb/FbLr7mqbgu5BKM5exfWb52ApAX8a8ZrXb9QU4Mw7r3FKjmrUIzURuNmdAC/zQiNBJPJdpLdgHc1nfv60+9gMEpaMVQuQ1WewAAAAxQZ4aRREsI/8AACK8eS0kwWzG/6HZl7Ea8wdd3kQBEy4crpiIIEkSq3bZgrMjH2R5OAAAACcBnjl0R/8AADYR4hotAihbhXW0K7qqp0uhToJL/ie4meXRNsLXJJ0AAAA0AZ47akf/AAA3N6zzTANnGltEjiTMBuktHT7wjHDtLZBz2jQ/PNjzHt5P5K1DkRhreSA0fAAAAIZBmj1JqEFsmUwIR//94QAABmxJKQGAF/m7N84D+dHmySARm92/PZxo/U0SeTizegWfHGoLN/KWbqX1xt5jns5uJAL0/T+txNYbI11CtJsTqimP7QYvmj1XmU58YT9/SFGm9Al9V60Sb63W2jYoeoi0k6zoK7uHsIouGJinBsPeHUvWC6SogQAAAEZBmkBJ4QpSZTAj//yEAAAZsAuM4vKZtSZb3/hvyqfMigvzpkV91HoOxL9XPmVFTGj0CdcS+Oeuh+SGQ4SIROMaA5eiIXjhAAAAMUGefkU0TCP/AAAjvioadoGHJgsaXxDo8z4VaCnvifpa/vTsk4+FAoAhrFgtoMUq+J8AAAAqAZ6fakf/AAA4kLMKXkPtHrnAJvZ0tcfroxxmCNQSP9/IcWO4LQqEW2zBAAAAVkGagUmoQWiZTAhH//3hAAAGlEwDu/ykkM/fUALFz2WiWDmSuGzB/WXW5pZcJM3XDJB42XOyMsPOl5fq3Mh/jPHhGBCy+7BT8UVRrMcwHot9ObzM1fFoAAAAREGao0nhClJlMFESwn/98QAABBN/PwuUhn6FE3Q+saZzd67QxZxOj3vQARNhw+PDZ0HCvSQSB3O1KvppSjoLglt5n1DFAAAAKgGewmpH/wAAOGo7jLyahxXJ1LpKXgES8ioJ3peOm72agSw7nN1dGKa3SQAAAE9BmsRJ4Q6JlMCE//3xAAAEE3nuKiH2dX3PnMAUctueQYeKsaHAX3cbnX9aP2ktbCf7ZxipmgIiAFmspWhtkK2BPtM+TWLnXcOG/JCtQjuUAAAAREGa5UnhDyZTAhP//fEAAAQb4BqPeJsOnqDEMwo5R2LTW95Hj0alS76sW5/38CaaiqbIOb4ABD6BfNxELIwJxrkcwaAIAAAANEGbB0nhDyZTBRE8f/yEAAADA6PoonL2KgWwiBvDeVs4emFWpsKZgSnmv51zN1AgDWOuM4EAAAAfAZ8makf/AAAH7/4mF6B3UAm7RBFRWTJYVIF3vG/jgAAAEedtb292AAAAbG12aGQAAAAAAAAAAAAAAAAAAAPoAAAZKAABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAREXRyYWsAAABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAAZKAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAACWAAAAZAAAAAAACRlZHRzAAAAHGVsc3QAAAAAAAAAAQAAGSgAAAIAAAEAAAAAEIltZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAADIAAAFCAFXEAAAAAAAtaGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAABA0bWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAAP9HN0YmwAAACYc3RzZAAAAAAAAAABAAAAiGF2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAACWAGQAEgAAABIAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8AAAAyYXZjQwFkAB//4QAZZ2QAH6zZQJgz5eEAAAMAAQAAAwBkDxgxlgEABmjr48siwAAAABhzdHRzAAAAAAAAAAEAAAFCAAABAAAAABhzdHNzAAAAAAAAAAIAAAABAAAA+wAACdhjdHRzAAAAAAAAATkAAAABAAACAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABAAAAAACAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAQAAAAAAgAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABAAAAAACAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAQAAAAAAgAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAAAwAAAAABAAABAAAAAAEAAAMAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAgAAAgAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAIAAAIAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAIAAAAAAQAAAwAAAAABAAABAAAAAAEAAAQAAAAAAgAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAIAAAAAAQAABAAAAAACAAABAAAAAAEAAAIAAAAAAQAAAwAAAAABAAABAAAAAAIAAAIAAAAAAQAAAwAAAAABAAABAAAAABxzdHNjAAAAAAAAAAEAAAABAAABQgAAAAEAAAUcc3RzegAAAAAAAAAAAAABQgAABMoAAABbAAAAOQAAACgAAAAiAAAAJQAAABwAAAAWAAAAEgAAAFAAAAAwAAAAIQAAACIAAABvAAAAMAAAACMAAAAgAAAAbwAAACcAAAAuAAAAHQAAAFgAAAAtAAAAFgAAACMAAAAbAAAAFgAAABIAAAAtAAAAXAAAACoAAAAoAAAAKgAAACUAAAAeAAAAHgAAADgAAAAcAAAAEgAAABgAAABEAAAAJAAAAB4AAAAVAAAAIgAAABkAAAAWAAAAFAAAAEoAAAAzAAAAGwAAAB8AAABOAAAAJAAAABUAAAAgAAAAOQAAADgAAAAfAAAALwAAAFwAAAApAAAAJAAAABoAAAAjAAAAKAAAAB0AAAAcAAAAVAAAACUAAAAdAAAAHQAAADwAAAAsAAAAHwAAABQAAABTAAAAKQAAABwAAAAeAAAAGwAAABQAAAASAAAAEgAAADYAAAAiAAAAGwAAABMAAABYAAAALwAAACEAAAAdAAAANgAAACEAAAAfAAAAEgAAAEYAAAAoAAAAIAAAACgAAAA6AAAANQAAACUAAAAgAAAANwAAACIAAAASAAAAIAAAAFQAAAAsAAAAGwAAAB4AAABYAAAAMQAAACQAAAAhAAAAJQAAACsAAAAgAAAAQwAAACIAAAAdAAAAFAAAADgAAAA0AAAALAAAACAAAABmAAAAMwAAABsAAAAgAAAARQAAACIAAAASAAAAMAAAADMAAAAiAAAAEgAAACAAAABAAAAAJAAAABIAAAAgAAAAQwAAAC4AAAApAAAAIAAAAE0AAAAhAAAAIAAAABIAAABTAAAALQAAADEAAAAhAAAAcgAAAC8AAAAvAAAAIQAAAEQAAAAnAAAAFAAAACEAAAA8AAAAIwAAABIAAAA8AAAAMwAAACMAAAASAAAAMQAAADEAAABAAAAAHQAAADAAAABYAAAAKwAAACUAAAAdAAAAQQAAACMAAAAhAAAAFgAAAHIAAAAzAAAAKwAAABwAAACEAAAAKQAAACkAAABkAAAAKAAAABsAAAAeAAAAigAAADwAAAAjAAAAKwAAAHUAAAA3AAAAJAAAADIAAABCAAAAMgAAAC0AAAArAAAAiQAAADIAAAAsAAAAJAAAAFYAAAAvAAAAMAAAACcAAACNAAAALwAAACsAAAB2AAAAPgAAACkAAAAqAAAAUwAAADIAAAAlAAAAJgAAAH4AAAAyAAAALwAAAC0AAABeAAAASAAAAD8AAAArAAAAaQAAADAAAAAlAAAAJAAAAHQAAAA3AAAAVAAAADAAAAA3AAAAPgAAACcAAAArAAAAgAAAAD4AAAA0AAAAKgAAADQAAAIfAAAAlwAAADwAAAA2AAAAMQAAALMAAABCAAAAMwAAADIAAABgAAAAKQAAACsAAAAoAAAAhwAAADYAAAAnAAAAMQAAAHoAAABQAAAALwAAAC8AAACGAAAARQAAADgAAABBAAAASwAAACwAAAA2AAAAOAAAAF0AAAAyAAAAOwAAACoAAAB5AAAATQAAAC4AAAA/AAAAXAAAACsAAAAvAAAAKgAAAE8AAABAAAAAYQAAADYAAAAqAAAAKwAAAHMAAABHAAAAKgAAACsAAABfAAAAWgAAACsAAAB/AAAAQwAAAC0AAABkAAAANQAAACsAAAA4AAAAigAAAEoAAAA1AAAALgAAAFoAAABIAAAALgAAAFMAAABIAAAAOAAAACMAAAAUc3RjbwAAAAAAAAABAAAAMAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhkbHIAAAAAAAAAAG1kaXJhcHBsAAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAAAABMYXZmNTguMjkuMTAw' controls>Sorry, seems like your browser doesn't support HTML5 audio/video</video></div>"
      ],
      "text/plain": [
       "<moviepy.video.io.html_tools.HTML2 object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run and visualize the best agent\n",
    "dqn.run()\n",
    "dqn.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3b40f",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "To get a full DQN, we need to do the following:\n",
    "- Add a replay buffer. We can add a replay buffer independently from the\n",
    "  target network. The version with a replay buffer and no target network\n",
    "  corresponds to [the NQF\n",
    "  algorithm](https://link.springer.com/content/pdf/10.1007/11564096_32.pdf).\n",
    "  This will be the aim of the next notebook.\n",
    "- Before adding the replay buffer, we will first move to a version of DQN\n",
    "  which uses the AutoResetGymAgent. This will be the aim of the next notebook\n",
    "  too.\n",
    "- We should also add a few extra-mechanisms which are present in the full DQN\n",
    "  version: starting to learn once the replay buffer is full enough, decreasing\n",
    "  the exploration rate epsilon...\n",
    "<!-- - We could also add visualization tools to visualize the learned Q network, by using the `plot_critic` function available in [`bbrl.visu.visu_critics`](https://github.com/osigaud/bbrl/blob/master/src/bbrl/visu/visu_critics.py#L13) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a1e54",
   "metadata": {},
   "source": [
    "## 接下来要做什么？\n",
    "\n",
    "为了完成一个完整的 DQN，我们需要做以下几件事：\n",
    "\n",
    "- 添加一个经验回放缓冲区（Replay Buffer）。我们可以独立地添加一个回放缓冲区，而不需要立即添加目标网络。没有目标网络的回放缓冲区版本对应于 [NQF 算法](https://link.springer.com/content/pdf/10.1007/11564096_32.pdf)。这是下一个笔记本的目标。\n",
    "- 在添加回放缓冲区之前，我们将首先转向使用 `AutoResetGymAgent` 的 DQN 版本。这也是下一个笔记本的目标。\n",
    "- 我们还应该添加一些在完整 DQN 版本中存在的额外机制，例如：在回放缓冲区足够满之前开始学习、逐渐降低探索率 epsilon 等。\n",
    "\n",
    "<!-- - 我们还可以添加一些可视化工具，以通过使用 `plot_critic` 函数（可在 [`bbrl.visu.visu_critics`](https://github.com/osigaud/bbrl/blob/master/src/bbrl/visu/visu_critics.py#L13) 中找到）来可视化已学习的 Q 网络。 -->"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
