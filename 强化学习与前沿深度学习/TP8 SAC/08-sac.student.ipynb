{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4c25c6",
   "metadata": {},
   "source": [
    " Copyright © Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e919969",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook we code the Soft Actor-Critic (SAC) algorithm using BBRL.\n",
    "This algorithm is described in [this\n",
    "paper](http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf) and [this\n",
    "paper](https://arxiv.org/pdf/1812.05905.pdf).\n",
    "\n",
    "To understand this code, you need to know more about [the BBRL interaction\n",
    "model](https://github.com/osigaud/bbrl/blob/master/docs/overview.md) Then you\n",
    "should run [a didactical\n",
    "example](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/03-multi_env_autoreset.student.ipynb)\n",
    "to see how agents interact in BBRL when autoreset=True.\n",
    "\n",
    "The algorithm is explained in [this\n",
    "video](https://www.youtube.com/watch?v=U20F-MvThjM) and you can also read [the\n",
    "corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/ps/12_sac.pdf).\n",
    "\n",
    "\n",
    "# Setting up the environment\n",
    "We first need to setup the environment\n",
    "Installs the necessary Python and system libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36835ba0",
   "metadata": {},
   "source": [
    "### 展望\n",
    "\n",
    "在此笔记本中，我们使用 BBRL 实现了 Soft Actor-Critic (SAC) 算法。该算法的详细介绍可以参考[这篇论文](http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf)和[这篇论文](https://arxiv.org/pdf/1812.05905.pdf)。\n",
    "\n",
    "为了理解这段代码，你需要熟悉 [BBRL 交互模型](https://github.com/osigaud/bbrl/blob/master/docs/overview.md)。然后，建议运行[一个教学示例](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/03-multi_env_autoreset.student.ipynb)，查看在 BBRL 中当 `autoreset=True` 时代理是如何交互的。\n",
    "\n",
    "算法的详细解释可以观看[这个视频](https://www.youtube.com/watch?v=U20F-MvThjM)，也可以参考[对应的幻灯片](http://pages.isir.upmc.fr/~sigaud/teach/ps/12_sac.pdf)。\n",
    "\n",
    "### 环境设置\n",
    "\n",
    "我们首先需要设置运行环境，安装必要的 Python 和系统库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b30deb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils>=0.5\").setup()\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.workspace import Workspace\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent, KWAgentWrapper\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from omegaconf import OmegaConf\n",
    "from torch.distributions import (\n",
    "    Normal,\n",
    "    Independent,\n",
    "    TransformedDistribution,\n",
    "    TanhTransform,\n",
    ")\n",
    "import bbrl_gymnasium  # noqa: F401"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c64dd",
   "metadata": {},
   "source": [
    "# Learning environment\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The learning environment is controlled by a configuration that define a few\n",
    "important things as described in the example below. This configuration can\n",
    "hold as many extra information as you need, the example below is the minimal\n",
    "one.\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    # This defines the a path for logs and saved models\n",
    "    \"base_dir\": \"${gym_env.env_name}/myalgo_${current_time:}\",\n",
    "\n",
    "    # The Gymnasium environment\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "\n",
    "    # Algorithm\n",
    "    \"algorithm\": {\n",
    "        # Seed used for the random number generator\n",
    "        \"seed\": 1023,\n",
    "\n",
    "        # Number of parallel training environments\n",
    "        \"n_envs\": 8,\n",
    "                \n",
    "        # Minimum number of steps between two evaluations\n",
    "        \"eval_interval\": 500,\n",
    "        \n",
    "        # Number of parallel evaluation environments\n",
    "        \"nb_evals\": 10,\n",
    "\n",
    "        # Number of epochs (loops)\n",
    "        \"max_epochs\": 40000,\n",
    "\n",
    "        # Number of steps (partial iteration)\n",
    "        \"n_steps\": 100,\n",
    "        \n",
    "    },\n",
    "}\n",
    "\n",
    "# Creates the configuration object, i.e. cfg.algorithm.nb_evals is 10\n",
    "cfg = OmegaConf.create(params)\n",
    "```\n",
    "\n",
    "## The RL algorithm\n",
    "\n",
    "In this notebook, the RL algorithm is based on `EpisodicAlgo`, that defines\n",
    "the algorithm environment when using episodes. To use such environment, we\n",
    "just need to subclass `EpisodicAlgo` and to define two things, namely the\n",
    "`train_policy` and the `eval_policy`. Both are BBRL agents that, given the\n",
    "environment state, select the action to perform.\n",
    "\n",
    "```py\n",
    "  class MyAlgo(EpisodicAlgo):\n",
    "      def __init__(self, cfg):\n",
    "          super().__init__(cfg)\n",
    "\n",
    "          # Define the train and evaluation policies\n",
    "          # (the agents compute the workspace `action` variable)\n",
    "          self.train_policy = MyPolicyAgent(...)\n",
    "          self.eval_policy = MyEvalAgent(...)\n",
    "\n",
    "algo = MyAlgo(cfg)\n",
    "```\n",
    "\n",
    "The `EpisodicAlgo` defines useful objects:\n",
    "\n",
    "- `algo.cfg` is the configuration\n",
    "- `algo.nb_steps` (integer) is the number of steps since the training began\n",
    "- `algo.logger` is a logger that can be used to collect statistics during training:\n",
    "    - `algo.logger.add_log(\"critic_loss\", critic_loss, algo.nb_steps)` registers the `critic_loss` value on tensorboard\n",
    "- `algo.evaluate()` evaluates the current `eval_policy` if needed, and keeps the\n",
    "agent if it was the best so far (average cumulated reward);\n",
    "- `algo.visualize_best()` runs the best agent on one episode, and displays the video\n",
    "\n",
    "\n",
    "\n",
    "Besides, it also defines an `iter_episodes` that allows to iterate over partial\n",
    "episodes (with `n_steps` from `n_envs` environments):\n",
    "\n",
    "```python3\n",
    "  # with partial episodes\n",
    "  for workspace in algo.iter_partial_episodes():\n",
    "      # workspace is a workspace containing 50 transitions\n",
    "      # (with autoreset)\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01357704",
   "metadata": {},
   "source": [
    "### 学习环境\n",
    "\n",
    "#### 配置\n",
    "\n",
    "学习环境通过配置文件来控制，该文件定义了一些重要的参数，具体说明如下示例。此配置文件可以包含任意数量的附加信息，以下示例展示了最基本的设置。\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    # 定义日志和模型保存的路径\n",
    "    \"base_dir\": \"${gym_env.env_name}/myalgo_${current_time:}\",\n",
    "\n",
    "    # Gymnasium 环境\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "\n",
    "    # 算法相关配置\n",
    "    \"algorithm\": {\n",
    "        # 随机数生成器的种子\n",
    "        \"seed\": 1023,\n",
    "\n",
    "        # 并行训练环境的数量\n",
    "        \"n_envs\": 8,\n",
    "                \n",
    "        # 两次评估之间的最小步数\n",
    "        \"eval_interval\": 500,\n",
    "        \n",
    "        # 并行评估环境的数量\n",
    "        \"nb_evals\": 10,\n",
    "\n",
    "        # 最大训练周期（循环）次数\n",
    "        \"max_epochs\": 40000,\n",
    "\n",
    "        # 每次迭代的步数\n",
    "        \"n_steps\": 100,\n",
    "    },\n",
    "}\n",
    "\n",
    "# 创建配置对象，如 cfg.algorithm.nb_evals 为 10\n",
    "cfg = OmegaConf.create(params)\n",
    "```\n",
    "\n",
    "#### 强化学习算法\n",
    "\n",
    "在此笔记本中，RL（强化学习）算法基于 `EpisodicAlgo` 类。该类用于定义带有“episode”概念的算法环境。为了使用这种环境，只需继承 `EpisodicAlgo` 并定义两个核心内容，即 `train_policy` 和 `eval_policy`。这两个策略均为 BBRL 代理（agent），根据环境状态选择需要执行的动作。\n",
    "\n",
    "```python\n",
    "class MyAlgo(EpisodicAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        # 定义训练和评估策略\n",
    "        # （这些代理会计算工作空间的 `action` 变量）\n",
    "        self.train_policy = MyPolicyAgent(...)\n",
    "        self.eval_policy = MyEvalAgent(...)\n",
    "\n",
    "algo = MyAlgo(cfg)\n",
    "```\n",
    "\n",
    "`EpisodicAlgo` 定义了一些有用的对象：\n",
    "\n",
    "- `algo.cfg`：配置文件对象\n",
    "- `algo.nb_steps`（整数）：从训练开始以来的步数\n",
    "- `algo.logger`：日志记录器，可用于收集训练期间的统计数据：\n",
    "    - `algo.logger.add_log(\"critic_loss\", critic_loss, algo.nb_steps)` 会在 tensorboard 上记录 `critic_loss` 值\n",
    "- `algo.evaluate()`：评估当前 `eval_policy`（如有必要），并保留迄今为止表现最佳的代理（根据平均累积奖励）；\n",
    "- `algo.visualize_best()`：在一个 episode 中运行最佳代理，并展示视频。\n",
    "\n",
    "此外，`EpisodicAlgo` 还定义了 `iter_episodes` 方法，允许对部分 episodes 进行迭代（`n_envs` 个环境中的 `n_steps` 步数）：\n",
    "\n",
    "```python\n",
    "# 使用部分 episodes\n",
    "for workspace in algo.iter_partial_episodes():\n",
    "    # workspace 是包含 50 个转换（transitions）的工作空间\n",
    "    # （带自动重置）\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9685b3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## The SquashedGaussianActor\n",
    "\n",
    "SAC works better with a Squashed Gaussian actor, which transforms a gaussian\n",
    "distribution with a $tanh$. The computation of the gradient  uses the\n",
    "reparametrization trick. Note that our attempts to use a\n",
    "`TunableVarianceContinuousActor` as we did for instance in the notebook about\n",
    "PPO completely failed. Such failure is also documented in the [OpenAI spinning\n",
    "up documentation page about\n",
    "SAC](https://spinningup.openai.com/en/latest/algorithms/sac.html).\n",
    "\n",
    "The code of the `SquashedGaussianActor` actor is below.\n",
    "\n",
    "The fact that we use the reparametrization trick is hidden inside the code of\n",
    "this distribution. You can read more about the reparametrization trick in at\n",
    "the following URLs:\n",
    "- [Goker Erdogan's\n",
    "  blog](http://gokererdogan.github.io/2016/07/01/reparameterization-trick/)\n",
    "  which shows the variance of different tricks to compute gradient of\n",
    "  expectations for $\\mathbb{E}(x^2)$ where $x \\sim \\mathcal{N}(\\theta, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7650cc6c",
   "metadata": {},
   "source": [
    "### SquashedGaussianActor（压缩高斯Actor）\n",
    "\n",
    "SAC（软演员-评论家）算法在使用“压缩高斯” actor 时效果更佳。这种 actor 使用 $ \\tanh $ 函数对高斯分布进行转换，同时计算梯度时使用了“重参数化技巧”（reparameterization trick）。需要注意的是，我们曾尝试在类似 PPO 笔记本中使用 `TunableVarianceContinuousActor`，但完全失败了。类似的失败在 [OpenAI Spinning Up 关于 SAC 的文档](https://spinningup.openai.com/en/latest/algorithms/sac.html)中也有提及。\n",
    "\n",
    "以下是 `SquashedGaussianActor` 的代码。\n",
    "\n",
    "我们在代码中使用了重参数化技巧，但这一过程在分布的代码实现中被隐藏了。您可以通过以下链接了解更多关于重参数化技巧的信息：\n",
    "- [Goker Erdogan 的博客](http://gokererdogan.github.io/2016/07/01/reparameterization-trick/)，展示了不同技巧在计算期望梯度时的方差，例如对于 $\\mathbb{E}(x^2)$，其中 $x \\sim \\mathcal{N}(\\theta, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ced72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquashedGaussianActor(Agent):  # 定义一个继承自Agent的SquashedGaussianActor类，用于构建SAC的策略网络\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim, min_std=1e-4):\n",
    "        \"\"\"Creates a new Squashed Gaussian actor\n",
    "\n",
    "        :param state_dim: The dimension of the state space\n",
    "        :param hidden_layers: Hidden layer sizes\n",
    "        :param action_dim: The dimension of the action space\n",
    "        :param min_std: The minimum standard deviation, defaults to 1e-4\n",
    "        \"\"\"\n",
    "        super().__init__()  # 调用父类Agent的初始化函数\n",
    "        self.min_std = min_std  # 设置最小标准差，防止模型训练中标准差过小导致数值问题\n",
    "        backbone_dim = [state_dim] + list(hidden_layers)  # 将状态维度和隐藏层维度组合成列表，以构建MLP\n",
    "        self.layers = build_mlp(backbone_dim, activation=nn.ReLU())  # 构建多层感知机(MLP)，各层激活函数为ReLU\n",
    "        self.backbone = nn.Sequential(*self.layers)  # 将所有层连接成一个有序神经网络模型\n",
    "        self.last_mean_layer = nn.Linear(hidden_layers[-1], action_dim)  # 定义最后一层用于输出动作均值的线性层\n",
    "        self.last_std_layer = nn.Linear(hidden_layers[-1], action_dim)  # 定义最后一层用于输出动作标准差的线性层\n",
    "        self.softplus = nn.Softplus()  # 使用Softplus激活函数保证标准差为正数\n",
    "\n",
    "        # cache_size avoids numerical infinites or NaNs when\n",
    "        # computing log probabilities\n",
    "        self.tanh_transform = TanhTransform(cache_size=1)  # 定义tanh变换，用于将动作限制在[-1, 1]区间\n",
    "\n",
    "    def normal_dist(self, obs: torch.Tensor):  # 定义正态分布函数，接收观测值obs\n",
    "        \"\"\"Compute normal distribution given observation(s)\"\"\"\n",
    "\n",
    "        backbone_output = self.backbone(obs)  # 将观测输入传入backbone网络，获取输出特征\n",
    "        mean = self.last_mean_layer(backbone_output)  # 计算动作的均值\n",
    "        std_out = self.last_std_layer(backbone_output)  # 计算动作的标准差\n",
    "        std = self.softplus(std_out) + self.min_std  # 应用Softplus并加上最小标准差，确保标准差为正\n",
    "        # Independent ensures that we have a multivariate\n",
    "        # Gaussian with a diagonal covariance matrix (given as\n",
    "        # a vector `std`)\n",
    "        return Independent(Normal(mean, std), 1)  # 返回以mean和std为参数的独立正态分布\n",
    "\n",
    "    def forward(self, t, stochastic=True):  # 前向传播函数，计算给定时间t下的动作及其对数概率\n",
    "        \"\"\"Computes the action a_t and its log-probability p(a_t| s_t)\n",
    "\n",
    "        :param stochastic: True when sampling\n",
    "        \"\"\"\n",
    "        normal_dist = self.normal_dist(self.get((\"env/env_obs\", t)))  # 根据观测生成正态分布\n",
    "        action_dist = TransformedDistribution(normal_dist, [self.tanh_transform])  # 应用tanh变换，生成动作分布\n",
    "        if stochastic:\n",
    "            # Uses the re-parametrization trick\n",
    "            action = action_dist.rsample()  # 使用重参数化技巧从分布中采样动作\n",
    "        else:\n",
    "            # Directly uses the mode of the distribution\n",
    "            action = self.tanh_transform(normal_dist.mode)  # 使用分布的模式作为动作\n",
    "\n",
    "        log_prob = action_dist.log_prob(action)  # 计算动作的对数概率\n",
    "        # This line allows to deepcopy the actor...\n",
    "        self.tanh_transform._cached_x_y = [None, None]  # 清除缓存，确保actor可以被deepcopy\n",
    "        self.set((\"action\", t), action)  # 存储动作\n",
    "        self.set((\"action_logprobs\", t), log_prob)  # 存储动作的对数概率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f5343",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Critic agent Q(s,a)\n",
    "\n",
    "As critics and target critics, SAC uses several instances of ContinuousQAgent\n",
    "class, as DDPG and TD3. See the [DDPG\n",
    "notebook](http://master-dac.isir.upmc.fr/rld/rl/04-ddpg-td3.student.ipynb) for\n",
    "details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b0326",
   "metadata": {},
   "source": [
    "### Critic 代理 Q(s, a)\n",
    "\n",
    "在 SAC 算法中，作为评论家（Critic）和目标评论家（Target Critic），使用了多个 `ContinuousQAgent` 类的实例，类似于 DDPG 和 TD3 算法。详情可以参考 [DDPG 笔记本](http://master-dac.isir.upmc.fr/rld/rl/04-ddpg-td3.student.ipynb)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5b1f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousQAgent(Agent):\n",
    "    def __init__(self, state_dim: int, hidden_layers: list[int], action_dim: int):\n",
    "        \"\"\"创建一个新的 Q 函数评论家代理: Q(s, a)\n",
    "\n",
    "        :param state_dim: 状态空间的维数（观测的维数）\n",
    "        :param hidden_layers: 神经网络的隐藏层大小列表\n",
    "        :param action_dim: 动作空间的维数\n",
    "        \"\"\"\n",
    "        super().__init__()  # 调用父类的初始化方法\n",
    "        self.is_q_function = True  # 标记该代理为Q函数\n",
    "        # 使用给定的状态维度和动作维度构建一个多层感知机（MLP）模型\n",
    "        # 输入层大小为 状态维度 + 动作维度，输出层大小为 1（Q值）\n",
    "        self.model = build_mlp(\n",
    "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        # 获取在时间步t的环境观测（状态）\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        # 获取在时间步t的动作\n",
    "        action = self.get((\"action\", t))\n",
    "        # 将状态和动作连接起来作为模型输入\n",
    "        obs_act = torch.cat((obs, action), dim=1)\n",
    "        # 使用模型计算Q值，squeeze(-1)用于移除多余的维度\n",
    "        q_value = self.model(obs_act).squeeze(-1)\n",
    "        # 将计算得到的Q值存储在字典中，以备后续使用\n",
    "        self.set((f\"{self.prefix}q_value\", t), q_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18ba7c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Building the complete training and evaluation agents\n",
    "\n",
    "In the code below we create the Squashed Gaussian actor, two critics and the\n",
    "corresponding target critics. Beforehand, we checked that the environment\n",
    "takes continuous actions (otherwise we would need a different code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1db61a",
   "metadata": {},
   "source": [
    "### 构建完整的训练和评估代理\n",
    "\n",
    "在下面的代码中，我们创建了压缩高斯 actor、两个评论家（Critic）以及对应的目标评论家。在此之前，我们已经确认环境接收连续动作（否则我们需要使用不同的代码）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb4a17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建SAC算法环境类\n",
    "class SACAlgo(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)  # 调用父类的初始化方法，传入配置参数cfg\n",
    "\n",
    "        # 获取状态空间和动作空间的大小\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "        # 断言动作空间是否为连续型，若不是则报错提示\n",
    "        assert (\n",
    "            self.train_env.is_continuous_action()\n",
    "        ), \"SAC代码专用于连续动作空间\"\n",
    "\n",
    "        # 创建一个actor（策略网络）\n",
    "        self.actor = SquashedGaussianActor(\n",
    "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "        )\n",
    "\n",
    "        # 创建第一个评论家网络（critic_1）来估计Q值\n",
    "        self.critic_1 = ContinuousQAgent(\n",
    "            obs_size,  # 状态空间的大小\n",
    "            cfg.algorithm.architecture.critic_hidden_size,  # 评论家网络的隐藏层大小\n",
    "            act_size,  # 动作空间的大小\n",
    "        ).with_prefix(\"critic-1/\")  # 添加前缀以区分网络\n",
    "\n",
    "        # 创建目标评论家网络target_critic_1，作为critic_1的深拷贝\n",
    "        self.target_critic_1 = copy.deepcopy(self.critic_1).with_prefix(\n",
    "            \"target-critic-1/\"\n",
    "        )\n",
    "\n",
    "        # 创建第二个评论家网络critic_2，作为SAC的双重Q网络\n",
    "        self.critic_2 = ContinuousQAgent(\n",
    "            obs_size,\n",
    "            cfg.algorithm.architecture.critic_hidden_size,\n",
    "            act_size,\n",
    "        ).with_prefix(\"critic-2/\")\n",
    "\n",
    "        # 创建目标评论家网络target_critic_2，作为critic_2的深拷贝\n",
    "        self.target_critic_2 = copy.deepcopy(self.critic_2).with_prefix(\n",
    "            \"target-critic-2/\"\n",
    "        )\n",
    "\n",
    "        # 训练策略网络的引用，指向actor\n",
    "        self.train_policy = self.actor\n",
    "        # 评估策略网络的引用，使用KWAgentWrapper封装actor，并设定stochastic=False，即使用确定性策略\n",
    "        self.eval_policy = KWAgentWrapper(self.actor, stochastic=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbdc189",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "For the entropy coefficient optimizer, the code is as follows. Note the trick\n",
    "which consists in using the log of this entropy coefficient. This trick was\n",
    "taken from the Stable baselines3 implementation of SAC, which is explained in\n",
    "[this\n",
    "notebook](https://colab.research.google.com/drive/12LER1_ShWOa_UhOL1nlX-LX_t5KQK9LV?usp=sharing).\n",
    "\n",
    "Tuning $\\alpha$ in SAC is an option. To chose to tune it, the `target_entropy`\n",
    "argument in the parameters should be `auto`. The initial value is given\n",
    "through the `entropy_coef` parameter. For any other value than `auto`, the\n",
    "value of $\\alpha$ will stay constant and correspond to the `entropy_coef`\n",
    "parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117b57ed",
   "metadata": {},
   "source": [
    "对于熵系数优化器，其代码如下。需要注意其中的小技巧，即使用该熵系数的对数。这一技巧来源于 Stable Baselines3 的 SAC 实现，详细解释请参考[这个笔记本](https://colab.research.google.com/drive/12LER1_ShWOa_UhOL1nlX-LX_t5KQK9LV?usp=sharing)。\n",
    "\n",
    "在 SAC 中调整 $\\alpha$ 是可选项。若选择调整它，则在参数中应将 `target_entropy` 设置为 `auto`，初始值通过 `entropy_coef` 参数给定。对于 `auto` 以外的任何值，$\\alpha$ 的值将保持不变，并与 `entropy_coef` 参数一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fceb55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_entropy_optimizers(cfg):\n",
    "    # 定义设置熵优化器的函数，参数为配置文件 `cfg`\n",
    "\n",
    "    if cfg.algorithm.entropy_mode == \"auto\":\n",
    "        # 如果配置中的熵模式为自动模式 (\"auto\")，则进行以下操作：\n",
    "\n",
    "        # 注释：优化熵系数的对数值，这略微不同于原论文中的做法，\n",
    "        # 详细讨论见 https://github.com/rail-berkeley/softlearning/issues/37\n",
    "        # 此注释和代码参考自稳定基线3（Stable Baselines3）的SAC实现版本\n",
    "\n",
    "        log_entropy_coef = nn.Parameter(\n",
    "            torch.log(torch.ones(1) * cfg.algorithm.init_entropy_coef)\n",
    "        )  # 定义一个可学习的参数log_entropy_coef，用于存储初始熵系数的对数值\n",
    "           # torch.log(torch.ones(1) * cfg.algorithm.init_entropy_coef) 将初始熵系数取对数以便直接优化其对数值\n",
    "\n",
    "        # 调用 `setup_optimizer` 函数为 `log_entropy_coef` 参数设置优化器\n",
    "        entropy_coef_optimizer = setup_optimizer(\n",
    "            cfg.entropy_coef_optimizer, log_entropy_coef\n",
    "        )\n",
    "\n",
    "        # 返回熵系数优化器 `entropy_coef_optimizer` 和 `log_entropy_coef` 参数\n",
    "        return entropy_coef_optimizer, log_entropy_coef\n",
    "    else:\n",
    "        # 如果熵模式不是自动模式，则返回两个 `None` 值，表示不进行熵系数优化\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8690cb0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Compute the critic loss\n",
    "\n",
    "With the notations of my slides, the equation corresponding to Eq. (5) and (6)\n",
    "in [this paper](https://arxiv.org/pdf/1812.05905.pdf) becomes:\n",
    "\n",
    "$$ loss_{Q_{\\boldsymbol{\\phi}_i}}({\\boldsymbol{\\theta}}) = {\\mathbb{E}}_{(\\mathbf{s}_t, \\mathbf{a}_t, \\mathbf{s}_{t+1}) \\sim\n",
    "\\mathcal{D}}\\left[\\left( r(\\mathbf{s}_t, \\mathbf{a}_t) + \\gamma {\\mathbb{E}}_{\\mathbf{a} \\sim\n",
    "\\pi_{\\boldsymbol{\\theta}}(.|\\mathbf{s}_{t+1})} \\left[\n",
    "\\min_{j\\in 1,2} \\hat{Q}^{\\mathrm{target}}_{\\boldsymbol{\\phi}_j}(\\mathbf{s}_{t+1}, \\mathbf{a}) - \\alpha\n",
    "\\log{\\pi_{\\boldsymbol{\\theta}}(\\mathbf{a}|\\mathbf{s}_{t+1})} \\right] - \\hat{Q}_{\\boldsymbol{\\phi}_i}(\\mathbf{s}_t, \\mathbf{a}_t) \\right)^2\n",
    "\\right] $$\n",
    "\n",
    "An important information in the above equation and the one about the actor\n",
    "loss below is the index of the expectations. These indexes tell us where the\n",
    "data should be taken from. In the above equation, one can see that the index\n",
    "of the outer expectation is over samples taken from the replay buffer, whereas\n",
    "in the inner expectation we consider actions from the current actor at the\n",
    "next state $s_{t+1}$.\n",
    "\n",
    "Thus, to compute the inner expectation, one needs to determine what actions\n",
    "the current actor would take in the next state of each sample. This is what\n",
    "the line\n",
    "\n",
    "`t_actor(rb_workspace, t=1, n_steps=1, stochastic=True)`\n",
    "\n",
    "does. The parameter `t=1` (instead of 0) ensures that we consider the next\n",
    "state $s_{t+1}$.\n",
    "\n",
    "Once we have determined these actions, we can determine their Q-values and\n",
    "their log probabilities, to compute the inner expectation.\n",
    "\n",
    "Note that at this stage, we only determine the log probabilities corresponding\n",
    "to actions taken at the next time step, by contrast with what we do for the\n",
    "actor in the `compute_actor_loss(...)` function later on.\n",
    "\n",
    "Finally, once we have computed the $$\n",
    "\\hat{Q}_{\\boldsymbol{\\phi}}(\\mathbf{s}_{t+1},\n",
    "\\mathbf{a}) $$ for both critics, we take the min and store it into\n",
    "`post_q_values`. By contrast, the Q-values corresponding to the last term of\n",
    "the equation are taken from the replay buffer, they are computed in the\n",
    "beginning of the function by applying the Q agents to the replay buffer\n",
    "*before* changing the action to that of the current actor.\n",
    "\n",
    "An important remark is that, if the entropy coefficient $\\alpha$ corresponding\n",
    "to the `ent_coef` variable is set to 0, then we retrieve exactly the critic\n",
    "loss computation function of the TD3 algorithm. As we will see later, this is\n",
    "also true of the actor loss computation.\n",
    "\n",
    "This remark proved very useful in debugging the SAC code. We have set\n",
    "`ent_coef` to 0 and ensured the behavior was strictly the same as the behavior\n",
    "of TD3.\n",
    "\n",
    "Note also that we compute the loss for two critics (initialized\n",
    "independently), and use two target critics (using the minimum of their\n",
    "prediction as the basis of the target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6345845e",
   "metadata": {},
   "source": [
    "### 计算评论家（Critic）损失\n",
    "\n",
    "在我的幻灯片中使用的符号中，公式 (5) 和 (6) 对应于[此论文](https://arxiv.org/pdf/1812.05905.pdf)中的方程为：\n",
    "\n",
    "$$\n",
    "\\text{loss}_{Q_{\\boldsymbol{\\phi}_i}}(\\boldsymbol{\\theta}) = \\mathbb{E}_{(\\mathbf{s}_t, \\mathbf{a}_t, \\mathbf{s}_{t+1}) \\sim \\mathcal{D}}\\left[\\left( r(\\mathbf{s}_t, \\mathbf{a}_t) + \\gamma \\mathbb{E}_{\\mathbf{a} \\sim \\pi_{\\boldsymbol{\\theta}}(.|\\mathbf{s}_{t+1})} \\left[ \\min_{j \\in 1,2} \\hat{Q}^{\\text{target}}_{\\boldsymbol{\\phi}_j}(\\mathbf{s}_{t+1}, \\mathbf{a}) - \\alpha \\log{\\pi_{\\boldsymbol{\\theta}}(\\mathbf{a}|\\mathbf{s}_{t+1})} \\right] - \\hat{Q}_{\\boldsymbol{\\phi}_i}(\\mathbf{s}_t, \\mathbf{a}_t) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "在上面的公式和接下来关于 actor 损失的公式中，期望值的索引非常重要。这些索引表明数据应该从哪里获取。在上面的公式中，可以看到外部期望是针对从重放缓冲区中提取的样本，而在内部期望中，我们在下一状态 $ s_{t+1} $ 中使用当前 actor 的动作。\n",
    "\n",
    "因此，为了计算内部期望，需要确定当前 actor 在每个样本的下一状态中会采取哪些动作。这可以通过以下代码行完成：\n",
    "\n",
    "```python\n",
    "t_actor(rb_workspace, t=1, n_steps=1, stochastic=True)\n",
    "```\n",
    "\n",
    "参数 `t=1`（而非 0）确保我们考虑的是下一状态 $ s_{t+1} $。\n",
    "\n",
    "一旦确定了这些动作，我们可以计算它们的 Q 值和对数概率，从而计算内部期望。\n",
    "\n",
    "请注意，在此阶段，我们只确定在下一时间步采取的动作对应的对数概率，这与稍后在 `compute_actor_loss(...)` 函数中对 actor 所做的处理不同。\n",
    "\n",
    "最后，在计算出两个评论家 $ \\hat{Q}_{\\boldsymbol{\\phi}}(\\mathbf{s}_{t+1}, \\mathbf{a}) $ 的值后，我们取它们的最小值并存储在 `post_q_values` 中。相反，公式中最后一项的 Q 值来自重放缓冲区，在函数开始时就已通过将 Q 代理应用于重放缓冲区计算出来 *（在将动作更改为当前 actor 的动作之前）*。\n",
    "\n",
    "需要注意的是，如果熵系数 $\\alpha$（对应 `ent_coef` 变量）设置为 0，那么我们得出的正是 TD3 算法中的评论家损失计算公式。稍后可以看到，这对 actor 损失的计算同样适用。\n",
    "\n",
    "这个观察在调试 SAC 代码时非常有用。我们将 `ent_coef` 设置为 0，确保其行为与 TD3 的行为完全一致。\n",
    "\n",
    "还需注意的是，我们为两个独立初始化的评论家计算损失，并使用两个目标评论家（将其预测的最小值作为目标的基础）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fe417e6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_critic_loss(\n",
    "    cfg,\n",
    "    reward: torch.Tensor,\n",
    "    must_bootstrap: torch.Tensor,\n",
    "    t_actor: TemporalAgent,\n",
    "    t_q_agents: TemporalAgent,\n",
    "    t_target_q_agents: TemporalAgent,\n",
    "    rb_workspace: Workspace,\n",
    "    ent_coef: torch.Tensor,\n",
    "):\n",
    "    r\"\"\"Computes the critic loss for a set of $S$ transition samples\n",
    "\n",
    "    Args:\n",
    "        cfg: The experimental configuration\n",
    "        reward: Tensor (2xS) of rewards\n",
    "        must_bootstrap: Tensor (2xS) of indicators\n",
    "        t_actor: The actor agent\n",
    "        t_q_agents: The critics\n",
    "        t_target_q_agents: The target of the critics\n",
    "        rb_workspace: The transition workspace\n",
    "        ent_coef: The entropy coefficient $\\alpha$\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: The two critic losses (scalars)\n",
    "    \"\"\"\n",
    "\n",
    "    # Replay the actor so we get the necessary statistics\n",
    "    \n",
    "    # Compute q_values from both critics with the actions present in the buffer:\n",
    "    # at t, we have Q(s,a) from (s,a) in the RB\n",
    "    t_q_agents(rb_workspace, t=0, n_steps=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Replay the current actor on the replay buffer to get actions of the current actor\n",
    "        t_actor(rb_workspace, t=1, n_steps=1)\n",
    "        action_logprobs_next = rb_workspace[\"action_logprobs\"]\n",
    "\n",
    "        # Compute target q_values from both target critics: at t+1, we have\n",
    "        # Q(s_{t+1}, a_{t+1}) from the (s_{t+1}, a_{t+1}) where a_{t+1} has been\n",
    "        # replaced in the RB with the t_actor line above\n",
    "        t_target_q_agents(rb_workspace, t=1, n_steps=1)\n",
    "\n",
    "    q_values_rb_1, q_values_rb_2, post_q_values_1, post_q_values_2 = rb_workspace[\n",
    "        \"critic-1/q_value\",\n",
    "        \"critic-2/q_value\",\n",
    "        \"target-critic-1/q_value\",\n",
    "        \"target-critic-2/q_value\"\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Compute temporal difference\n",
    "\n",
    "    q_next = torch.minimum(post_q_values_1[1], post_q_values_2[1])\n",
    "    v_phi = q_next - ent_coef * action_logprobs_next[1]\n",
    "\n",
    "    target = reward[1] + cfg.algorithm.discount_factor * v_phi * must_bootstrap[1].int()\n",
    "    critic_loss_1 = nn.functional.mse_loss(q_values_rb_1[0], target)\n",
    "    critic_loss_2 = nn.functional.mse_loss(q_values_rb_2[0], target)\n",
    "    \n",
    "\n",
    "    return critic_loss_1, critic_loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d19c7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Compute the actor Loss\n",
    "\n",
    "With the notations of my slides, the equation of the actor loss corresponding\n",
    "to Eq. (7) in [this paper](https://arxiv.org/pdf/1812.05905.pdf) becomes:\n",
    "\n",
    "$$ loss_\\pi({\\boldsymbol{\\theta}}) = {\\mathbb{E}}_{\\mathbf{s}_t \\sim\n",
    "\\mathcal{D}}\\left[ {\\mathbb{E}}_{\\mathbf{a}_t\\sim\n",
    "\\pi_{\\boldsymbol{\\theta}}(.|\\mathbf{s}_t)} \\left[ \\alpha\n",
    "\\log{\\pi_{\\boldsymbol{\\theta}}(\\mathbf{a}_t|\\mathbf{s}_t) -\n",
    "\\hat{Q}_{\\boldsymbol{\\phi}_{i}}(\\mathbf{s}_t,\n",
    "\\mathbf{a}_t)} \\right] \\right] $$\n",
    "\n",
    "Note that [the paper](https://arxiv.org/pdf/1812.05905.pdf) mistakenly writes\n",
    "$Q_\\theta(s_t,s_t)$\n",
    "\n",
    "As for the critic loss, we have two expectations, one over the states from the\n",
    "replay buffer, and one over the actions of the current actor. Thus we need to\n",
    "apply again the current actor to the content of the replay buffer.\n",
    "\n",
    "But this time, we consider the current state, thus we parametrize it with\n",
    "`t=0` and `n_steps=1`. This way, we get the log probabilities and Q-values at\n",
    "the current step.\n",
    "\n",
    "A nice thing is that this way, there is no overlap between the log probability\n",
    "data used to update the critic and the actor, which avoids having to 'retain'\n",
    "the computation graph so that it can be reused for the actor and the critic.\n",
    "\n",
    "This small trick is one of the features that makes coding SAC the most\n",
    "difficult.\n",
    "\n",
    "Again, once we have computed the Q values over both critics, we take the min\n",
    "and put it into `current_q_values`.\n",
    "\n",
    "As for the critic loss, if we set `ent_coef` to 0, we retrieve the actor loss\n",
    "function of DDPG and TD3, which simply tries to get actions that maximize the\n",
    "Q values (by minimizing -Q)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf92c0",
   "metadata": {},
   "source": [
    "### 计算 Actor 损失\n",
    "\n",
    "在我的幻灯片符号中，actor 损失对应于[此论文](https://arxiv.org/pdf/1812.05905.pdf)中的方程 (7)：\n",
    "\n",
    "$$\n",
    "\\text{loss}_\\pi(\\boldsymbol{\\theta}) = \\mathbb{E}_{\\mathbf{s}_t \\sim \\mathcal{D}} \\left[ \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_{\\boldsymbol{\\theta}}(.|\\mathbf{s}_t)} \\left[ \\alpha \\log{\\pi_{\\boldsymbol{\\theta}}(\\mathbf{a}_t|\\mathbf{s}_t)} - \\hat{Q}_{\\boldsymbol{\\phi}_{i}}(\\mathbf{s}_t, \\mathbf{a}_t) \\right] \\right]\n",
    "$$\n",
    "\n",
    "请注意，[该论文](https://arxiv.org/pdf/1812.05905.pdf)中错误地写成了 $ Q_\\theta(s_t, s_t) $。\n",
    "\n",
    "和评论家损失一样，这里也有两个期望值，一个是重放缓冲区中的状态，另一个是当前 actor 的动作。因此，我们需要再次将当前 actor 应用于重放缓冲区的内容。\n",
    "\n",
    "不过这次我们关注的是当前状态，因此使用参数 `t=0` 和 `n_steps=1`。通过这种方式，我们可以在当前时间步得到对数概率和 Q 值。\n",
    "\n",
    "这样做的好处是，更新评论家和 actor 时使用的对数概率数据不重叠，避免了需要“保留”计算图以便在评论家和 actor 间重复使用。\n",
    "\n",
    "这个小技巧是实现 SAC 时最复杂的部分之一。\n",
    "\n",
    "同样地，计算出两个评论家的 Q 值后，我们取其最小值并存入 `current_q_values`。\n",
    "\n",
    "和评论家损失一样，如果将 `ent_coef` 设置为 0，那么我们得到的就是 DDPG 和 TD3 的 actor 损失函数，它简单地通过最小化 $-Q$ 来尝试获得能够最大化 Q 值的动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eeb9dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_actor_loss(\n",
    "    ent_coef, t_actor: TemporalAgent, t_q_agents: TemporalAgent, rb_workspace: Workspace\n",
    "):\n",
    "    r\"\"\"\n",
    "    Actor loss computation\n",
    "    :param ent_coef: The entropy coefficient $\\alpha$\n",
    "    :param t_actor: The actor agent (temporal agent)\n",
    "    :param t_q_agents: The critics (as temporal agent)\n",
    "    :param rb_workspace: The replay buffer (2 time steps, $t$ and $t+1$)\n",
    "    \"\"\"\n",
    "\n",
    "    # Recompute the action with the current actor (at $a_t$)\n",
    "\n",
    "    # Step 1: 使用当前 actor 重新计算当前状态下的动作 a_t 和对数概率\n",
    "    t_actor(rb_workspace, t=0, n_steps=1, stochastic=True)\n",
    "    action_logprobs_new = rb_workspace[\"action_logprobs\"]\n",
    "\n",
    "    # Compute Q-values\n",
    "\n",
    "    # Step 2: 使用 Critic 计算当前状态下的 Q 值\n",
    "    t_q_agents(rb_workspace, t=0, n_steps=1)\n",
    "    q_values_1 = rb_workspace[\"critic-1/q_value\"]\n",
    "    q_values_2 = rb_workspace[\"critic-2/q_value\"]\n",
    "\n",
    "    # Step 3: 取两个 Q 值的最小值，减少估值偏差\n",
    "    current_q_values = torch.min(q_values_1, q_values_2)\n",
    "\n",
    "    # Compute the actor loss\n",
    "    # Step 4: 计算 actor 损失\n",
    "\n",
    "    actor_loss = ent_coef * action_logprobs_new[0] - current_q_values[0]\n",
    "    \n",
    "    return actor_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a6afa4",
   "metadata": {},
   "source": [
    "## Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8492e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def run_sac(sac: SACAlgo):\n",
    "    cfg = sac.cfg\n",
    "    logger = sac.logger\n",
    "\n",
    "    # init_entropy_coef is the initial value of the entropy coef alpha\n",
    "    ent_coef = cfg.algorithm.init_entropy_coef\n",
    "    tau = cfg.algorithm.tau_target\n",
    "\n",
    "    # Creates the temporal actors\n",
    "    t_actor = TemporalAgent(sac.train_policy)\n",
    "    q_agents = TemporalAgent(Agents(sac.critic_1, sac.critic_2))\n",
    "    target_q_agents = TemporalAgent(Agents(sac.target_critic_1, sac.target_critic_2))\n",
    "\n",
    "    # Configure the optimizer\n",
    "    actor_optimizer = setup_optimizer(cfg.actor_optimizer, sac.actor)\n",
    "    critic_optimizer = setup_optimizer(cfg.critic_optimizer, sac.critic_1, sac.critic_2)\n",
    "    entropy_coef_optimizer, log_entropy_coef = setup_entropy_optimizers(cfg) \n",
    "\n",
    "    # If entropy_mode is not auto, the entropy coefficient ent_coef remains\n",
    "    # fixed. Otherwise, computes the target entropy\n",
    "    if cfg.algorithm.entropy_mode == \"auto\":\n",
    "        # target_entropy is \\mathcal{H}_0 in the SAC and aplications paper.\n",
    "        target_entropy = -np.prod(sac.train_env.action_space.shape).astype(np.float32)\n",
    "\n",
    "    # Loops over successive replay buffers\n",
    "    for rb in sac.iter_replay_buffers():\n",
    "\n",
    "        # Implement the SAC algorithm\n",
    "        rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n",
    "        terminated, reward = rb_workspace[\"env/terminated\", \"env/reward\"]\n",
    "\n",
    "        must_boostrap = ~terminated\n",
    "\n",
    "        if entropy_coef_optimizer is not None:\n",
    "            ent_coef = torch.exp(log_entropy_coef.detach())\n",
    "\n",
    "        # Critic update part #############################\n",
    "        (critic_loss_1, critic_loss_2) = compute_critic_loss(\n",
    "            cfg = cfg,\n",
    "            reward = reward,\n",
    "            must_bootstrap = must_boostrap,\n",
    "            t_actor = t_actor,\n",
    "            t_q_agents = q_agents,\n",
    "            t_target_q_agents = target_q_agents,\n",
    "            rb_workspace = rb_workspace,\n",
    "            ent_coef = ent_coef\n",
    "        )\n",
    "\n",
    "        # 记录 Critic 损失\n",
    "        logger.add_log(\"critic_loss_1\", critic_loss_1, sac.nb_steps)\n",
    "        logger.add_log(\"critic_loss_2\", critic_loss_2, sac.nb_steps)\n",
    "\n",
    "        # 反向传播并更新 Critic 参数\n",
    "        critic_loss = critic_loss_1 + critic_loss_2\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            sac.critic_1.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            sac.critic_2.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        # Actor update part #############################\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss = compute_actor_loss(\n",
    "            ent_coef = ent_coef,\n",
    "            t_actor = t_actor,\n",
    "            t_q_agents = q_agents,\n",
    "            rb_workspace = rb_workspace\n",
    "        )\n",
    "\n",
    "        # 记录 Actor 损失\n",
    "        logger.add_log(\"actor_loss\", actor_loss, sac.nb_steps)\n",
    "\n",
    "        # 反向传播并更新 Actor 参数\n",
    "        actor_loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            sac.actor.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        # Entropy optimizer part\n",
    "        if entropy_coef_optimizer is not None:\n",
    "            # See Eq. (17) of the SAC and Applications paper. The log\n",
    "            # probabilities *must* have been computed when computing the actor\n",
    "            # loss.\n",
    "            action_logprobs_rb = rb_workspace[\"action_logprobs\"].detach()\n",
    "            entropy_coef_loss = -(\n",
    "                log_entropy_coef.exp() * (action_logprobs_rb + target_entropy)\n",
    "            ).mean()\n",
    "            entropy_coef_optimizer.zero_grad()\n",
    "            entropy_coef_loss.backward()\n",
    "            entropy_coef_optimizer.step()\n",
    "            logger.add_log(\"entropy_coef_loss\", entropy_coef_loss, sac.nb_steps)\n",
    "            logger.add_log(\"entropy_coef\", ent_coef, sac.nb_steps)\n",
    "\n",
    "        ####################################################\n",
    "\n",
    "        # Soft update of target q function\n",
    "        soft_update_params(sac.critic_1, sac.target_critic_1, tau)\n",
    "        soft_update_params(sac.critic_2, sac.target_critic_2, tau)\n",
    "\n",
    "        agents.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8c2d0",
   "metadata": {},
   "source": [
    "## Definition of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db6869b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"save_best\": True,\n",
    "    \"base_dir\": \"${gym_env.env_name}/sac-S${algorithm.seed}_${current_time:}\",\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 1,\n",
    "        \"n_envs\": 8,\n",
    "        \"n_steps\": 32,\n",
    "        \"buffer_size\": 1e6,\n",
    "        \"batch_size\": 256,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"nb_evals\": 16,\n",
    "        \"eval_interval\": 2_000,\n",
    "        \"learning_starts\": 10_000,\n",
    "        \"max_epochs\": 2_000,\n",
    "        \"discount_factor\": 0.98,\n",
    "        \"entropy_mode\": \"auto\",  # \"auto\" or \"fixed\"\n",
    "        \"init_entropy_coef\": 2e-7,\n",
    "        \"tau_target\": 0.05,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [64, 64],\n",
    "            \"critic_hidden_size\": [256, 256],\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\"env_name\": \"CartPoleContinuous-v1\"},\n",
    "    \"actor_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "    \"entropy_coef_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad3804",
   "metadata": {},
   "source": [
    "## Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc3fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73f79153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 752), started 35 days, 0:55:52 ago. (Use '!kill 752' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d665d5dc7638c45\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d665d5dc7638c45\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed2c4895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e44a5c147eb4c0faae4111ec327efe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agents = SACAlgo(OmegaConf.create(params))\n",
    "run_sac(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17f0fde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video of best agent recorded in outputs/CartPoleContinuous-v1/sac-S1_20241104-171044/best_agent.mp4\n",
      "Moviepy - Building video /home/chen_guanyu/M2A/M2A_RLD/outputs/CartPoleContinuous-v1/sac-S1_20241104-171044/best_agent.mp4.\n",
      "Moviepy - Writing video /home/chen_guanyu/M2A/M2A_RLD/outputs/CartPoleContinuous-v1/sac-S1_20241104-171044/best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chen_guanyu/M2A/M2A_RLD/outputs/CartPoleContinuous-v1/sac-S1_20241104-171044/best_agent.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div align=middle><video src='data:video/mp4;base64,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' controls>Sorry, seems like your browser doesn't support HTML5 audio/video</video></div>"
      ],
      "text/plain": [
       "<moviepy.video.io.html_tools.HTML2 object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the best policy\n",
    "agents.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac33e08",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- use the same code on the Pendulum-v1 environment. This one is harder to\n",
    "  tune. Get the parameters from the\n",
    "  [rl-baseline3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo) and see if\n",
    "  you manage to get SAC working on Pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d56ebc",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "- 在 `Pendulum-v1` 环境中使用相同的代码。这个环境更难调试。可以从 [rl-baseline3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo) 获取参数，看看能否让 SAC 在 `Pendulum` 环境中正常运行。"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
