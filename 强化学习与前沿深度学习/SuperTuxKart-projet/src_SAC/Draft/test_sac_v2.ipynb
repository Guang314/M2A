{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils>=0.5\").setup()\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from bbrl.agents import Agent\n",
    "from bbrl_utils.nn import build_mlp\n",
    "\n",
    "from torch.distributions import (\n",
    "    Normal,\n",
    "    Independent,\n",
    "    TransformedDistribution,\n",
    "    TanhTransform,\n",
    ")\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "from pystk2_gymnasium.definitions import ActionObservationWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只是模板，可以随意更改\n",
    "class MyWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, option: int):\n",
    "        super().__init__(env)\n",
    "        self.option = option\n",
    "\n",
    "    def action(self, action):\n",
    "        # We do nothing here\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义 Wrapper : 删除 observation space 中的 离散特征\n",
    "class OnlyContinousObservationWrapper(ActionObservationWrapper):\n",
    "    \"\"\"Removes discrete features from the observation space.\"\"\"\n",
    "    \n",
    "    def __init__(self, env: gym.Env, **kwargs):\n",
    "        super().__init__(env, **kwargs)\n",
    "        \n",
    "        # 过滤掉离散特征，只保留非离散特征\n",
    "        self._observation_space = env.observation_space['continuous']\n",
    "\n",
    "    def observation(self, observation: Dict):\n",
    "        return observation['continuous']\n",
    "    \n",
    "    def action(self, action):\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP 08-sac 中的 class SquashedGaussianActor(Agent)\n",
    "class SquashedGaussianActor(Agent):\n",
    "    \"\"\"Computes probabilities over action\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim, min_std=1e-4):\n",
    "        \"\"\"Creates a new Squashed Gaussian actor\n",
    "\n",
    "        :param state_dim: The dimension of the state space\n",
    "        :param hidden_layers: Hidden layer sizes\n",
    "        :param action_dim: The dimension of the action space\n",
    "        :param min_std: The minimum standard deviation, defaults to 1e-4\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.min_std = min_std\n",
    "        backbone_dim = [state_dim] + list(hidden_layers)\n",
    "        self.layers = build_mlp(backbone_dim, activation=nn.ReLU())\n",
    "        self.backbone = nn.Sequential(*self.layers)\n",
    "        self.last_mean_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
    "        self.last_std_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "        # cache_size avoids numerical infinites or NaNs when\n",
    "        # computing log probabilities\n",
    "        self.tanh_transform = TanhTransform(cache_size=1)\n",
    "\n",
    "    def normal_dist(self, obs:torch.Tensor):\n",
    "        \"\"\"compute normal distribution given observation(s)\"\"\"\n",
    "\n",
    "        backbone_output = self.backbone(obs)\n",
    "        mean = self.last_mean_layer(backbone_output)\n",
    "        std_out = self.last_std_layer(backbone_output)\n",
    "        std = self.softplus(std_out) + self.min_std\n",
    "\n",
    "        # Independent ensures that we have a multivariate\n",
    "        # Gaussian with a diagonal covariance matrix (given as a vector `std`)\n",
    "        return Independent(Normal(mean, std), 1)\n",
    "\n",
    "    def forward(self, t: int, stochastic=True):\n",
    "        \"\"\"Computes the action a_t and its log-probability p(a_t| s_t)\n",
    "\n",
    "        :param stochastic: True when sampling\n",
    "        \"\"\"\n",
    "\n",
    "        # Computes probabilities over actions\n",
    "        normal_dist = self.normal_dist(self.get((\"env/env_obs\", t)))\n",
    "        action_dist = TransformedDistribution(normal_dist, [self.tanh_transform])\n",
    "        if stochastic:\n",
    "            # Uses the re-parametrization trick\n",
    "            action = action_dist.rsample()\n",
    "        else:\n",
    "            # Directly uses the mode of the distribution\n",
    "            action = self.tanh_transform(normal_dist.mode)\n",
    "\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "\n",
    "        # This line allows to deepcopy the actor...\n",
    "        self.tanh_transform._cached_x_y = [None, None]\n",
    "        self.set((\"action\", t), action)\n",
    "        self.set((\"action_logprobs\", t), log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 没有 state 时，随机生成的 action，不需要更改\n",
    "class SamplingActor(Agent):\n",
    "    \"\"\"Samples random actions\"\"\"\n",
    "\n",
    "    def __init__(self, action_space: gym.Space):\n",
    "        super().__init__()\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def forward(self, t: int):\n",
    "        self.set((\"action\", t), torch.LongTensor([self.action_space.sample()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 Critic 类\n",
    "class ContinuousQAgent(Agent):\n",
    "    def __init__(self, state_dim: int, hidden_layers: list[int], action_dim: int):\n",
    "        \"\"\"创建一个新的 Q 函数评论家代理: Q(s, a)\n",
    "\n",
    "        :param state_dim: 状态空间的维数（观测的维数）\n",
    "        :param hidden_layers: 神经网络的隐藏层大小列表\n",
    "        :param action_dim: 动作空间的维数\n",
    "        \"\"\"\n",
    "        super().__init__()  # 调用父类的初始化方法\n",
    "        self.is_q_function = True  # 标记该代理为Q函数\n",
    "        # 使用给定的状态维度和动作维度构建一个多层感知机（MLP）模型\n",
    "        # 输入层大小为 状态维度 + 动作维度，输出层大小为 1（Q值）\n",
    "        self.model = build_mlp(\n",
    "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        obs = self.get((\"env/env_obs\", t))        # 获取在时间步t的环境观测（状态）\n",
    "        action = self.get((\"action\", t))          # 获取在时间步t的动作\n",
    "        obs_act = torch.cat((obs, action), dim=1) # 将状态和动作连接起来作为模型输入\n",
    "        q_value = self.model(obs_act).squeeze(-1) # 使用模型计算Q值，squeeze(-1)用于移除多余的维度\n",
    "        self.set((f\"{self.prefix}q_value\", t), q_value) # 将计算得到的Q值存储在字典中，以备后续使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pystk_actor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "from bbrl.agents import Agents, Agent    # 不需要 Agents 了\n",
    "import gymnasium as gym\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#: The base environment name\n",
    "env_name = \"supertuxkart/flattened_continuous_actions-v0\"      # 由于SAC算法只能用于连续动作空间，将动作空间中的 离散操作 删除\n",
    "\n",
    "#: Player name\n",
    "player_name = \"SAC_agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在后面的 SACAlgo 类中，直接创建了 actor\n",
    "# 这里的 get_actor 变得没有必要了, 但是不知道有没有其他用处\n",
    "def get_actor(\n",
    "    state, observation_space: gym.spaces.Space, action_space: gym.spaces.Space\n",
    ") -> Agent:\n",
    "    # 创建SAC的Actor和Critic网络\n",
    "    actor = SquashedGaussianActor(observation_space, action_space)\n",
    "    critic_1 = ContinuousQAgent(observation_space, action_space)\n",
    "    critic_2 = ContinuousQAgent(observation_space, action_space)\n",
    "    \n",
    "    # 创建目标Critic网络\n",
    "    target_critic_1 = copy.deepcopy(critic_1).with_prefix(\"target-critic-1/\")\n",
    "    target_critic_2 = copy.deepcopy(critic_2).with_prefix(\"target-critic-2/\") \n",
    "\n",
    "    if state is None:\n",
    "        # 如果没有预训练的状态, 返回随机采样的Actor\n",
    "        return actor\n",
    "    \n",
    "    # 加载预训练的状态\n",
    "    actor.load_state_dict(state[\"actor\"])\n",
    "    critic_1.load_state_dict(state[\"critic_1\"]) \n",
    "    critic_2.load_state_dict(state[\"critic_2\"])\n",
    "    target_critic_1.load_state_dict(state[\"target_critic_1\"])\n",
    "    target_critic_2.load_state_dict(state[\"target_critic_2\"])\n",
    "\n",
    "    return Agents(actor, critic_1, critic_2, target_critic_1, target_critic_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wrappers() -> List[Callable[[gym.Env], gym.Wrapper]]:\n",
    "    \"\"\"Returns a list of additional wrappers to be applied to the base\n",
    "    environment\"\"\"\n",
    "    return [\n",
    "        # Example of a custom wrapper\n",
    "        # lambda env: MyWrapper(env, option=\"1\")               # 需要添加 Wrapper 时，在此处操作\n",
    "        lambda env: OnlyContinousObservationWrapper(env)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen_guanyu/deepdac/lib/python3.10/site-packages/bbrl_utils/notebook.py:46: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # noqa: F401\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils>=0.5\").setup()\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.workspace import Workspace\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent, KWAgentWrapper\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from omegaconf import OmegaConf\n",
    "from torch.distributions import (\n",
    "    Normal,\n",
    "    Independent,\n",
    "    TransformedDistribution,\n",
    "    TanhTransform,\n",
    ")\n",
    "import bbrl_gymnasium  # noqa: F401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pystk2_gymnasium import AgentSpec\n",
    "from functools import partial\n",
    "import inspect\n",
    "from bbrl.agents.gymnasium import ParallelGymAgent, make_env\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 13:51:15.128099: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-28 13:51:15.152981: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-28 13:51:15.160714: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-28 13:51:15.178218: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-28 13:51:17.143517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"save_best\": True,\n",
    "    \"base_dir\": \"${gym_env.env_name}/sac-S${algorithm.seed}_${current_time:}\",\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 1,\n",
    "        \"n_envs\": 8,\n",
    "        \"n_steps\": 32,\n",
    "        \"buffer_size\": 1e6,\n",
    "        \"batch_size\": 256,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"nb_evals\": 16,\n",
    "        \"eval_interval\": 2_000,\n",
    "        \"learning_starts\": 10_000,\n",
    "        \"max_epochs\": 2_000,\n",
    "        \"discount_factor\": 0.98,\n",
    "        \"entropy_mode\": \"auto\",  # \"auto\" or \"fixed\"\n",
    "        \"init_entropy_coef\": 2e-7,\n",
    "        \"tau_target\": 0.05,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [64, 64],\n",
    "            \"critic_hidden_size\": [256, 256],\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\"env_name\": f\"{env_name}\"},       # 修改为 supertuxkart 的环境名称，但是不确定能否使用 bbrl 库\n",
    "    \"actor_optimizer\": {                          # 如果可行，则后面代码不需要改动\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "    \"entropy_coef_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建SAC算法环境类\n",
    "class SACAlgo(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)  # 调用父类的初始化方法，传入配置参数cfg\n",
    "\n",
    "        # TODO: 重写 self.train_env , 满足该环境要求     \n",
    "        make_stkenv = partial(                                 \n",
    "            make_env,                                          \n",
    "            env_name, \n",
    "            wrappers=get_wrappers(),\n",
    "            render_mode=None,\n",
    "            autoreset=True,\n",
    "            agent=AgentSpec(use_ai=False, name=player_name),\n",
    "        )\n",
    "\n",
    "        self.train_env = ParallelGymAgent(\n",
    "            make_stkenv, \n",
    "            1\n",
    "        ).seed(cfg.algorithm.seed)\n",
    "\n",
    "        # 确保环境正确初始化并设置env/env_obs\n",
    "        # initial_workspace = Workspace()\n",
    "        # self.train_env(initial_workspace, t=0, n_steps=1)\n",
    "    \n",
    "        # 获取状态空间和动作空间的大小\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "        # 断言动作空间是否为连续型，若不是则报错提示\n",
    "        assert (\n",
    "            self.train_env.is_continuous_action()\n",
    "        ), \"SAC代码专用于连续动作空间\"\n",
    "\n",
    "        # 创建一个actor（策略网络）\n",
    "        self.actor = SquashedGaussianActor(\n",
    "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "        )\n",
    "\n",
    "        # 创建第一个评论家网络（critic_1）来估计Q值\n",
    "        self.critic_1 = ContinuousQAgent(\n",
    "            obs_size,  # 状态空间的大小\n",
    "            cfg.algorithm.architecture.critic_hidden_size,  # 评论家网络的隐藏层大小\n",
    "            act_size,  # 动作空间的大小\n",
    "        ).with_prefix(\"critic-1/\")  # 添加前缀以区分网络\n",
    "\n",
    "        # 创建目标评论家网络target_critic_1，作为critic_1的深拷贝\n",
    "        self.target_critic_1 = copy.deepcopy(self.critic_1).with_prefix(\n",
    "            \"target-critic-1/\"\n",
    "        )\n",
    "\n",
    "        # 创建第二个评论家网络critic_2，作为SAC的双重Q网络\n",
    "        self.critic_2 = ContinuousQAgent(\n",
    "            obs_size,\n",
    "            cfg.algorithm.architecture.critic_hidden_size,\n",
    "            act_size,\n",
    "        ).with_prefix(\"critic-2/\")\n",
    "\n",
    "        # 创建目标评论家网络target_critic_2，作为critic_2的深拷贝\n",
    "        self.target_critic_2 = copy.deepcopy(self.critic_2).with_prefix(\n",
    "            \"target-critic-2/\"\n",
    "        )\n",
    "\n",
    "        # 训练策略网络的引用，指向actor\n",
    "        self.train_policy = self.actor\n",
    "        # 评估策略网络的引用，使用KWAgentWrapper封装actor，并设定stochastic=False，即使用确定性策略\n",
    "        self.eval_policy = KWAgentWrapper(self.actor, stochastic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_entropy_optimizers(cfg):\n",
    "    # 定义设置熵优化器的函数，参数为配置文件 `cfg`\n",
    "\n",
    "    if cfg.algorithm.entropy_mode == \"auto\":\n",
    "        # 如果配置中的熵模式为自动模式 (\"auto\")，则进行以下操作：\n",
    "\n",
    "        # 注释：优化熵系数的对数值，这略微不同于原论文中的做法，\n",
    "        # 详细讨论见 https://github.com/rail-berkeley/softlearning/issues/37\n",
    "        # 此注释和代码参考自稳定基线3（Stable Baselines3）的SAC实现版本\n",
    "\n",
    "        log_entropy_coef = nn.Parameter(\n",
    "            torch.log(torch.ones(1) * cfg.algorithm.init_entropy_coef)\n",
    "        )  # 定义一个可学习的参数log_entropy_coef，用于存储初始熵系数的对数值\n",
    "           # torch.log(torch.ones(1) * cfg.algorithm.init_entropy_coef) 将初始熵系数取对数以便直接优化其对数值\n",
    "\n",
    "        # 调用 `setup_optimizer` 函数为 `log_entropy_coef` 参数设置优化器\n",
    "        entropy_coef_optimizer = setup_optimizer(\n",
    "            cfg.entropy_coef_optimizer, log_entropy_coef\n",
    "        )\n",
    "\n",
    "        # 返回熵系数优化器 `entropy_coef_optimizer` 和 `log_entropy_coef` 参数\n",
    "        return entropy_coef_optimizer, log_entropy_coef\n",
    "    else:\n",
    "        # 如果熵模式不是自动模式，则返回两个 `None` 值，表示不进行熵系数优化\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_critic_loss(\n",
    "    cfg,\n",
    "    reward: torch.Tensor,\n",
    "    must_bootstrap: torch.Tensor,\n",
    "    t_actor: TemporalAgent,\n",
    "    t_q_agents: TemporalAgent,\n",
    "    t_target_q_agents: TemporalAgent,\n",
    "    rb_workspace: Workspace,\n",
    "    ent_coef: torch.Tensor,\n",
    "):\n",
    "    r\"\"\"Computes the critic loss for a set of $S$ transition samples\n",
    "\n",
    "    Args:\n",
    "        cfg: The experimental configuration\n",
    "        reward: Tensor (2xS) of rewards\n",
    "        must_bootstrap: Tensor (2xS) of indicators\n",
    "        t_actor: The actor agent\n",
    "        t_q_agents: The critics\n",
    "        t_target_q_agents: The target of the critics\n",
    "        rb_workspace: The transition workspace\n",
    "        ent_coef: The entropy coefficient $\\alpha$\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: The two critic losses (scalars)\n",
    "    \"\"\"\n",
    "\n",
    "    # Replay the actor so we get the necessary statistics\n",
    "    \n",
    "    # Compute q_values from both critics with the actions present in the buffer:\n",
    "    # at t, we have Q(s,a) from (s,a) in the RB\n",
    "    t_q_agents(rb_workspace, t=0, n_steps=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Replay the current actor on the replay buffer to get actions of the current actor\n",
    "        t_actor(rb_workspace, t=1, n_steps=1)\n",
    "        action_logprobs_next = rb_workspace[\"action_logprobs\"]\n",
    "\n",
    "        # Compute target q_values from both target critics: at t+1, we have\n",
    "        # Q(s_{t+1}, a_{t+1}) from the (s_{t+1}, a_{t+1}) where a_{t+1} has been\n",
    "        # replaced in the RB with the t_actor line above\n",
    "        t_target_q_agents(rb_workspace, t=1, n_steps=1)\n",
    "\n",
    "    q_values_rb_1, q_values_rb_2, post_q_values_1, post_q_values_2 = rb_workspace[\n",
    "        \"critic-1/q_value\",\n",
    "        \"critic-2/q_value\",\n",
    "        \"target-critic-1/q_value\",\n",
    "        \"target-critic-2/q_value\"\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Compute temporal difference\n",
    "\n",
    "    q_next = torch.minimum(post_q_values_1[1], post_q_values_2[1])\n",
    "    v_phi = q_next - ent_coef * action_logprobs_next[1]\n",
    "\n",
    "    target = reward[1] + cfg.algorithm.discount_factor * v_phi * must_bootstrap[1].int()\n",
    "    critic_loss_1 = nn.functional.mse_loss(q_values_rb_1[0], target)\n",
    "    critic_loss_2 = nn.functional.mse_loss(q_values_rb_2[0], target)\n",
    "    \n",
    "\n",
    "    return critic_loss_1, critic_loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_actor_loss(\n",
    "    ent_coef, t_actor: TemporalAgent, t_q_agents: TemporalAgent, rb_workspace: Workspace\n",
    "):\n",
    "    r\"\"\"\n",
    "    Actor loss computation\n",
    "    :param ent_coef: The entropy coefficient $\\alpha$\n",
    "    :param t_actor: The actor agent (temporal agent)\n",
    "    :param t_q_agents: The critics (as temporal agent)\n",
    "    :param rb_workspace: The replay buffer (2 time steps, $t$ and $t+1$)\n",
    "    \"\"\"\n",
    "\n",
    "    # Recompute the action with the current actor (at $a_t$)\n",
    "\n",
    "    # Step 1: 使用当前 actor 重新计算当前状态下的动作 a_t 和对数概率\n",
    "    t_actor(rb_workspace, t=0, n_steps=1, stochastic=True)\n",
    "    action_logprobs_new = rb_workspace[\"action_logprobs\"]\n",
    "\n",
    "    # Compute Q-values\n",
    "\n",
    "    # Step 2: 使用 Critic 计算当前状态下的 Q 值\n",
    "    t_q_agents(rb_workspace, t=0, n_steps=1)\n",
    "    q_values_1 = rb_workspace[\"critic-1/q_value\"]\n",
    "    q_values_2 = rb_workspace[\"critic-2/q_value\"]\n",
    "\n",
    "    # Step 3: 取两个 Q 值的最小值，减少估值偏差\n",
    "    current_q_values = torch.min(q_values_1, q_values_2)\n",
    "\n",
    "    # Compute the actor loss\n",
    "    # Step 4: 计算 actor 损失\n",
    "\n",
    "    actor_loss = ent_coef * action_logprobs_new[0] - current_q_values[0]\n",
    "    \n",
    "    return actor_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sac(sac: SACAlgo):\n",
    "    cfg = sac.cfg\n",
    "    logger = sac.logger\n",
    "\n",
    "    # init_entropy_coef is the initial value of the entropy coef alpha\n",
    "    ent_coef = cfg.algorithm.init_entropy_coef\n",
    "    tau = cfg.algorithm.tau_target\n",
    "\n",
    "    # Creates the temporal actors\n",
    "    t_actor = TemporalAgent(sac.train_policy)\n",
    "    q_agents = TemporalAgent(Agents(sac.critic_1, sac.critic_2))\n",
    "    target_q_agents = TemporalAgent(Agents(sac.target_critic_1, sac.target_critic_2))\n",
    "\n",
    "    # Configure the optimizer\n",
    "    actor_optimizer = setup_optimizer(cfg.actor_optimizer, sac.actor)\n",
    "    critic_optimizer = setup_optimizer(cfg.critic_optimizer, sac.critic_1, sac.critic_2)\n",
    "    entropy_coef_optimizer, log_entropy_coef = setup_entropy_optimizers(cfg) \n",
    "\n",
    "    # If entropy_mode is not auto, the entropy coefficient ent_coef remains\n",
    "    # fixed. Otherwise, computes the target entropy\n",
    "    if cfg.algorithm.entropy_mode == \"auto\":\n",
    "        # target_entropy is \\mathcal{H}_0 in the SAC and aplications paper.\n",
    "        target_entropy = -np.prod(sac.train_env.action_space.shape).astype(np.float32)\n",
    "\n",
    "    # Loops over successive replay buffers\n",
    "    for rb in sac.iter_replay_buffers():     # TODO: Unknown variable: env/env_obs —— 使用自定义环境时，workspace中没有 env/env_obs\n",
    "        # Implement the SAC algorithm\n",
    "        rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n",
    "\n",
    "        # 看 workspace 中有什么\n",
    "        print(rb_workspace)\n",
    "\n",
    "        terminated, reward = rb_workspace[\"env/terminated\", \"env/reward\"]\n",
    "\n",
    "        must_boostrap = ~terminated\n",
    "\n",
    "        if entropy_coef_optimizer is not None:\n",
    "            ent_coef = torch.exp(log_entropy_coef.detach())\n",
    "\n",
    "        # Critic update part #############################\n",
    "        (critic_loss_1, critic_loss_2) = compute_critic_loss(\n",
    "            cfg = cfg,\n",
    "            reward = reward,\n",
    "            must_bootstrap = must_boostrap,\n",
    "            t_actor = t_actor,\n",
    "            t_q_agents = q_agents,\n",
    "            t_target_q_agents = target_q_agents,\n",
    "            rb_workspace = rb_workspace,\n",
    "            ent_coef = ent_coef\n",
    "        )\n",
    "\n",
    "        # 记录 Critic 损失\n",
    "        logger.add_log(\"critic_loss_1\", critic_loss_1, sac.nb_steps)\n",
    "        logger.add_log(\"critic_loss_2\", critic_loss_2, sac.nb_steps)\n",
    "\n",
    "        # 反向传播并更新 Critic 参数\n",
    "        critic_loss = critic_loss_1 + critic_loss_2\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            sac.critic_1.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            sac.critic_2.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        # Actor update part #############################\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss = compute_actor_loss(\n",
    "            ent_coef = ent_coef,\n",
    "            t_actor = t_actor,\n",
    "            t_q_agents = q_agents,\n",
    "            rb_workspace = rb_workspace\n",
    "        )\n",
    "\n",
    "        # 记录 Actor 损失\n",
    "        logger.add_log(\"actor_loss\", actor_loss, sac.nb_steps)\n",
    "\n",
    "        # 反向传播并更新 Actor 参数\n",
    "        actor_loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            sac.actor.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        # Entropy optimizer part\n",
    "        if entropy_coef_optimizer is not None:\n",
    "            # See Eq. (17) of the SAC and Applications paper. The log\n",
    "            # probabilities *must* have been computed when computing the actor\n",
    "            # loss.\n",
    "            action_logprobs_rb = rb_workspace[\"action_logprobs\"].detach()\n",
    "            entropy_coef_loss = -(\n",
    "                log_entropy_coef.exp() * (action_logprobs_rb + target_entropy)\n",
    "            ).mean()\n",
    "            entropy_coef_optimizer.zero_grad()\n",
    "            entropy_coef_loss.backward()\n",
    "            entropy_coef_optimizer.step()\n",
    "            logger.add_log(\"entropy_coef_loss\", entropy_coef_loss, sac.nb_steps)\n",
    "            logger.add_log(\"entropy_coef\", ent_coef, sac.nb_steps)\n",
    "\n",
    "        ####################################################\n",
    "\n",
    "        # Soft update of target q function\n",
    "        soft_update_params(sac.critic_1, sac.target_critic_1, tau)\n",
    "        soft_update_params(sac.critic_2, sac.target_critic_2, tau)\n",
    "\n",
    "        sac.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47f0323bfb944ed9fdbf8f4d2d0a6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agents \u001b[38;5;241m=\u001b[39m SACAlgo(OmegaConf\u001b[38;5;241m.\u001b[39mcreate(params))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_sac\u001b[49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36mrun_sac\u001b[0;34m(sac)\u001b[0m\n\u001b[1;32m     23\u001b[0m     target_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mprod(sac\u001b[38;5;241m.\u001b[39mtrain_env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Loops over successive replay buffers\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rb \u001b[38;5;129;01min\u001b[39;00m sac\u001b[38;5;241m.\u001b[39miter_replay_buffers():     \u001b[38;5;66;03m# TODO: Unknown variable: env/env_obs —— 使用自定义环境时，workspace中没有 env/env_obs\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Implement the SAC algorithm\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     rb_workspace \u001b[38;5;241m=\u001b[39m rb\u001b[38;5;241m.\u001b[39mget_shuffled(cfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# 看 workspace 中有什么\u001b[39;00m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl_utils/algorithms.py:291\u001b[0m, in \u001b[0;36mEpochBasedAlgo.iter_replay_buffers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    290\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mcopy_n_last_steps(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_workspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstochastic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mn_steps \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mn_envs\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Add transitions to buffer\u001b[39;00m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/utils.py:79\u001b[0m, in \u001b[0;36mTemporalAgent.__call__\u001b[0;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m _t \u001b[38;5;241m=\u001b[39m t\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_variable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         s \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mget(stop_variable, _t)\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/utils.py:31\u001b[0m, in \u001b[0;36mAgents.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, workspace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m---> 31\u001b[0m         \u001b[43ma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/agent.py:84\u001b[0m, in \u001b[0;36mAgent.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m workspace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Agent.__call__] workspace must not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m workspace\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/gymnasium.py:480\u001b[0m, in \u001b[0;36mParallelGymAgent.forward\u001b[0;34m(self, t, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step(k, dict_slice(k, action))\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 480\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# Use last frame\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_frame[k]\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/gymnasium.py:428\u001b[0m, in \u001b[0;36mParallelGymAgent._step\u001b[0;34m(self, k, action)\u001b[0m\n\u001b[1;32m    425\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[k]\n\u001b[1;32m    427\u001b[0m action: Union[\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray[\u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m _convert_action(action)\n\u001b[0;32m--> 428\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timestep[k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulated_reward[k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/gymnasium/wrappers/autoreset.py:56\u001b[0m, in \u001b[0;36mAutoResetWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment with action and resets the environment if a terminated or truncated signal is encountered.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m        The autoreset environment :meth:`step`\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[1;32m     58\u001b[0m         new_obs, new_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/pystk2_gymnasium/definitions.py:63\u001b[0m, in \u001b[0;36mActionObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m:meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction(action)\n\u001b[0;32m---> 63\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/pystk2_gymnasium/definitions.py:63\u001b[0m, in \u001b[0;36mActionObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m:meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction(action)\n\u001b[0;32m---> 63\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/pystk2_gymnasium/definitions.py:63\u001b[0m, in \u001b[0;36mActionObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m:meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction(action)\n\u001b[0;32m---> 63\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/gymnasium/core.py:522\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    521\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/gymnasium/core.py:522\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    521\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/pystk2_gymnasium/envs.py:447\u001b[0m, in \u001b[0;36mSTKRaceEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrace_step(get_action(action))\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld_update()\n\u001b[0;32m--> 447\u001b[0m obs, reward, terminated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkart_ix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_ai\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (obs, reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, info)\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/pystk2_gymnasium/envs.py:220\u001b[0m, in \u001b[0;36mBaseSTKRaceEnv.get_state\u001b[0;34m(self, kart_ix, use_ai)\u001b[0m\n\u001b[1;32m    217\u001b[0m terminated \u001b[38;5;241m=\u001b[39m kart\u001b[38;5;241m.\u001b[39mhas_finished_race\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Get the observation and update the world state\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkart_ix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_ai\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m d_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, kart\u001b[38;5;241m.\u001b[39moverall_distance)\n\u001b[1;32m    223\u001b[0m f_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/pystk2_gymnasium/envs.py:362\u001b[0m, in \u001b[0;36mBaseSTKRaceEnv.get_observation\u001b[0;34m(self, kart_ix, use_ai)\u001b[0m\n\u001b[1;32m    316\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process\u001b[38;5;241m.\u001b[39mget_kart_action(kart_ix)\n\u001b[1;32m    317\u001b[0m     obs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    319\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([action\u001b[38;5;241m.\u001b[39macceleration], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m         }\n\u001b[1;32m    327\u001b[0m     }\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mobs,\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;66;03m# Kart properties\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpowerup\u001b[39m\u001b[38;5;124m\"\u001b[39m: kart\u001b[38;5;241m.\u001b[39mpowerup\u001b[38;5;241m.\u001b[39mnum,\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattachment\u001b[39m\u001b[38;5;124m\"\u001b[39m: kart\u001b[38;5;241m.\u001b[39mattachment\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattachment_time_left\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    335\u001b[0m         [kart\u001b[38;5;241m.\u001b[39mattachment\u001b[38;5;241m.\u001b[39mtime_left], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    336\u001b[0m     ),\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_steer_angle\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([kart\u001b[38;5;241m.\u001b[39mmax_steer_angle], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([kart\u001b[38;5;241m.\u001b[39menergy], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskeed_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([kart\u001b[38;5;241m.\u001b[39mskeed_factor], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshield_time\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([kart\u001b[38;5;241m.\u001b[39mshield_time], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjumping\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kart\u001b[38;5;241m.\u001b[39mjumping \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;66;03m# Kart physics (from the kart point view)\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance_down_track\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    344\u001b[0m         [kart\u001b[38;5;241m.\u001b[39mdistance_down_track], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    345\u001b[0m     ),\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvelocity\u001b[39m\u001b[38;5;124m\"\u001b[39m: kart\u001b[38;5;241m.\u001b[39mvelocity_lc,\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfront\u001b[39m\u001b[38;5;124m\"\u001b[39m: kartview(kart\u001b[38;5;241m.\u001b[39mfront),\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# path center\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenter_path_distance\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([center_path_distance], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenter_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(x_orth),\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;66;03m# Items (kart point of view)\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(items_position),\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(items_type),\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# Other karts (kart point of view)\u001b[39;00m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkarts_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(karts_position),\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Paths\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaths_distance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(iterate_from(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack\u001b[38;5;241m.\u001b[39mpath_distance, path_ix)),\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaths_width\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(iterate_from(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack\u001b[38;5;241m.\u001b[39mpath_width, path_ix)),\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaths_start\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    360\u001b[0m         kartview(x[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m iterate_from(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack\u001b[38;5;241m.\u001b[39mpath_nodes, path_ix)\n\u001b[1;32m    361\u001b[0m     ),\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaths_end\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkartview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterate_from\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_ix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    365\u001b[0m }\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/pystk2_gymnasium/envs.py:363\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    316\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process\u001b[38;5;241m.\u001b[39mget_kart_action(kart_ix)\n\u001b[1;32m    317\u001b[0m     obs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    319\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([action\u001b[38;5;241m.\u001b[39macceleration], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m         }\n\u001b[1;32m    327\u001b[0m     }\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mobs,\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;66;03m# Kart properties\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpowerup\u001b[39m\u001b[38;5;124m\"\u001b[39m: kart\u001b[38;5;241m.\u001b[39mpowerup\u001b[38;5;241m.\u001b[39mnum,\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattachment\u001b[39m\u001b[38;5;124m\"\u001b[39m: kart\u001b[38;5;241m.\u001b[39mattachment\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattachment_time_left\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    335\u001b[0m         [kart\u001b[38;5;241m.\u001b[39mattachment\u001b[38;5;241m.\u001b[39mtime_left], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    336\u001b[0m     ),\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_steer_angle\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([kart\u001b[38;5;241m.\u001b[39mmax_steer_angle], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([kart\u001b[38;5;241m.\u001b[39menergy], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskeed_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([kart\u001b[38;5;241m.\u001b[39mskeed_factor], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshield_time\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([kart\u001b[38;5;241m.\u001b[39mshield_time], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjumping\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kart\u001b[38;5;241m.\u001b[39mjumping \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;66;03m# Kart physics (from the kart point view)\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance_down_track\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    344\u001b[0m         [kart\u001b[38;5;241m.\u001b[39mdistance_down_track], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    345\u001b[0m     ),\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvelocity\u001b[39m\u001b[38;5;124m\"\u001b[39m: kart\u001b[38;5;241m.\u001b[39mvelocity_lc,\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfront\u001b[39m\u001b[38;5;124m\"\u001b[39m: kartview(kart\u001b[38;5;241m.\u001b[39mfront),\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# path center\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenter_path_distance\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([center_path_distance], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenter_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(x_orth),\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;66;03m# Items (kart point of view)\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(items_position),\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(items_type),\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# Other karts (kart point of view)\u001b[39;00m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkarts_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(karts_position),\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Paths\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaths_distance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(iterate_from(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack\u001b[38;5;241m.\u001b[39mpath_distance, path_ix)),\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaths_width\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(iterate_from(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack\u001b[38;5;241m.\u001b[39mpath_width, path_ix)),\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaths_start\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    360\u001b[0m         kartview(x[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m iterate_from(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack\u001b[38;5;241m.\u001b[39mpath_nodes, path_ix)\n\u001b[1;32m    361\u001b[0m     ),\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaths_end\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m--> 363\u001b[0m         \u001b[43mkartview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m iterate_from(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack\u001b[38;5;241m.\u001b[39mpath_nodes, path_ix)\n\u001b[1;32m    364\u001b[0m     ),\n\u001b[1;32m    365\u001b[0m }\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/pystk2_gymnasium/envs.py:248\u001b[0m, in \u001b[0;36mBaseSTKRaceEnv.get_observation.<locals>.kartview\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkartview\u001b[39m(x):\n\u001b[1;32m    244\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a vector in the kart frame\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m    X right, Y up, Z forwards\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/pystk2_gymnasium/utils.py:33\u001b[0m, in \u001b[0;36mrotate\u001b[0;34m(v, q)\u001b[0m\n\u001b[1;32m     30\u001b[0m x, y, z \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m     31\u001b[0m qw, qx, qy, qz \u001b[38;5;241m=\u001b[39m q\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mqx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mqw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mqw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agents = SACAlgo(OmegaConf.create(params))\n",
    "run_sac(agents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
