{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils>=0.5\").setup()\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from bbrl.agents import Agent\n",
    "from bbrl_utils.nn import build_mlp\n",
    "\n",
    "from torch.distributions import (\n",
    "    Normal,\n",
    "    Independent,\n",
    "    TransformedDistribution,\n",
    "    TanhTransform,\n",
    ")\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "from pystk2_gymnasium.definitions import ActionObservationWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只是模板，可以随意更改\n",
    "class MyWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, option: int):\n",
    "        super().__init__(env)\n",
    "        self.option = option\n",
    "\n",
    "    def action(self, action):\n",
    "        # We do nothing here\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义 Wrapper : 删除 observation space 中的 离散特征\n",
    "class OnlyContinousObservationWrapper(ActionObservationWrapper):\n",
    "    \"\"\"Removes discrete features from the observation space.\"\"\"\n",
    "    \n",
    "    def __init__(self, env: gym.Env, **kwargs):\n",
    "        super().__init__(env, **kwargs)\n",
    "        \n",
    "        # 过滤掉离散特征，只保留非离散特征\n",
    "        self._observation_space = env.observation_space['continuous']\n",
    "\n",
    "    def observation(self, observation: Dict):\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP 08-sac 中的 class SquashedGaussianActor(Agent)\n",
    "class SquashedGaussianActor(Agent):\n",
    "    \"\"\"Computes probabilities over action\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim, min_std=1e-4):\n",
    "        \"\"\"Creates a new Squashed Gaussian actor\n",
    "\n",
    "        :param state_dim: The dimension of the state space\n",
    "        :param hidden_layers: Hidden layer sizes\n",
    "        :param action_dim: The dimension of the action space\n",
    "        :param min_std: The minimum standard deviation, defaults to 1e-4\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.min_std = min_std\n",
    "        backbone_dim = [state_dim] + list(hidden_layers)\n",
    "        self.layers = build_mlp(backbone_dim, activation=nn.ReLU())\n",
    "        self.backbone = nn.Sequential(*self.layers)\n",
    "        self.last_mean_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
    "        self.last_std_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "        # cache_size avoids numerical infinites or NaNs when\n",
    "        # computing log probabilities\n",
    "        self.tanh_transform = TanhTransform(cache_size=1)\n",
    "\n",
    "    def normal_dist(self, obs:torch.Tensor):\n",
    "        \"\"\"compute normal distribution given observation(s)\"\"\"\n",
    "\n",
    "        backbone_output = self.backbone(obs)\n",
    "        mean = self.last_mean_layer(backbone_output)\n",
    "        std_out = self.last_std_layer(backbone_output)\n",
    "        std = self.softplus(std_out) + self.min_std\n",
    "\n",
    "        # Independent ensures that we have a multivariate\n",
    "        # Gaussian with a diagonal covariance matrix (given as a vector `std`)\n",
    "        return Independent(Normal(mean, std), 1)\n",
    "\n",
    "    def forward(self, t: int, stochastic=True):\n",
    "        \"\"\"Computes the action a_t and its log-probability p(a_t| s_t)\n",
    "\n",
    "        :param stochastic: True when sampling\n",
    "        \"\"\"\n",
    "\n",
    "        # Computes probabilities over actions\n",
    "        normal_dist = self.normal_dist(self.get((\"env/env_obs\", t)))\n",
    "        action_dist = TransformedDistribution(normal_dist, [self.tanh_transform])\n",
    "        if stochastic:\n",
    "            # Uses the re-parametrization trick\n",
    "            action = action_dist.rsample()\n",
    "        else:\n",
    "            # Directly uses the mode of the distribution\n",
    "            action = self.tanh_transform(normal_dist.mode)\n",
    "\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "\n",
    "        # This line allows to deepcopy the actor...\n",
    "        self.tanh_transform._cached_x_y = [None, None]\n",
    "        self.set((\"action\", t), action)\n",
    "        self.set((\"action_logprobs\", t), log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 没有 state 时，随机生成的 action，不需要更改\n",
    "class SamplingActor(Agent):\n",
    "    \"\"\"Samples random actions\"\"\"\n",
    "\n",
    "    def __init__(self, action_space: gym.Space):\n",
    "        super().__init__()\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def forward(self, t: int):\n",
    "        self.set((\"action\", t), torch.LongTensor([self.action_space.sample()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 Critic 类\n",
    "class ContinuousQAgent(Agent):\n",
    "    def __init__(self, state_dim: int, hidden_layers: list[int], action_dim: int):\n",
    "        \"\"\"创建一个新的 Q 函数评论家代理: Q(s, a)\n",
    "\n",
    "        :param state_dim: 状态空间的维数（观测的维数）\n",
    "        :param hidden_layers: 神经网络的隐藏层大小列表\n",
    "        :param action_dim: 动作空间的维数\n",
    "        \"\"\"\n",
    "        super().__init__()  # 调用父类的初始化方法\n",
    "        self.is_q_function = True  # 标记该代理为Q函数\n",
    "        # 使用给定的状态维度和动作维度构建一个多层感知机（MLP）模型\n",
    "        # 输入层大小为 状态维度 + 动作维度，输出层大小为 1（Q值）\n",
    "        self.model = build_mlp(\n",
    "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        obs = self.get((\"env/env_obs\", t))        # 获取在时间步t的环境观测（状态）\n",
    "        action = self.get((\"action\", t))          # 获取在时间步t的动作\n",
    "        obs_act = torch.cat((obs, action), dim=1) # 将状态和动作连接起来作为模型输入\n",
    "        q_value = self.model(obs_act).squeeze(-1) # 使用模型计算Q值，squeeze(-1)用于移除多余的维度\n",
    "        self.set((f\"{self.prefix}q_value\", t), q_value) # 将计算得到的Q值存储在字典中，以备后续使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pystk_actor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "from bbrl.agents import Agents, Agent    # 不需要 Agents 了\n",
    "import gymnasium as gym\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#: The base environment name\n",
    "env_name = \"supertuxkart/flattened_continuous_actions-v0\"      # 由于SAC算法只能用于连续动作空间，将动作空间中的 离散操作 删除\n",
    "\n",
    "#: Player name\n",
    "player_name = \"SAC_agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在后面的 SACAlgo 类中，直接创建了 actor\n",
    "# 这里的 get_actor 变得没有必要了, 但是不知道有没有其他用处\n",
    "def get_actor(\n",
    "    state, observation_space: gym.spaces.Space, action_space: gym.spaces.Space\n",
    ") -> Agent:\n",
    "    # 创建SAC的Actor和Critic网络\n",
    "    actor = SquashedGaussianActor(observation_space, action_space)\n",
    "    critic_1 = ContinuousQAgent(observation_space, action_space)\n",
    "    critic_2 = ContinuousQAgent(observation_space, action_space)\n",
    "    \n",
    "    # 创建目标Critic网络\n",
    "    target_critic_1 = copy.deepcopy(critic_1).with_prefix(\"target-critic-1/\")\n",
    "    target_critic_2 = copy.deepcopy(critic_2).with_prefix(\"target-critic-2/\") \n",
    "\n",
    "    if state is None:\n",
    "        # 如果没有预训练的状态, 返回随机采样的Actor\n",
    "        return actor\n",
    "    \n",
    "    # 加载预训练的状态\n",
    "    actor.load_state_dict(state[\"actor\"])\n",
    "    critic_1.load_state_dict(state[\"critic_1\"]) \n",
    "    critic_2.load_state_dict(state[\"critic_2\"])\n",
    "    target_critic_1.load_state_dict(state[\"target_critic_1\"])\n",
    "    target_critic_2.load_state_dict(state[\"target_critic_2\"])\n",
    "\n",
    "    return Agents(actor, critic_1, critic_2, target_critic_1, target_critic_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wrappers() -> List[Callable[[gym.Env], gym.Wrapper]]:\n",
    "    \"\"\"Returns a list of additional wrappers to be applied to the base\n",
    "    environment\"\"\"\n",
    "    return [\n",
    "        # Example of a custom wrapper\n",
    "        # lambda env: MyWrapper(env, option=\"1\")               # 需要添加 Wrapper 时，在此处操作\n",
    "        lambda env: OnlyContinousObservationWrapper(env)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen_guanyu/deepdac/lib/python3.10/site-packages/bbrl_utils/notebook.py:46: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # noqa: F401\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils>=0.5\").setup()\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.workspace import Workspace\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent, KWAgentWrapper\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from omegaconf import OmegaConf\n",
    "from torch.distributions import (\n",
    "    Normal,\n",
    "    Independent,\n",
    "    TransformedDistribution,\n",
    "    TanhTransform,\n",
    ")\n",
    "import bbrl_gymnasium  # noqa: F401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pystk2_gymnasium import AgentSpec\n",
    "from functools import partial\n",
    "import inspect\n",
    "from bbrl.agents.gymnasium import ParallelGymAgent, make_env\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 18:06:12.080929: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-27 18:06:12.208347: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-27 18:06:12.249297: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-27 18:06:12.496728: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-27 18:06:14.976367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"save_best\": True,\n",
    "    \"base_dir\": \"${gym_env.env_name}/sac-S${algorithm.seed}_${current_time:}\",\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 1,\n",
    "        \"n_envs\": 8,\n",
    "        \"n_steps\": 32,\n",
    "        \"buffer_size\": 1e6,\n",
    "        \"batch_size\": 256,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"nb_evals\": 16,\n",
    "        \"eval_interval\": 2_000,\n",
    "        \"learning_starts\": 10_000,\n",
    "        \"max_epochs\": 2_000,\n",
    "        \"discount_factor\": 0.98,\n",
    "        \"entropy_mode\": \"auto\",  # \"auto\" or \"fixed\"\n",
    "        \"init_entropy_coef\": 2e-7,\n",
    "        \"tau_target\": 0.05,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [64, 64],\n",
    "            \"critic_hidden_size\": [256, 256],\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\"env_name\": f\"{env_name}\"},       # 修改为 supertuxkart 的环境名称，但是不确定能否使用 bbrl 库\n",
    "    \"actor_optimizer\": {                          # 如果可行，则后面代码不需要改动\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "    \"entropy_coef_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建SAC算法环境类\n",
    "class SACAlgo(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)  # 调用父类的初始化方法，传入配置参数cfg\n",
    "\n",
    "        # TODO: 重写 self.train_env , 满足该环境要求     \n",
    "        make_stkenv = partial(                                 \n",
    "            make_env,                                          \n",
    "            env_name, \n",
    "            wrappers=get_wrappers(),\n",
    "            render_mode=None,\n",
    "            autoreset=True,\n",
    "            agent=AgentSpec(use_ai=False, name=player_name),\n",
    "        )\n",
    "\n",
    "        self.train_env = ParallelGymAgent(\n",
    "            make_stkenv, \n",
    "            1\n",
    "        ).seed(cfg.algorithm.seed)\n",
    "    \n",
    "        # 获取状态空间和动作空间的大小\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "        # 断言动作空间是否为连续型，若不是则报错提示\n",
    "        assert (\n",
    "            self.train_env.is_continuous_action()\n",
    "        ), \"SAC代码专用于连续动作空间\"\n",
    "\n",
    "        # 创建一个actor（策略网络）\n",
    "        self.actor = SquashedGaussianActor(\n",
    "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "        )\n",
    "\n",
    "        # 创建第一个评论家网络（critic_1）来估计Q值\n",
    "        self.critic_1 = ContinuousQAgent(\n",
    "            obs_size,  # 状态空间的大小\n",
    "            cfg.algorithm.architecture.critic_hidden_size,  # 评论家网络的隐藏层大小\n",
    "            act_size,  # 动作空间的大小\n",
    "        ).with_prefix(\"critic-1/\")  # 添加前缀以区分网络\n",
    "\n",
    "        # 创建目标评论家网络target_critic_1，作为critic_1的深拷贝\n",
    "        self.target_critic_1 = copy.deepcopy(self.critic_1).with_prefix(\n",
    "            \"target-critic-1/\"\n",
    "        )\n",
    "\n",
    "        # 创建第二个评论家网络critic_2，作为SAC的双重Q网络\n",
    "        self.critic_2 = ContinuousQAgent(\n",
    "            obs_size,\n",
    "            cfg.algorithm.architecture.critic_hidden_size,\n",
    "            act_size,\n",
    "        ).with_prefix(\"critic-2/\")\n",
    "\n",
    "        # 创建目标评论家网络target_critic_2，作为critic_2的深拷贝\n",
    "        self.target_critic_2 = copy.deepcopy(self.critic_2).with_prefix(\n",
    "            \"target-critic-2/\"\n",
    "        )\n",
    "\n",
    "        # 训练策略网络的引用，指向actor\n",
    "        self.train_policy = self.actor\n",
    "        # 评估策略网络的引用，使用KWAgentWrapper封装actor，并设定stochastic=False，即使用确定性策略\n",
    "        self.eval_policy = KWAgentWrapper(self.actor, stochastic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_entropy_optimizers(cfg):\n",
    "    # 定义设置熵优化器的函数，参数为配置文件 `cfg`\n",
    "\n",
    "    if cfg.algorithm.entropy_mode == \"auto\":\n",
    "        # 如果配置中的熵模式为自动模式 (\"auto\")，则进行以下操作：\n",
    "\n",
    "        # 注释：优化熵系数的对数值，这略微不同于原论文中的做法，\n",
    "        # 详细讨论见 https://github.com/rail-berkeley/softlearning/issues/37\n",
    "        # 此注释和代码参考自稳定基线3（Stable Baselines3）的SAC实现版本\n",
    "\n",
    "        log_entropy_coef = nn.Parameter(\n",
    "            torch.log(torch.ones(1) * cfg.algorithm.init_entropy_coef)\n",
    "        )  # 定义一个可学习的参数log_entropy_coef，用于存储初始熵系数的对数值\n",
    "           # torch.log(torch.ones(1) * cfg.algorithm.init_entropy_coef) 将初始熵系数取对数以便直接优化其对数值\n",
    "\n",
    "        # 调用 `setup_optimizer` 函数为 `log_entropy_coef` 参数设置优化器\n",
    "        entropy_coef_optimizer = setup_optimizer(\n",
    "            cfg.entropy_coef_optimizer, log_entropy_coef\n",
    "        )\n",
    "\n",
    "        # 返回熵系数优化器 `entropy_coef_optimizer` 和 `log_entropy_coef` 参数\n",
    "        return entropy_coef_optimizer, log_entropy_coef\n",
    "    else:\n",
    "        # 如果熵模式不是自动模式，则返回两个 `None` 值，表示不进行熵系数优化\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_critic_loss(\n",
    "    cfg,\n",
    "    reward: torch.Tensor,\n",
    "    must_bootstrap: torch.Tensor,\n",
    "    t_actor: TemporalAgent,\n",
    "    t_q_agents: TemporalAgent,\n",
    "    t_target_q_agents: TemporalAgent,\n",
    "    rb_workspace: Workspace,\n",
    "    ent_coef: torch.Tensor,\n",
    "):\n",
    "    r\"\"\"Computes the critic loss for a set of $S$ transition samples\n",
    "\n",
    "    Args:\n",
    "        cfg: The experimental configuration\n",
    "        reward: Tensor (2xS) of rewards\n",
    "        must_bootstrap: Tensor (2xS) of indicators\n",
    "        t_actor: The actor agent\n",
    "        t_q_agents: The critics\n",
    "        t_target_q_agents: The target of the critics\n",
    "        rb_workspace: The transition workspace\n",
    "        ent_coef: The entropy coefficient $\\alpha$\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: The two critic losses (scalars)\n",
    "    \"\"\"\n",
    "\n",
    "    # Replay the actor so we get the necessary statistics\n",
    "    \n",
    "    # Compute q_values from both critics with the actions present in the buffer:\n",
    "    # at t, we have Q(s,a) from (s,a) in the RB\n",
    "    t_q_agents(rb_workspace, t=0, n_steps=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Replay the current actor on the replay buffer to get actions of the current actor\n",
    "        t_actor(rb_workspace, t=1, n_steps=1)\n",
    "        action_logprobs_next = rb_workspace[\"action_logprobs\"]\n",
    "\n",
    "        # Compute target q_values from both target critics: at t+1, we have\n",
    "        # Q(s_{t+1}, a_{t+1}) from the (s_{t+1}, a_{t+1}) where a_{t+1} has been\n",
    "        # replaced in the RB with the t_actor line above\n",
    "        t_target_q_agents(rb_workspace, t=1, n_steps=1)\n",
    "\n",
    "    q_values_rb_1, q_values_rb_2, post_q_values_1, post_q_values_2 = rb_workspace[\n",
    "        \"critic-1/q_value\",\n",
    "        \"critic-2/q_value\",\n",
    "        \"target-critic-1/q_value\",\n",
    "        \"target-critic-2/q_value\"\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Compute temporal difference\n",
    "\n",
    "    q_next = torch.minimum(post_q_values_1[1], post_q_values_2[1])\n",
    "    v_phi = q_next - ent_coef * action_logprobs_next[1]\n",
    "\n",
    "    target = reward[1] + cfg.algorithm.discount_factor * v_phi * must_bootstrap[1].int()\n",
    "    critic_loss_1 = nn.functional.mse_loss(q_values_rb_1[0], target)\n",
    "    critic_loss_2 = nn.functional.mse_loss(q_values_rb_2[0], target)\n",
    "    \n",
    "\n",
    "    return critic_loss_1, critic_loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_actor_loss(\n",
    "    ent_coef, t_actor: TemporalAgent, t_q_agents: TemporalAgent, rb_workspace: Workspace\n",
    "):\n",
    "    r\"\"\"\n",
    "    Actor loss computation\n",
    "    :param ent_coef: The entropy coefficient $\\alpha$\n",
    "    :param t_actor: The actor agent (temporal agent)\n",
    "    :param t_q_agents: The critics (as temporal agent)\n",
    "    :param rb_workspace: The replay buffer (2 time steps, $t$ and $t+1$)\n",
    "    \"\"\"\n",
    "\n",
    "    # Recompute the action with the current actor (at $a_t$)\n",
    "\n",
    "    # Step 1: 使用当前 actor 重新计算当前状态下的动作 a_t 和对数概率\n",
    "    t_actor(rb_workspace, t=0, n_steps=1, stochastic=True)\n",
    "    action_logprobs_new = rb_workspace[\"action_logprobs\"]\n",
    "\n",
    "    # Compute Q-values\n",
    "\n",
    "    # Step 2: 使用 Critic 计算当前状态下的 Q 值\n",
    "    t_q_agents(rb_workspace, t=0, n_steps=1)\n",
    "    q_values_1 = rb_workspace[\"critic-1/q_value\"]\n",
    "    q_values_2 = rb_workspace[\"critic-2/q_value\"]\n",
    "\n",
    "    # Step 3: 取两个 Q 值的最小值，减少估值偏差\n",
    "    current_q_values = torch.min(q_values_1, q_values_2)\n",
    "\n",
    "    # Compute the actor loss\n",
    "    # Step 4: 计算 actor 损失\n",
    "\n",
    "    actor_loss = ent_coef * action_logprobs_new[0] - current_q_values[0]\n",
    "    \n",
    "    return actor_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sac(sac: SACAlgo):\n",
    "    cfg = sac.cfg\n",
    "    logger = sac.logger\n",
    "\n",
    "    # init_entropy_coef is the initial value of the entropy coef alpha\n",
    "    ent_coef = cfg.algorithm.init_entropy_coef\n",
    "    tau = cfg.algorithm.tau_target\n",
    "\n",
    "    # Creates the temporal actors\n",
    "    t_actor = TemporalAgent(sac.train_policy)\n",
    "    q_agents = TemporalAgent(Agents(sac.critic_1, sac.critic_2))\n",
    "    target_q_agents = TemporalAgent(Agents(sac.target_critic_1, sac.target_critic_2))\n",
    "\n",
    "    # Configure the optimizer\n",
    "    actor_optimizer = setup_optimizer(cfg.actor_optimizer, sac.actor)\n",
    "    critic_optimizer = setup_optimizer(cfg.critic_optimizer, sac.critic_1, sac.critic_2)\n",
    "    entropy_coef_optimizer, log_entropy_coef = setup_entropy_optimizers(cfg) \n",
    "\n",
    "    # If entropy_mode is not auto, the entropy coefficient ent_coef remains\n",
    "    # fixed. Otherwise, computes the target entropy\n",
    "    if cfg.algorithm.entropy_mode == \"auto\":\n",
    "        # target_entropy is \\mathcal{H}_0 in the SAC and aplications paper.\n",
    "        target_entropy = -np.prod(sac.train_env.action_space.shape).astype(np.float32)\n",
    "\n",
    "    # Loops over successive replay buffers\n",
    "    for rb in sac.iter_replay_buffers():     # TODO: Unknown variable: env/env_obs —— 使用自定义环境时，workspace中没有 env/env_obs\n",
    "        # Implement the SAC algorithm\n",
    "        rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n",
    "        terminated, reward = rb_workspace[\"env/terminated\", \"env/reward\"]\n",
    "\n",
    "        must_boostrap = ~terminated\n",
    "\n",
    "        if entropy_coef_optimizer is not None:\n",
    "            ent_coef = torch.exp(log_entropy_coef.detach())\n",
    "\n",
    "        # Critic update part #############################\n",
    "        (critic_loss_1, critic_loss_2) = compute_critic_loss(\n",
    "            cfg = cfg,\n",
    "            reward = reward,\n",
    "            must_bootstrap = must_boostrap,\n",
    "            t_actor = t_actor,\n",
    "            t_q_agents = q_agents,\n",
    "            t_target_q_agents = target_q_agents,\n",
    "            rb_workspace = rb_workspace,\n",
    "            ent_coef = ent_coef\n",
    "        )\n",
    "\n",
    "        # 记录 Critic 损失\n",
    "        logger.add_log(\"critic_loss_1\", critic_loss_1, sac.nb_steps)\n",
    "        logger.add_log(\"critic_loss_2\", critic_loss_2, sac.nb_steps)\n",
    "\n",
    "        # 反向传播并更新 Critic 参数\n",
    "        critic_loss = critic_loss_1 + critic_loss_2\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            sac.critic_1.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            sac.critic_2.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        # Actor update part #############################\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss = compute_actor_loss(\n",
    "            ent_coef = ent_coef,\n",
    "            t_actor = t_actor,\n",
    "            t_q_agents = q_agents,\n",
    "            rb_workspace = rb_workspace\n",
    "        )\n",
    "\n",
    "        # 记录 Actor 损失\n",
    "        logger.add_log(\"actor_loss\", actor_loss, sac.nb_steps)\n",
    "\n",
    "        # 反向传播并更新 Actor 参数\n",
    "        actor_loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            sac.actor.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        # Entropy optimizer part\n",
    "        if entropy_coef_optimizer is not None:\n",
    "            # See Eq. (17) of the SAC and Applications paper. The log\n",
    "            # probabilities *must* have been computed when computing the actor\n",
    "            # loss.\n",
    "            action_logprobs_rb = rb_workspace[\"action_logprobs\"].detach()\n",
    "            entropy_coef_loss = -(\n",
    "                log_entropy_coef.exp() * (action_logprobs_rb + target_entropy)\n",
    "            ).mean()\n",
    "            entropy_coef_optimizer.zero_grad()\n",
    "            entropy_coef_loss.backward()\n",
    "            entropy_coef_optimizer.step()\n",
    "            logger.add_log(\"entropy_coef_loss\", entropy_coef_loss, sac.nb_steps)\n",
    "            logger.add_log(\"entropy_coef\", ent_coef, sac.nb_steps)\n",
    "\n",
    "        ####################################################\n",
    "\n",
    "        # Soft update of target q function\n",
    "        soft_update_params(sac.critic_1, sac.target_critic_1, tau)\n",
    "        soft_update_params(sac.critic_2, sac.target_critic_2, tau)\n",
    "\n",
    "        sac.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::....:: Antarctica Rendering Engine 2.0 ::..\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262da6b498094ecda7f8bd4f027ba524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Unknown variable: env/env_obs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agents \u001b[38;5;241m=\u001b[39m SACAlgo(OmegaConf\u001b[38;5;241m.\u001b[39mcreate(params))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_sac\u001b[49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36mrun_sac\u001b[0;34m(sac)\u001b[0m\n\u001b[1;32m     23\u001b[0m     target_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mprod(sac\u001b[38;5;241m.\u001b[39mtrain_env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Loops over successive replay buffers\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rb \u001b[38;5;129;01min\u001b[39;00m sac\u001b[38;5;241m.\u001b[39miter_replay_buffers():\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Implement the SAC algorithm\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     rb_workspace \u001b[38;5;241m=\u001b[39m rb\u001b[38;5;241m.\u001b[39mget_shuffled(cfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m     30\u001b[0m     terminated, reward \u001b[38;5;241m=\u001b[39m rb_workspace[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv/terminated\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv/reward\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl_utils/algorithms.py:281\u001b[0m, in \u001b[0;36mEpochBasedAlgo.iter_replay_buffers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m epochs_pb:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# This is the tricky part with transition buffers. The difficulty lies in the\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# copy of the last step and the way to deal with the n_steps return.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# epoch. This is explained in more details in [a previous\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5).\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;66;03m# First run: we start from scratch\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_workspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstochastic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;66;03m# Other runs: we copy the last step and start from there\u001b[39;00m\n\u001b[1;32m    289\u001b[0m         train_workspace\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/utils.py:79\u001b[0m, in \u001b[0;36mTemporalAgent.__call__\u001b[0;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m _t \u001b[38;5;241m=\u001b[39m t\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_variable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         s \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mget(stop_variable, _t)\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/utils.py:31\u001b[0m, in \u001b[0;36mAgents.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, workspace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m---> 31\u001b[0m         \u001b[43ma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/agent.py:84\u001b[0m, in \u001b[0;36mAgent.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m workspace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Agent.__call__] workspace must not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m workspace\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m, in \u001b[0;36mSquashedGaussianActor.forward\u001b[0;34m(self, t, stochastic)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the action a_t and its log-probability p(a_t| s_t)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m:param stochastic: True when sampling\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Computes probabilities over actions\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m normal_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormal_dist(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv/env_obs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     46\u001b[0m action_dist \u001b[38;5;241m=\u001b[39m TransformedDistribution(normal_dist, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh_transform])\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stochastic:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Uses the re-parametrization trick\u001b[39;00m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/agent.py:132\u001b[0m, in \u001b[0;36mAgent.get\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace\u001b[38;5;241m.\u001b[39mget_full(index)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkspace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/workspace.py:368\u001b[0m, in \u001b[0;36mWorkspace.get\u001b[0;34m(self, var_name, t, batch_dims)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m, var_name: \u001b[38;5;28mstr\u001b[39m, t: \u001b[38;5;28mint\u001b[39m, batch_dims: Optional[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    366\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the variable var_name at time t\"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m var_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[var_name]\u001b[38;5;241m.\u001b[39mget(t, batch_dims\u001b[38;5;241m=\u001b[39mbatch_dims)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Unknown variable: env/env_obs"
     ]
    }
   ],
   "source": [
    "agents = SACAlgo(OmegaConf.create(params))\n",
    "run_sac(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::.."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "The path /dev/dri/ cannot be opened or is not available\n",
      "Unable to initialize SDL!: No available video device\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39b61e07c124046b10f853590435116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Unknown variable: env/env_obs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     target_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mprod(sac\u001b[38;5;241m.\u001b[39mtrain_env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Loops over successive replay buffers\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rb \u001b[38;5;129;01min\u001b[39;00m sac\u001b[38;5;241m.\u001b[39miter_replay_buffers():               \u001b[38;5;66;03m# TODO: Unknown variable: env/env_obs\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Implement the SAC algorithm\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     rb_workspace \u001b[38;5;241m=\u001b[39m rb\u001b[38;5;241m.\u001b[39mget_shuffled(cfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m     34\u001b[0m     terminated, reward \u001b[38;5;241m=\u001b[39m rb_workspace[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv/terminated\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv/reward\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl_utils/algorithms.py:281\u001b[0m, in \u001b[0;36mEpochBasedAlgo.iter_replay_buffers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m epochs_pb:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# This is the tricky part with transition buffers. The difficulty lies in the\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# copy of the last step and the way to deal with the n_steps return.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# epoch. This is explained in more details in [a previous\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5).\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;66;03m# First run: we start from scratch\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_workspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstochastic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;66;03m# Other runs: we copy the last step and start from there\u001b[39;00m\n\u001b[1;32m    289\u001b[0m         train_workspace\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/utils.py:79\u001b[0m, in \u001b[0;36mTemporalAgent.__call__\u001b[0;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m _t \u001b[38;5;241m=\u001b[39m t\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_variable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         s \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mget(stop_variable, _t)\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/utils.py:31\u001b[0m, in \u001b[0;36mAgents.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, workspace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m---> 31\u001b[0m         \u001b[43ma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/agent.py:84\u001b[0m, in \u001b[0;36mAgent.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m workspace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Agent.__call__] workspace must not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m workspace\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, t, stochastic)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the action a_t and its log-probability p(a_t| s_t)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m:param stochastic: True when sampling\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Computes probabilities over actions\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m normal_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormal_dist(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv/env_obs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     46\u001b[0m action_dist \u001b[38;5;241m=\u001b[39m TransformedDistribution(normal_dist, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh_transform])\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stochastic:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Uses the re-parametrization trick\u001b[39;00m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/agents/agent.py:132\u001b[0m, in \u001b[0;36mAgent.get\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace\u001b[38;5;241m.\u001b[39mget_full(index)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkspace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.10/site-packages/bbrl/workspace.py:368\u001b[0m, in \u001b[0;36mWorkspace.get\u001b[0;34m(self, var_name, t, batch_dims)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m, var_name: \u001b[38;5;28mstr\u001b[39m, t: \u001b[38;5;28mint\u001b[39m, batch_dims: Optional[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    366\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the variable var_name at time t\"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m var_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[var_name]\u001b[38;5;241m.\u001b[39mget(t, batch_dims\u001b[38;5;241m=\u001b[39mbatch_dims)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Unknown variable: env/env_obs"
     ]
    }
   ],
   "source": [
    "\"\"\" # 将 run_sac 集成到 main 程序中\n",
    "if __name__ == \"__main__\":\n",
    "    sac = SACAlgo(OmegaConf.create(params))\n",
    "\n",
    "    # 以下部分是 run_sac 函数的主体\n",
    "    cfg = sac.cfg\n",
    "    logger = sac.logger\n",
    "\n",
    "    # init_entropy_coef is the initial value of the entropy coef alpha\n",
    "    ent_coef = cfg.algorithm.init_entropy_coef\n",
    "    tau = cfg.algorithm.tau_target\n",
    "\n",
    "    # Creates the temporal actors\n",
    "    t_actor = TemporalAgent(sac.train_policy)\n",
    "    q_agents = TemporalAgent(Agents(sac.critic_1, sac.critic_2))\n",
    "    target_q_agents = TemporalAgent(Agents(sac.target_critic_1, sac.target_critic_2))\n",
    "\n",
    "    # Configure the optimizer\n",
    "    actor_optimizer = setup_optimizer(cfg.actor_optimizer, sac.actor)\n",
    "    critic_optimizer = setup_optimizer(cfg.critic_optimizer, sac.critic_1, sac.critic_2)\n",
    "    entropy_coef_optimizer, log_entropy_coef = setup_entropy_optimizers(cfg) \n",
    "\n",
    "    # If entropy_mode is not auto, the entropy coefficient ent_coef remains\n",
    "    # fixed. Otherwise, computes the target entropy\n",
    "    if cfg.algorithm.entropy_mode == \"auto\":\n",
    "        # target_entropy is \\mathcal{H}_0 in the SAC and aplications paper.\n",
    "        target_entropy = -np.prod(sac.train_env.action_space.shape).astype(np.float32)\n",
    "\n",
    "    # Loops over successive replay buffers\n",
    "    for rb in sac.iter_replay_buffers():               # TODO: Unknown variable: env/env_obs\n",
    "\n",
    "        # Implement the SAC algorithm\n",
    "        rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n",
    "        terminated, reward = rb_workspace[\"env/terminated\", \"env/reward\"]\n",
    "\n",
    "        must_boostrap = ~terminated\n",
    "\n",
    "        if entropy_coef_optimizer is not None:\n",
    "            ent_coef = torch.exp(log_entropy_coef.detach())\n",
    "\n",
    "        # Critic update part #############################\n",
    "        (critic_loss_1, critic_loss_2) = compute_critic_loss(\n",
    "            cfg = cfg,\n",
    "            reward = reward,\n",
    "            must_bootstrap = must_boostrap,\n",
    "            t_actor = t_actor,\n",
    "            t_q_agents = q_agents,\n",
    "            t_target_q_agents = target_q_agents,\n",
    "            rb_workspace = rb_workspace,\n",
    "            ent_coef = ent_coef\n",
    "        )\n",
    "\n",
    "        # 记录 Critic 损失\n",
    "        logger.add_log(\"critic_loss_1\", critic_loss_1, sac.nb_steps)\n",
    "        logger.add_log(\"critic_loss_2\", critic_loss_2, sac.nb_steps)\n",
    "\n",
    "        # 反向传播并更新 Critic 参数\n",
    "        critic_loss = critic_loss_1 + critic_loss_2\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            sac.critic_1.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            sac.critic_2.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        # Actor update part #############################\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss = compute_actor_loss(\n",
    "            ent_coef = ent_coef,\n",
    "            t_actor = t_actor,\n",
    "            t_q_agents = q_agents,\n",
    "            rb_workspace = rb_workspace\n",
    "        )\n",
    "\n",
    "        # 记录 Actor 损失\n",
    "        logger.add_log(\"actor_loss\", actor_loss, sac.nb_steps)\n",
    "\n",
    "        # 反向传播并更新 Actor 参数\n",
    "        actor_loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            sac.actor.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        # Entropy optimizer part\n",
    "        if entropy_coef_optimizer is not None:\n",
    "            # See Eq. (17) of the SAC and Applications paper. The log\n",
    "            # probabilities *must* have been computed when computing the actor\n",
    "            # loss.\n",
    "            action_logprobs_rb = rb_workspace[\"action_logprobs\"].detach()\n",
    "            entropy_coef_loss = -(\n",
    "                log_entropy_coef.exp() * (action_logprobs_rb + target_entropy)\n",
    "            ).mean()\n",
    "            entropy_coef_optimizer.zero_grad()\n",
    "            entropy_coef_loss.backward()\n",
    "            entropy_coef_optimizer.step()\n",
    "            logger.add_log(\"entropy_coef_loss\", entropy_coef_loss, sac.nb_steps)\n",
    "            logger.add_log(\"entropy_coef\", ent_coef, sac.nb_steps)\n",
    "\n",
    "        ####################################################\n",
    "\n",
    "        # Soft update of target q function\n",
    "        soft_update_params(sac.critic_1, sac.target_critic_1, tau)\n",
    "        soft_update_params(sac.critic_2, sac.target_critic_2, tau)\n",
    "\n",
    "        # agents.evaluate() \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
