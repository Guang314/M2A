{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f12bd97",
   "metadata": {},
   "source": [
    " Copyright © Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee096ca",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "In this notebook we code the Truncated Quantile Critic (TQC) algorithm using\n",
    "BBRL. This algorithm is described in [this\n",
    "paper](http://proceedings.mlr.press/v119/kuznetsov20a/kuznetsov20a.pdf).\n",
    "\n",
    "To understand this code, you need to know more about [the BBRL interaction\n",
    "model](https://github.com/osigaud/bbrl/blob/master/docs/overview.md) Then you\n",
    "should run [a didactical\n",
    "example](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/03-multi_env_autoreset.student.ipynb)\n",
    "to see how agents interact in BBRL when autoreset=True.\n",
    "\n",
    "The algorithm is explained in [this\n",
    "video](https://www.youtube.com/watch?v=U20F-MvThjM) (in the end, after SAC)\n",
    "and you can also read [the corresponding\n",
    "slides](https://dac.lip6.fr/wp-content/uploads/2022/11/12_sac.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71b65ae",
   "metadata": {},
   "source": [
    "# 展望\n",
    "\n",
    "在此笔记本中，我们使用 BBRL 实现了截断分位数评论家（Truncated Quantile Critic, TQC）算法。该算法在[此论文](http://proceedings.mlr.press/v119/kuznetsov20a/kuznetsov20a.pdf)中有详细描述。\n",
    "\n",
    "为了理解此代码，你需要了解更多关于[BBRL 交互模型](https://github.com/osigaud/bbrl/blob/master/docs/overview.md)的知识。接着可以运行[一个教学示例](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/03-multi_env_autoreset.student.ipynb)，以了解在 `autoreset=True` 时代理在 BBRL 中的交互方式。\n",
    "\n",
    "算法的讲解可以参考[这个视频](https://www.youtube.com/watch?v=U20F-MvThjM)（在最后部分，继 SAC 之后），你也可以查看[相应的幻灯片](https://dac.lip6.fr/wp-content/uploads/2022/11/12_sac.pdf)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7a291c",
   "metadata": {},
   "source": [
    "## Installation and Imports\n",
    "\n",
    "### Installation\n",
    "\n",
    "The BBRL library is [here](https://github.com/osigaud/bbrl).\n",
    "\n",
    "Below, we import standard python packages, pytorch packages and gymnasium\n",
    "environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3447f869",
   "metadata": {},
   "source": [
    "## 安装与导入\n",
    "\n",
    "### 安装\n",
    "\n",
    "BBRL 库可以在 [这里](https://github.com/osigaud/bbrl) 找到。\n",
    "\n",
    "下面，我们导入标准的 Python 包、PyTorch 包和 Gymnasium 环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d76d05",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install easypip\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.3.5\")\n",
    "easyinstall(\"bbrl_gymnasium[box2d]\")\n",
    "easyinstall(\"bbrl_gymnasium[classic_control]\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89669854",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import bbrl_gymnasium\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Optional, Iterator\n",
    "from functools import partial\n",
    "from omegaconf import OmegaConf\n",
    "from abc import abstractmethod, ABC\n",
    "from time import strftime\n",
    "from bbrl import instantiate_class\n",
    "\n",
    "\n",
    "# Useful when using a timestamp for a directory name\n",
    "OmegaConf.register_new_resolver(\n",
    "    \"current_time\", lambda: strftime(\"%Y%m%d-%H%M%S\"), replace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f2c52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "lines_to_next_cell": 2,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports all the necessary classes and functions from BBRL\n",
    "from bbrl import get_arguments, get_class, instantiate_class\n",
    "\n",
    "# The workspace is the main class in BBRL, this is where all data is collected and stored\n",
    "from bbrl.workspace import Workspace\n",
    "\n",
    "# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n",
    "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace,\n",
    "# or until a given condition is reached\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent, KWAgentWrapper\n",
    "\n",
    "# ParallelGymAgent is an agent able to execute a batch of gymnasium environments\n",
    "# with auto-resetting. These agents produce multiple variables in the workspace:\n",
    "# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/terminated’,\n",
    "# 'env/truncated', 'env/done', ’env/cumulated_reward’, ...\n",
    "#\n",
    "# When called at timestep t=0, the environments are automatically reset. At\n",
    "# timestep t>0, these agents will read the ’action’ variable in the workspace at\n",
    "# time t − 1\n",
    "from bbrl.agents.gymnasium import GymAgent, ParallelGymAgent, make_env, record_video\n",
    "\n",
    "# Replay buffers are useful to store past transitions when training\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab9815b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Utility function for launching tensorboard\n",
    "# For Colab - otherwise, it is easier and better to launch tensorboard from\n",
    "# the terminal\n",
    "def setup_tensorboard(path):\n",
    "    path = Path(path)\n",
    "    answer = \"\"\n",
    "    if is_notebook():\n",
    "        if get_ipython().__class__.__module__ == \"google.colab._shell\":\n",
    "            answer = \"y\"\n",
    "        while answer not in [\"y\", \"n\"]:\n",
    "            answer = input(\n",
    "                f\"Do you want to launch tensorboard in this notebook [y/n] \"\n",
    "            ).lower()\n",
    "\n",
    "    if answer == \"y\":\n",
    "        get_ipython().run_line_magic(\"load_ext\", \"tensorboard\")\n",
    "        get_ipython().run_line_magic(\"tensorboard\", f\"--logdir {path.absolute()}\")\n",
    "    else:\n",
    "        import sys\n",
    "        import os\n",
    "        import os.path as osp\n",
    "\n",
    "        print(\n",
    "            f\"Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir={path.absolute()}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39138e5f",
   "metadata": {},
   "source": [
    "## Definition of Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11079a03",
   "metadata": {},
   "source": [
    "### Functions to build networks\n",
    "\n",
    "We define a few utilitary functions to build neural networks\n",
    "\n",
    "The function below builds a multi-layer perceptron where the size of each\n",
    "layer is given in the `size` list. We also specify the activation function of\n",
    "neurons at each layer and optionally a different activation function for the\n",
    "final layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49455f8d",
   "metadata": {},
   "source": [
    "### 构建网络的函数\n",
    "\n",
    "我们定义了一些实用函数来构建神经网络。\n",
    "\n",
    "下面的函数构建一个多层感知器，每一层的大小由 `size` 列表给出。我们还指定了每层神经元的激活函数，并可以选择为最后一层指定不同的激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4710d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n",
    "    \"\"\"Helper function to build a multi-layer perceptron (function from $\\mathbb R^n$ to $\\mathbb R^p$)\n",
    "    \n",
    "    Args:\n",
    "        sizes (List[int]): the number of neurons at each layer\n",
    "        activation (nn.Module): a PyTorch activation function (after each layer but the last)\n",
    "        output_activation (nn.Module): a PyTorch activation function (last layer)\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = activation if j < len(sizes) - 2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_backbone(sizes, activation):\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        layers += [nn.Linear(sizes[j], sizes[j + 1]), activation]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8738a659",
   "metadata": {},
   "source": [
    "### Base Actor\n",
    "\n",
    "All actors should inherit from the ```BaseActor``` class, so that we can easily copy their parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc9265e",
   "metadata": {},
   "source": [
    "### 基础 Actor\n",
    "\n",
    "所有的 Actor 都应该继承自 `BaseActor` 类，这样我们可以方便地复制它们的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d71b515",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BaseActor(Agent):\n",
    "    \"\"\" Generic class to centralize copy_parameters\"\"\"\n",
    "\n",
    "    def copy_parameters(self, other):\n",
    "        \"\"\"Copy parameters from other agent\"\"\"\n",
    "        for self_p, other_p in zip(self.parameters(), other.parameters()):\n",
    "            self_p.data.copy_(other_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ab114",
   "metadata": {},
   "source": [
    "### Squashed Gaussian Policy\n",
    "\n",
    "Like SAC, TQC works better with a Squashed Gaussian policy, which enables the reparametrization trick.\n",
    "\n",
    "The code of the `SquashedGaussianActor` policy is below, it is the same as for SAC.\n",
    "\n",
    "It relies on a specific type of distribution, the `SquashedDiagGaussianDistribution` which is taken from [the Stable Baselines3 library](https://github.com/DLR-RM/stable-baselines3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519afa59",
   "metadata": {},
   "source": [
    "### 压缩高斯策略\n",
    "\n",
    "与 SAC 一样，TQC 在使用压缩高斯策略时效果更好，这种策略能够实现重参数化技巧。\n",
    "\n",
    "下面是 `SquashedGaussianActor` 策略的代码，与 SAC 中的代码相同。\n",
    "\n",
    "它依赖于一种特定类型的分布，即 `SquashedDiagGaussianDistribution`，该分布来自 [Stable Baselines3 库](https://github.com/DLR-RM/stable-baselines3)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834d392",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from bbrl.utils.distributions import SquashedDiagGaussianDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a882c62",
   "metadata": {},
   "source": [
    "The fact that we use the reparametrization trick is hidden inside the code of this distribution. In more details, the key is that the [`sample(self)` method](https://github.com/osigaud/bbrl/blob/master/src/bbrl/utils/distributions.py#L200) calls `rsample()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7bbcad",
   "metadata": {},
   "source": [
    "我们使用重参数化技巧的事实隐藏在该分布的代码中。更具体地说，关键在于 [`sample(self)` 方法](https://github.com/osigaud/bbrl/blob/master/src/bbrl/utils/distributions.py#L200) 调用了 `rsample()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d344450",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SquashedGaussianActor(BaseActor):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        backbone_dim = [state_dim] + list(hidden_layers)\n",
    "        self.layers = build_backbone(backbone_dim, activation=nn.Tanh())\n",
    "        self.backbone = nn.Sequential(*self.layers)\n",
    "        self.last_mean_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
    "        self.last_std_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
    "        self.action_dist = SquashedDiagGaussianDistribution(action_dim)\n",
    "\n",
    "    def get_distribution(self, obs: torch.Tensor):\n",
    "        backbone_output = self.backbone(obs)\n",
    "        mean = self.last_mean_layer(backbone_output)\n",
    "        std_out = self.last_std_layer(backbone_output)\n",
    "\n",
    "        std_out = std_out.clamp(-20, 2)  # as in the official code\n",
    "        std = torch.exp(std_out)\n",
    "        return self.action_dist.make_distribution(mean, std)\n",
    "\n",
    "    def forward(self, t, stochastic=False, predict_proba=False, **kwargs):\n",
    "        action_dist = self.get_distribution(self.get((\"env/env_obs\", t)))\n",
    "        if predict_proba:\n",
    "            action = self.get((\"action\", t))\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            self.set((\"logprob_predict\", t), log_prob)\n",
    "        else:\n",
    "            if stochastic:\n",
    "                action = action_dist.sample()\n",
    "            else:\n",
    "                action = action_dist.mode()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            self.set((\"action\", t), action)\n",
    "            self.set((\"action_logprobs\", t), log_prob)\n",
    "\n",
    "    def predict_action(self, obs, stochastic=False):\n",
    "        \"\"\"Predict just one action (without using the workspace)\"\"\"\n",
    "        action_dist = self.get_distribution(obs)\n",
    "        return action_dist.sample() if stochastic else action_dist.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add19d4b",
   "metadata": {},
   "source": [
    "### CriticAgent\n",
    "\n",
    "As critic, TQC uses a network with several heads, defined below. As seen in the forward function, it outputs a vector of quantiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562688a4",
   "metadata": {},
   "source": [
    "### 评论家代理（CriticAgent）\n",
    "\n",
    "作为评论家，TQC 使用一个具有多个头部的网络，具体定义如下。在前向传播函数中，它输出一个分位数向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe643b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TruncatedQuantileNetwork(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, n_nets, action_dim, n_quantiles):\n",
    "        super().__init__()\n",
    "        self.is_q_function = True\n",
    "        self.nets = []\n",
    "        for i in range(n_nets):\n",
    "            net = build_mlp([state_dim + action_dim] + list(hidden_layers) + [n_quantiles], activation=nn.ReLU())\n",
    "            self.add_module(f'qf{i}', net)\n",
    "            self.nets.append(net)\n",
    "\n",
    "    def forward(self, t):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = self.get((\"action\", t))\n",
    "        obs_act = torch.cat((obs, action), dim=1)\n",
    "        quantiles = torch.stack(tuple(net(obs_act) for net in self.nets), dim=1)\n",
    "        self.set((\"quantiles\", t), quantiles)\n",
    "        return quantiles\n",
    "\n",
    "    def predict_value(self, obs, action):\n",
    "        obs_act = torch.cat((obs, action), dim=0)\n",
    "        quantiles = torch.stack(tuple(net(obs_act) for net in self.nets), dim=1)\n",
    "        return quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a71aa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Training and evaluation environments\n",
    "\n",
    "We build two environments: one for training and another one for evaluation.\n",
    "\n",
    "For training, it is more efficient to use an autoreset agent, as we do not\n",
    "want to waste time if the task is done in an environment sooner than in the\n",
    "others.\n",
    "\n",
    "By contrast, for evaluation, we just need to perform a fixed number of\n",
    "episodes (for statistics), thus it is more convenient to use a\n",
    "noautoreset agent with a set of environments and just run one episode in\n",
    "each environment. Thus we can use the `env/done` stop variable and take the\n",
    "average over the cumulated reward of all environments.\n",
    "\n",
    "See [this\n",
    "notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing)\n",
    "for explanations about agents and environment agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a47ea86",
   "metadata": {},
   "source": [
    "### 训练和评估环境\n",
    "\n",
    "我们构建了两个环境：一个用于训练，另一个用于评估。\n",
    "\n",
    "对于训练，使用自动重置代理更有效，因为如果任务在某个环境中比在其他环境中完成得更快，我们不想浪费时间。\n",
    "\n",
    "相反，对于评估，我们只需要执行固定数量的回合（用于统计），因此使用不自动重置的代理与一组环境更方便，并且只需在每个环境中运行一个回合。这样我们可以使用 `env/done` 停止变量，并计算所有环境的累积奖励的平均值。\n",
    "\n",
    "有关代理和环境代理的解释，请参见[此笔记本](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from bbrl.agents.gymnasium import make_env, GymAgent, ParallelGymAgent\n",
    "from functools import partial\n",
    "\n",
    "def get_env_agents(cfg, *, autoreset=True, include_last_state=True) -> Tuple[GymAgent, GymAgent]:\n",
    "    # Returns a pair of environments (train / evaluation) based on a configuration `cfg`\n",
    "    \n",
    "    # Train environment\n",
    "    train_env_agent = ParallelGymAgent(\n",
    "        partial(make_env, cfg.gym_env.env_name, autoreset=autoreset),\n",
    "        cfg.algorithm.n_envs, \n",
    "        include_last_state=include_last_state\n",
    "    ).seed(cfg.algorithm.seed)\n",
    "\n",
    "    # Test environment\n",
    "    eval_env_agent = ParallelGymAgent(\n",
    "        partial(make_env, cfg.gym_env.env_name), \n",
    "        cfg.algorithm.nb_evals,\n",
    "        include_last_state=include_last_state\n",
    "    ).seed(cfg.algorithm.seed)\n",
    "\n",
    "    return train_env_agent, eval_env_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16303166",
   "metadata": {},
   "source": [
    "### Building the complete training and evaluation agents\n",
    "\n",
    "In the code below we create the Squashed Gaussian actor, one critic and the corresponding target critic. Beforehand, we checked that the environment takes continuous actions (otherwise we would need a different code).\n",
    "\n",
    "An good exercise is to check that TQC also works without a target critic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb1e4f",
   "metadata": {},
   "source": [
    "### 构建完整的训练和评估代理\n",
    "\n",
    "在下面的代码中，我们创建了压缩高斯 Actor、一个评论家和相应的目标评论家。在此之前，我们检查了环境是否接受连续动作（否则我们需要不同的代码）。\n",
    "\n",
    "一个好的练习是检查 TQC 在没有目标评论家的情况下是否也能正常工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9195bff",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create the TQC Agent\n",
    "def create_tqc_agent(cfg, train_env_agent, eval_env_agent):\n",
    "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n",
    "    assert (\n",
    "        train_env_agent.is_continuous_action()\n",
    "    ), \"TQC code dedicated to continuous actions\"\n",
    "\n",
    "    # Actor\n",
    "    actor = SquashedGaussianActor(\n",
    "        obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "    )\n",
    "\n",
    "    # Train/Test agents\n",
    "    tr_agent = Agents(train_env_agent, actor)\n",
    "    ev_agent = Agents(eval_env_agent, actor)\n",
    "\n",
    "    # Builds the critics\n",
    "    critic = TruncatedQuantileNetwork(\n",
    "        obs_size, cfg.algorithm.architecture.critic_hidden_size,\n",
    "        cfg.algorithm.architecture.n_nets, act_size,\n",
    "        cfg.algorithm.architecture.n_quantiles\n",
    "    )\n",
    "    target_critic = copy.deepcopy(critic)\n",
    "\n",
    "    train_agent = TemporalAgent(tr_agent)\n",
    "    eval_agent = TemporalAgent(ev_agent)\n",
    "    train_agent.seed(cfg.algorithm.seed)\n",
    "    return (\n",
    "        train_agent,\n",
    "        eval_agent,\n",
    "        actor,\n",
    "        critic,\n",
    "        target_critic\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810ff1f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### The Logger class\n",
    "\n",
    "The logger is in charge of collecting statistics during the training process.\n",
    "The logger defines the following methods, where `steps` is the number of steps\n",
    "since the training began:\n",
    "- `logger.log_losses(critic_loss: float, entropy_loss: float, actor_loss:\n",
    "  float, steps: int)`\n",
    "- `logger.log_reward_losses(self, rewards: torch.Tensor, nb_steps)`\n",
    "- `logger.add_log(log_string: float, loss: float, steps: int)`\n",
    "\n",
    "Having logging provided under the hood is one of the features allowing you\n",
    "to save time when using RL libraries like BBRL.\n",
    "\n",
    "In these notebooks, the logger is defined as `bbrl.utils.logger.TFLogger` so as\n",
    "to use a tensorboard visualisation (see the configuration parameters).\n",
    "\n",
    "Note that the BBRL Logger is also saving the log in a readable format such\n",
    "that you can use `Logger.read_directories(...)` to read multiple logs, create\n",
    "a dataframe, and analyze many experiments afterward in a notebook for\n",
    "instance. The code for the different kinds of loggers is available in the\n",
    "[bbrl/utils/logger.py](https://github.com/osigaud/bbrl/blob/master/src/bbrl/utils/logger.py)\n",
    "file.\n",
    "\n",
    "`instantiate_class` is an inner BBRL mechanism. The\n",
    "`instantiate_class`function is available in the\n",
    "[`bbrl/__init__.py`](https://github.com/osigaud/bbrl/blob/master/src/bbrl/__init__.py)\n",
    "file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784f926",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "### Logger 类\n",
    "\n",
    "Logger 负责在训练过程中收集统计数据。Logger 定义了以下方法，其中 `steps` 是训练开始以来的步数：\n",
    "- `logger.log_losses(critic_loss: float, entropy_loss: float, actor_loss: float, steps: int)`\n",
    "- `logger.log_reward_losses(self, rewards: torch.Tensor, nb_steps)`\n",
    "- `logger.add_log(log_string: float, loss: float, steps: int)`\n",
    "\n",
    "在后台提供日志记录是使用 RL 库（如 BBRL）时节省时间的功能之一。\n",
    "\n",
    "在这些笔记本中，Logger 被定义为 `bbrl.utils.logger.TFLogger`，以便使用 tensorboard 可视化（请参见配置参数）。\n",
    "\n",
    "请注意，BBRL Logger 还将日志保存为可读格式，这样您可以使用 `Logger.read_directories(...)` 来读取多个日志，创建数据框，并在笔记本中分析许多实验。例如。不同类型的 Logger 代码可在 [bbrl/utils/logger.py](https://github.com/osigaud/bbrl/blob/master/src/bbrl/utils/logger.py) 文件中找到。\n",
    "\n",
    "`instantiate_class` 是 BBRL 的一个内部机制。`instantiate_class` 函数在 [`bbrl/__init__.py`](https://github.com/osigaud/bbrl/blob/master/src/bbrl/__init__.py) 文件中可用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62fcae1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, cfg):\n",
    "        self.logger = instantiate_class(cfg.logger)\n",
    "\n",
    "    def add_log(self, log_string: float, loss: float, steps: int):\n",
    "        self.logger.add_scalar(log_string, loss.item(), steps)\n",
    "\n",
    "    # A specific function for RL algorithms having a critic, an actor and an\n",
    "    # entropy losses\n",
    "    def log_losses(\n",
    "        self, critic_loss: float, entropy_loss: float, actor_loss: float, steps: int\n",
    "    ):\n",
    "        self.add_log(\"critic_loss\", critic_loss, steps)\n",
    "        self.add_log(\"entropy_loss\", entropy_loss, steps)\n",
    "        self.add_log(\"actor_loss\", actor_loss, steps)\n",
    "\n",
    "    def log_reward_losses(self, rewards: torch.Tensor, nb_steps):\n",
    "        self.add_log(\"reward/mean\", rewards.mean(), nb_steps)\n",
    "        self.add_log(\"reward/max\", rewards.max(), nb_steps)\n",
    "        self.add_log(\"reward/min\", rewards.min(), nb_steps)\n",
    "        self.add_log(\"reward/median\", rewards.median(), nb_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d576bb",
   "metadata": {},
   "source": [
    "### Setup the optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the optimizer\n",
    "def setup_optimizers(cfg, actor, critic):\n",
    "    actor_optimizer_args = get_arguments(cfg.actor_optimizer)\n",
    "    parameters = actor.parameters()\n",
    "    actor_optimizer = get_class(cfg.actor_optimizer)(parameters, **actor_optimizer_args)\n",
    "    critic_optimizer_args = get_arguments(cfg.critic_optimizer)\n",
    "    parameters = critic.parameters()\n",
    "    critic_optimizer = get_class(cfg.critic_optimizer)(\n",
    "        parameters, **critic_optimizer_args\n",
    "    )\n",
    "    return actor_optimizer, critic_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0dfd4e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def setup_entropy_optimizers(cfg):\n",
    "    if cfg.algorithm.target_entropy == \"auto\":\n",
    "        entropy_coef_optimizer_args = get_arguments(cfg.entropy_coef_optimizer)\n",
    "        # Note: we optimize the log of the entropy coef which is slightly different from the paper\n",
    "        # as discussed in https://github.com/rail-berkeley/softlearning/issues/37\n",
    "        # Comment and code taken from the SB3 version of SAC\n",
    "        log_entropy_coef = torch.log(\n",
    "            torch.ones(1) * cfg.algorithm.entropy_coef\n",
    "        ).requires_grad_(True)\n",
    "        entropy_coef_optimizer = get_class(cfg.entropy_coef_optimizer)(\n",
    "            [log_entropy_coef], **entropy_coef_optimizer_args\n",
    "        )\n",
    "    else:\n",
    "        log_entropy_coef = 0\n",
    "        entropy_coef_optimizer = None\n",
    "    return entropy_coef_optimizer, log_entropy_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756aa1f3",
   "metadata": {},
   "source": [
    "### Compute the critic loss\n",
    "\n",
    "By contrast with the SAC version, we prepare data and compute the critic loss into a single function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a917d0",
   "metadata": {},
   "source": [
    "### 计算评论家损失\n",
    "\n",
    "与 SAC 版本不同，我们在一个函数中准备数据并计算评论家损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873eca2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_critic_loss(\n",
    "        cfg, reward, must_bootstrap,\n",
    "        t_actor,\n",
    "        q_agent,\n",
    "        target_q_agent,\n",
    "        rb_workspace,\n",
    "        ent_coef\n",
    "):\n",
    "    # Compute quantiles from critic with the actions present in the buffer:\n",
    "    # at t, we have Qu  ntiles(s,a) from the (s,a) in the RB\n",
    "    q_agent(rb_workspace, t=0, n_steps=1)\n",
    "    quantiles = rb_workspace[\"quantiles\"].squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Replay the current actor on the replay buffer to get actions of the\n",
    "        # current policy\n",
    "        t_actor(rb_workspace, t=1, n_steps=1, stochastic=True)\n",
    "        action_logprobs_next = rb_workspace[\"action_logprobs\"]\n",
    "\n",
    "        # Compute target quantiles from the target critic: at t+1, we have\n",
    "        # Quantiles(s+1,a+1) from the (s+1,a+1) where a+1 has been replaced in the RB\n",
    "\n",
    "        target_q_agent(rb_workspace, t=1, n_steps=1)\n",
    "        post_quantiles = rb_workspace[\"quantiles\"][1]\n",
    "\n",
    "        sorted_quantiles, _ = torch.sort(post_quantiles.reshape(quantiles.shape[0], -1))\n",
    "        quantiles_to_drop_total = cfg.algorithm.top_quantiles_to_drop * cfg.algorithm.architecture.n_nets\n",
    "        truncated_sorted_quantiles = sorted_quantiles[:,\n",
    "                                     :quantiles.size(-1) * quantiles.size(-2) - quantiles_to_drop_total]\n",
    "\n",
    "        # compute the target\n",
    "        logprobs = ent_coef * action_logprobs_next[1]\n",
    "        y = reward[0].unsqueeze(-1) + must_bootstrap.int().unsqueeze(-1) * cfg.algorithm.discount_factor * (\n",
    "                    truncated_sorted_quantiles - logprobs.unsqueeze(-1))\n",
    "\n",
    "    # computing the Huber loss\n",
    "    pairwise_delta = y[:, None, None, :] - quantiles[:, :, :, None]  # batch x nets x quantiles x samples\n",
    "\n",
    "    abs_pairwise_delta = torch.abs(pairwise_delta)\n",
    "    huber_loss = torch.where(abs_pairwise_delta > 1,\n",
    "                             abs_pairwise_delta - 0.5,\n",
    "                             pairwise_delta ** 2 * 0.5)\n",
    "\n",
    "    n_quantiles = quantiles.shape[2]\n",
    "    tau = torch.arange(n_quantiles).float() / n_quantiles + 1 / 2 / n_quantiles\n",
    "    loss = (torch.abs(tau[None, None, :, None] - (pairwise_delta < 0).float()) * huber_loss).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a915dcd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Soft parameter updates\n",
    "\n",
    "To update the target critic, one uses the following equation:\n",
    "$\\theta' \\leftarrow \\tau \\theta + (1- \\tau) \\theta'$\n",
    "where $\\theta$ is the vector of parameters of the critic, and $\\theta'$ is the vector of parameters of the target critic.\n",
    "The `soft_update_params(...)` function is in charge of performing this soft update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fecf287",
   "metadata": {},
   "source": [
    "### 软参数更新\n",
    "\n",
    "为了更新目标评论家，使用以下公式：\n",
    "$$\n",
    "\\theta' \\leftarrow \\tau \\theta + (1 - \\tau) \\theta'\n",
    "$$\n",
    "其中 $\\theta$ 是评论家的参数向量，而 $\\theta'$ 是目标评论家的参数向量。`soft_update_params(...)` 函数负责执行这个软更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d1f99",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def soft_update_params(net, target_net, tau):\n",
    "    for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89841a74",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Compute the actor loss\n",
    "\n",
    "Again, by contrast with the SAC version, we prepare data and compute the actor loss into a single function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b6758",
   "metadata": {},
   "source": [
    "### 计算演员损失\n",
    "\n",
    "同样，与 SAC 版本不同，我们在一个函数中准备数据并计算演员损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f799660",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_actor_loss(ent_coef, t_actor, q_agent, rb_workspace):\n",
    "    \"\"\"Actor loss computation\n",
    "\n",
    "    :param ent_coef: The entropy coefficient $\\alpha$\n",
    "    :param t_actor: The actor agent (temporal agent)\n",
    "    :param q_agent: The critic (temporal agent) (n net of m quantiles)\n",
    "    :param rb_workspace: The replay buffer (2 time steps, $t$ and $t+1$)\n",
    "    \"\"\"\n",
    "    # Recompute the quantiles from the current policy, not from the actions in the buffer\n",
    "\n",
    "    t_actor(rb_workspace, t=0, n_steps=1, stochastic=True)\n",
    "    action_logprobs_new = rb_workspace[\"action_logprobs\"]\n",
    "\n",
    "    q_agent(rb_workspace, t=0, n_steps=1)\n",
    "    quantiles = rb_workspace[\"quantiles\"][0]\n",
    "\n",
    "    actor_loss = (ent_coef * action_logprobs_new[0] - quantiles.mean(2).mean(1))\n",
    "\n",
    "    return actor_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2eb7f",
   "metadata": {},
   "source": [
    "## Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69af9f2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_tqc(cfg):\n",
    "    # 1)  Build the  logger\n",
    "    logger = Logger(cfg)\n",
    "    best_reward = float('-inf')\n",
    "    ent_coef = cfg.algorithm.entropy_coef\n",
    "\n",
    "    # 2) Create the environment agent\n",
    "    train_env_agent = AutoResetGymAgent(\n",
    "        get_class(cfg.gym_env),\n",
    "        get_arguments(cfg.gym_env),\n",
    "        cfg.algorithm.n_envs,\n",
    "        cfg.algorithm.seed,\n",
    "    )\n",
    "    eval_env_agent = NoAutoResetGymAgent(\n",
    "        get_class(cfg.gym_env),\n",
    "        get_arguments(cfg.gym_env),\n",
    "        cfg.algorithm.nb_evals,\n",
    "        cfg.algorithm.seed,\n",
    "    )\n",
    "\n",
    "    # 3) Create the A2C Agent\n",
    "    (\n",
    "        train_agent,\n",
    "        eval_agent,\n",
    "        actor,\n",
    "        critic,\n",
    "        target_critic\n",
    "    ) = create_tqc_agent(cfg, train_env_agent, eval_env_agent)\n",
    "\n",
    "    t_actor = TemporalAgent(actor)\n",
    "    q_agent = TemporalAgent(critic)\n",
    "    target_q_agent = TemporalAgent(target_critic)\n",
    "    train_workspace = Workspace()\n",
    "\n",
    "    # Creates a replay buffer\n",
    "    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n",
    "\n",
    "    # Configure the optimizer\n",
    "    actor_optimizer, critic_optimizer = setup_optimizers(cfg, actor, critic)\n",
    "    entropy_coef_optimizer, log_entropy_coef = setup_entropy_optimizers(cfg)\n",
    "    nb_steps = 0\n",
    "    tmp_steps = 0\n",
    "\n",
    "    # Initial value of the entropy coef alpha. If target_entropy is not auto,\n",
    "    # will remain fixed\n",
    "    if cfg.algorithm.target_entropy == \"auto\":\n",
    "        target_entropy = -np.prod(train_env_agent.action_space.shape).astype(np.float32)\n",
    "    else:\n",
    "        target_entropy = cfg.algorithm.target_entropy\n",
    "\n",
    "    # Training loop\n",
    "    pbar = tqdm(range(cfg.algorithm.nb_episodes))\n",
    "    for episode in pbar:\n",
    "        # Execute the agent in the workspace\n",
    "        if epoch > 0:\n",
    "            train_workspace.zero_grad()\n",
    "            train_workspace.copy_n_last_steps(1)\n",
    "            train_agent(\n",
    "                train_workspace,\n",
    "                t=1,\n",
    "                n_steps=cfg.algorithm.n_steps - 1,\n",
    "                stochastic=True,\n",
    "            )\n",
    "        else:\n",
    "            train_agent(\n",
    "                train_workspace,\n",
    "                t=0,\n",
    "                n_steps=cfg.algorithm.n_steps,\n",
    "                stochastic=True,\n",
    "            )\n",
    "\n",
    "        transition_workspace = train_workspace.get_transitions()\n",
    "        action = transition_workspace[\"action\"]\n",
    "        nb_steps += action[0].shape[0]\n",
    "        rb.put(transition_workspace)\n",
    "\n",
    "        if nb_steps > cfg.algorithm.learning_starts:\n",
    "            # Get a sample from the workspace\n",
    "            rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n",
    "\n",
    "            done, truncated, reward, action_logprobs_rb = rb_workspace[\n",
    "                \"env/done\", \"env/truncated\", \"env/reward\", \"action_logprobs\"\n",
    "            ]\n",
    "\n",
    "            # Determines whether values of the critic should be propagated\n",
    "            # True if the episode reached a time limit or if the task was not done\n",
    "            # See https://github.com/osigaud/bbrl/blob/master/docs/time_limits.md\n",
    "            must_bootstrap = ~terminated[1]\n",
    "\n",
    "            critic_loss = compute_critic_loss(cfg, reward, must_bootstrap,\n",
    "                                              t_actor, q_agent, target_q_agent,\n",
    "                                              rb_workspace, ent_coef)\n",
    "\n",
    "            logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n",
    "\n",
    "            actor_loss = compute_actor_loss(\n",
    "                ent_coef, t_actor, q_agent, rb_workspace\n",
    "            )\n",
    "            logger.add_log(\"actor_loss\", actor_loss, nb_steps)\n",
    "\n",
    "            # Entropy coef update part ########################\n",
    "            if entropy_coef_optimizer is not None:\n",
    "                # Important: detach the variable from the graph\n",
    "                # so that we don't change it with other losses\n",
    "                # see https://github.com/rail-berkeley/softlearning/issues/60\n",
    "                ent_coef = torch.exp(log_entropy_coef.detach())\n",
    "                entropy_coef_loss = -(\n",
    "                        log_entropy_coef * (action_logprobs_rb + target_entropy)\n",
    "                ).mean()\n",
    "                entropy_coef_optimizer.zero_grad()\n",
    "                # We need to retain the graph because we reuse the\n",
    "                # action_logprobs are used to compute both the actor loss and\n",
    "                # the critic loss\n",
    "                entropy_coef_loss.backward(retain_graph=True)\n",
    "                entropy_coef_optimizer.step()\n",
    "                logger.add_log(\"entropy_coef_loss\", entropy_coef_loss, nb_steps)\n",
    "                logger.add_log(\"entropy_coef\", ent_coef, nb_steps)\n",
    "\n",
    "            # Actor update part ###############################\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                actor.parameters(), cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # Critic update part ###############################\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                critic.parameters(), cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            critic_optimizer.step()\n",
    "            ####################################################\n",
    "\n",
    "            # Soft update of target q function\n",
    "            tau = cfg.algorithm.tau_target\n",
    "            soft_update_params(critic, target_critic, tau)\n",
    "            # soft_update_params(actor, target_actor, tau)\n",
    "\n",
    "        # Evaluate ###########################################\n",
    "        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n",
    "            tmp_steps = nb_steps\n",
    "            eval_workspace = Workspace()  # Used for evaluation\n",
    "            eval_agent(\n",
    "                eval_workspace,\n",
    "                t=0,\n",
    "                stop_variable=\"env/done\",\n",
    "                stochastic=False,\n",
    "            )\n",
    "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
    "            mean = rewards.mean()\n",
    "            logger.log_reward_losses(mean, nb_steps)\n",
    "\n",
    "            pbar.set_description(f\"nb_steps: {nb_steps}, reward: {mean:.3f}\")\n",
    "            if cfg.save_best and mean > best_reward:\n",
    "                best_reward = mean\n",
    "                directory = f\"./agents/{cfg.gym_env.env_name}/tqc_agent/\"\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                filename = directory + cfg.gym_env.env_name + \"#tqc#team\" + str(mean.item()) + \".agt\"\n",
    "                actor.save_model(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099c01d",
   "metadata": {},
   "source": [
    "## Definition of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b668954",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "  \"save_best\": False,\n",
    "  \"logger\":{\n",
    "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
    "    \"log_dir\": \"./tblogs/\" + str(time.time()),\n",
    "    \"cache_size\": 10000,\n",
    "    \"every_n_seconds\": 10,\n",
    "    \"verbose\": False,    \n",
    "    },\n",
    "\n",
    "  \"algorithm\":{\n",
    "    \"seed\": 1,\n",
    "    \"n_envs\": 1,\n",
    "    \"n_steps\": 512,\n",
    "    \"n_updates\": 512,\n",
    "    \"buffer_size\": 1e6,\n",
    "    \"batch_size\": 256,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"nb_evals\":10,\n",
    "    \"eval_interval\": 2000,\n",
    "    \"learning_starts\": 10000,\n",
    "    \"max_epochs\": 8000,\n",
    "    \"discount_factor\": 0.98,\n",
    "    \"entropy_coef\": 1e-7,\n",
    "    \"target_entropy\": \"auto\",\n",
    "    \"tau_target\": 0.05,\n",
    "    \"top_quantiles_to_drop\": 2,\n",
    "    \"architecture\":{\n",
    "      \"actor_hidden_size\": [32, 32],\n",
    "      \"critic_hidden_size\": [256, 256],\n",
    "      \"n_nets\": 2,\n",
    "      \"n_quantiles\": 25,\n",
    "    },\n",
    "  },\n",
    "  \"gym_env\":{\n",
    "    \"classname\": \"__main__.make_gym_env\",\n",
    "    \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "  \"actor_optimizer\":{\n",
    "    \"classname\": \"torch.optim.Adam\",\n",
    "    \"lr\": 1e-3,\n",
    "    },\n",
    "  \"critic_optimizer\":{\n",
    "    \"classname\": \"torch.optim.Adam\",\n",
    "    \"lr\": 1e-3,\n",
    "    },\n",
    "  \"entropy_coef_optimizer\":{\n",
    "    \"classname\": \"torch.optim.Adam\",\n",
    "    \"lr\": 1e-3,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d3f2c",
   "metadata": {},
   "source": [
    "### Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe1053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./tmp\n",
    "\n",
    "config=OmegaConf.create(params)\n",
    "torch.manual_seed(config.algorithm.seed)\n",
    "run_tqc(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7e7ff",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- use the same code on the Pendulum-v1 environment. This one is harder to tune. Get the parameters from the [rl-baseline3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo) and see if you manage to get SAC working on Pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d04eba",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "- 在 Pendulum-v1 环境中使用相同的代码。这个环境更难调优。可以从 [rl-baseline3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo) 获取参数，看看你是否能成功让 SAC 在 Pendulum 上运行。"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
