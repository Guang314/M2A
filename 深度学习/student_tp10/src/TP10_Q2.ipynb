{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(movies_fn, ratings_fn,min_rating=5, min_weight=10):\n",
    "    \"\"\" Construit le graphe des films:\n",
    "    * movies_fn : movies.csv\n",
    "    * ratings_fn : ratings.csv\n",
    "    * min_rating : seuil minimal du score pour lier un utilisateur à un film\n",
    "    * min_weight : seuil minimal du poids d'une arête pour la garder dans le graphe\n",
    "    \"\"\"\n",
    "    movies = pd.read_csv(movies_fn)\n",
    "    ratings = pd.read_csv(ratings_fn)\n",
    "\n",
    "    rated_movies = ratings[ratings.rating >=min_rating]\n",
    "    grouped_movies = rated_movies[['userId','movieId']].groupby('userId').agg(list)\n",
    "    pair_freq = defaultdict(int)\n",
    "    item_freq = defaultdict(int)\n",
    "\n",
    "    for lst_movies in tqdm(grouped_movies['movieId']):\n",
    "        pairs = combinations(sorted(lst_movies),2)\n",
    "        for i in lst_movies:\n",
    "            item_freq[i] += 1\n",
    "        for (i,j) in pairs:\n",
    "            pair_freq[(i,j)] += 1\n",
    "\n",
    "    movies_graph = nx.Graph()\n",
    "    log_total = math.log(sum(item_freq.values()))\n",
    "    # Pointwise Mutual Information : pmi(x,y) = log p(x,y)/(p(x)p(y)) = log (p(x,y)) - log(p(x)) -log(p(y))\n",
    "    for (i,j),f in pair_freq.items():\n",
    "        pmi = f*(math.log(f) - math.log(item_freq[i]) - math.log(item_freq[j]) + log_total)\n",
    "        if pmi >= min_weight:\n",
    "            movies_graph.add_edge(i,j,weight=pmi)\n",
    "\n",
    "    return movies_graph, movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(graph, num_walks=5, num_steps=10, p=1, q=1):\n",
    "    \"\"\"\"\n",
    "        Construit un ensemble de chemins dans le graphe par marche aléatoire biaisée :\n",
    "        * graph : graphe\n",
    "        * num_walks: nombre de chemins par noeud\n",
    "        * num_step : longueur des chemins\n",
    "        * p : plus p est grand, plus l'exploration est incitée, p  petit -> plus il y a des retours en arriere\n",
    "        * q : plus q est grand, plus la marche reste localisée, q petit -> s'écarte des noeuds explorés\n",
    "    \"\"\"\n",
    "    def next_step(previous, current):\n",
    "        def get_pq(n):\n",
    "            if n == current: return p\n",
    "            if graph.has_edge(n,previous): return 1\n",
    "            return q\n",
    "        weights = [w['weight']/get_pq(n) for n,w in graph[current].items()]\n",
    "        return random.choices(list(graph[current]),weights=weights)[0]\n",
    "    walks = []\n",
    "    nodes = list(graph.nodes())\n",
    "    for walk_iter in range((num_walks)):\n",
    "        for node in tqdm(nodes):\n",
    "            walk = [node]\n",
    "            cur_node = node\n",
    "            prev_node = None\n",
    "            for step  in range(num_steps):\n",
    "                next_node = next_step(prev_node,cur_node)\n",
    "                walk.append(next_node)\n",
    "                prev_node = cur_node\n",
    "                cur_node = next_node\n",
    "            walks.append(walk)\n",
    "    return walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 11:07:28.579753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736244449.020604    7627 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736244449.145912    7627 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-07 11:07:30.223625: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# from utils import random_walk,construct_graph\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    \"\"\"Dataset for Skip-Gram training\"\"\"\n",
    "    def __init__(self, walks, window_size, nodes2id):\n",
    "        self.pairs = []\n",
    "        for walk in walks:\n",
    "            for i, center_node in enumerate(walk):\n",
    "                context_start = max(0, i - window_size)\n",
    "                context_end = min(len(walk), i + window_size + 1)\n",
    "                for j in range(context_start, context_end):\n",
    "                    if i != j:\n",
    "                        self.pairs.append((nodes2id[center_node], nodes2id[walk[j]]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.pairs[index][0]), torch.tensor(self.pairs[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    \"\"\"Skip-Gram Model for Vec2node\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.input_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, center, context, negative):\n",
    "        center_emb = self.input_embeddings(center)\n",
    "        context_emb = self.output_embeddings(context)\n",
    "        negative_emb = self.output_embeddings(negative)\n",
    "\n",
    "        # Positive score\n",
    "        pos_score = torch.sum(center_emb * context_emb, dim=1)\n",
    "        pos_loss = -torch.log(torch.sigmoid(pos_score) + 1e-10).mean()\n",
    "\n",
    "        # Negative score\n",
    "        neg_score = torch.bmm(negative_emb, center_emb.unsqueeze(2)).squeeze()\n",
    "        neg_loss = -torch.log(1 - torch.sigmoid(neg_score) + 1e-10).mean()\n",
    "\n",
    "        return pos_loss + neg_loss\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        return self.input_embeddings.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram(model, dataloader, optimizer, epochs, negative_samples, vocab_size, id2title, log_dir='./runs/Q2'):\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for center, context in dataloader:\n",
    "            center = center.to(device)\n",
    "            context = context.to(device)\n",
    "\n",
    "            # Generate negative samples\n",
    "            negative = torch.randint(0, vocab_size, (center.size(0), negative_samples), device=device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(center, context, negative)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        logging.info(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.eval()\n",
    "            embeddings = model.embeddings.weight.detach().cpu().numpy()\n",
    "            writer.add_embedding(\n",
    "                torch.tensor(embeddings), metadata=id2title, tag=f\"Epoch {epoch+1}\"\n",
    "            )\n",
    "            model.train()\n",
    "\n",
    "    writer.close()\n",
    "    logging.info(\"TensorBoard embeddings logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    PATH = \"data/ml-latest-small/\"\n",
    "    logging.info(\"Constructing graph\")\n",
    "    movies_graph, movies = construct_graph(PATH + \"movies.csv\", PATH + \"ratings.csv\")    # 构建 图\n",
    "    logging.info(\"Sampling walks\")\n",
    "    walks = random_walk(movies_graph,5,10,1,1)    # 随机游走\n",
    "    nodes2id = dict(zip(movies_graph.nodes(),range(len(movies_graph.nodes()))))\n",
    "    id2nodes = list(movies_graph.nodes())\n",
    "    id2title = [movies[movies.movieId==idx].iloc[0].title for idx in id2nodes]\n",
    "    ##  TODO: \n",
    "\n",
    "    # Parameters\n",
    "    embedding_dim = 128\n",
    "    window_size = 2\n",
    "    negative_samples = 5\n",
    "    epochs = 10\n",
    "    batch_size = 128\n",
    "\n",
    "    # Generate dataset\n",
    "    dataset = SkipGramDataset(walks, window_size, nodes2id)\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize model\n",
    "    vocab_size = len(nodes2id)\n",
    "    model = SkipGramModel(vocab_size, embedding_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Train model\n",
    "    logging.info(\"Training loss with TensorBoard visualization\")\n",
    "    train_skipgram(model, dataloader, optimizer, epochs, negative_samples, vocab_size, id2title)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
